\subsection{Automation and orchestration tools}
Automation and orchestration are two different things although are sometimes
used interchangeably (and incorrectly so). Automation refers to completing a
single task, such as deploying a virtual machine, shutting down an interface,
or generating a report. Orchestration refers to assembling/coordinating a
process/workflow, which is effectively an ordered set of tasks glued together
with conditions. For example, deploy this virtual machine, and if it fails,
shutdown this interface and generate a report. Automation is to task as
orchestration is to process/workflow.

Often times the task to automate is what an engineer would configure using
some programming/scripting language such as Java, C, Python, Perl, Ruby, etc.
The variance in tasks can be very large since an engineer could be presented
with a totally different task every hour. Creating 500 VLANs on 500 switches
isnt difficult, but is monotonous, so writing a short script to complete this
task is ideal. Adding this script as an input for an orchestration engine
could properly insert this task into a workflow. For example, run the
VLAN-creation script after the nightly backups but before 6:00 AM the
following day. If it fails, the orchestrator can revert all configurations so
that the developer can troubleshoot any script errors.

With all the advances in network automation, it is important to understand the
role of configuration management (CM) and how new technologies may change the
logic. Depending on the industry, the term CM may be synonymous with source
code management (SCM) or version control (VC). Traditional networking CM
typically consisted of a configuration control board (CCB) along with an
organization that maintained device configurations. While the corporate
governance gained by the CCB has value, the maintenance of device
configurations may not. Using the ``infrastructure as code'' concept,
organizations can template/script their device configurations and apply CM
practices only to the scripts. One example is using Ansible with the Jinja2
template language. Simply maintaining these scripts, along with their
associated playbooks and variable files, has many benefits:

\begin{enumerate}
  \item \textbf{Less to manage:} A network with many nodes is likely to have many
  device configurations that are almost identical. One such example would be
  restaurant/retail chains as it relates to WAN sites. By creating a template
  for a common architecture, then maintaining site-specific variable files,
  updating configurations becomes simpler.
  \item \textbf{Enforcement:} Simply running the script will baseline the entire
  network based on the CCBs policy. This can be done on a regular basis to wipe
  away and vestigial (or malicious/damaging) configurations from devices quickly.
  \item \textbf{Easy to test:} Running the scripts in a development environment, such
  as on some VMs in a private data center or compute instances in public cloud,
  can simplify the testing of your code before applying it to the production network.
\end{enumerate}

\subsubsection{Cloud Center}
Cisco Cloud Center (formerly CliQr) is a software solution design for
application deployment in multi-cloud environments. Large organizations often
use a variety of cloud providers for different purposes. For example, a
company may use Amazon AWS for code development and integration testing using
the CodeCommit and CodeBuild SaaS offerings, respectively. The same
organization could be using Microsoft Azure for its Active Directory (AD)
services as Azure offers AD as a service. Last, the organization may use a
private cloud (e.g. OpenStack or VMware) to host sensitive applications which
are Government-regulated and have strict data protection requirements.

Managing each of these clouds independently, using their respective dashboards
and APIs, can become cumbersome. Cisco Cloud Center is designed to be another
level of abstraction in an organization's cloud management strategy by
providing a single point for applications to be deployed based on user policy.
Using the example above, there are certain applications that are best operated
on a specific cloud provider. Other applications may not have strict
requirements, but Cloud Center can deploy and migrate applications between
clouds based on user policy. For example, one application may require very
high disk read/write capabilities, and perhaps this is less expensive in
Azure. Another application may require very high availability, and perhaps
this is best achieved in AWS\@. Note that these are examples only and not
indicative of any cloud provider in particular.

Applications can be abstracted into individual components, usually virtual
machines or containers, and Cloud Center can deploy those applications where
they best serve the organization's needs. The administrator can ``just say go''
and Cloud Center interacts with the different cloud providers through their
various APIs, reducing development costs for large organizations that would
need to develop their own. Cloud Center also has southbound APIs to other
Cisco Data Center products, such as UCS Director, to help manage application
deployment in private cloud environments.

\subsubsection{Digital Network Architecture Center (DNA-C) Demonstration}
DNA-C is Cisco's enterprise \textit{management and control solution for the Digital
Network Architecture (DNA).} DNA is Cisco's intent-based networking solution
which means that the desired state is configured within DNA-C, and the system
makes this desired state a reality in the network without the administrator
needing to know or care about the current state. The solution is like a
``manager of managers'' and can tie into other Cisco management products, such
as Identity Services Engine (ISE) and Viptela vManage, using REST APIs. These
integrations allow DNA-C to seamlessly support SDA and SD-WAN within an
enterprise LAN/WAN environment. DNA-C is broken down into three sequential
workflow types:

\begin{enumerate}
  \item \textbf{Design:} This is where the administrators define the ``intent'' for the
  network. For example, an administrator may define a geographical region
  everywhere the company operates, and add sites into each region. There can be
  regionally-significant variables and design criteria which are supplemented by
  site-specified design criteria. One example could be IP pools, whereby the
  entire region fits into a large /14 and each individual site gets a /24,
  allowing up to 1024 sites per region and keeping the IP numbering scheme
  predictable. There are many more options here; some are covered briefly in the
  upcoming demonstration.
  \item \textbf{Policy:} Generally relates to SDA security policies and gives granular
  control to the administrator. Access and LAN security technologies are
  configured here, such as 802.1X, Trustsec using Scalable Group Tags (SGT),
  virtual networking and segmentation, and traffic copying via encapsulated
  remote switch port analyzer (ERSPAN). Some of these features require ISE
  integration, such as Trustsec, but not all do. As such, DNA-C can provide
  improved security for the LAN environment even without ISE present.
  \item \textbf{Provision:} After the network has been designed with its appropriate
  policies attached, DNA-C can provision these new sites. This workflow usually
  includes pushing VNF images and their corresponding day 0 configurations onto
  hypervisors, such as NFVIS\@. This is detailed in the upcoming demonstration as
  describing it in the abstract is difficult.
\end{enumerate}

The demonstration in this session ties in with the previous NFVIS
demonstration which discussed the hypervisor and its local management
capabilities. Specifically, DNA-C provides improved orchestration over the
NFVIS nodes. DNA-C can provide day 0 configurations and setup for a variety of
VNFs on NFVIS\@. It can also provide the NFVIS hypervisor software itself,
allowing for scaled software updates. Upon logging into DNAC, the screenshot
below is displayed. The three main workflows (design, policy, and provision)
are navigable hyperlinks, making it easy to get started. \textbf{DNA-C version 1.2
is used in this demonstration.} Today, Cisco provides DNA-C as a physical UCS
server.

\addimg{dnac-main.png}{0.7}{DNA-C Home Dashboard}

After clicking on the \textbf{Design} option, the main design screen displays
a geographic map of the network in the \textbf{Network Hierarchy} view. In
this small network, the region of \textbf{Aberdeen} has two sites within it,
\textbf{Site200} and \textbf{Site300}. Each of these sites has a Cisco ENCS
5412 platform running NFVIS 3.8.1-FC3; they represent large branch sites.
Additional sites can be added manually or imported from a comma-separated
values (CSV) file. Each of the other subsections is worth a brief discussion:

\begin{enumerate}
  \item Network Settings: This is where the administrator defines basic
  network options such as IP address pools, QoS settings, and integration with
  wireless technologies.
  \item Image Repository: The inventory of all images, virtual and physical,
  that are used in the network. Multiple flavors of an image can be stored, with
  one marked as the ``golden image'' that DNA-C will ensure is running on the
  corresponding network devices.
  \item Network Profiles: These network profiles bind the specific VNF
  instances to a network hierarchy, serving as network-based intent instructions
  for DNA-C. A profile can be applied globally, regionally, or to a site. In
  this demonstration, the ``Routing \& NFV'' profile is used, but DNA-C also
  supports a ``Switching'' profile and a ``Wireless'' profile, both of which
  simplify SDA operations.
  \item Auth Template: These templates enable faster IEEE 802.1X
  configuration. The 3 main options include closed authentication (strict mode),
  easy connect (low impact mode), and open authentication (anyone can connect).
  Administrators can add their own port-based authentication profiles here for
  more granularity. Since 802.1X is not used in this demonstration, this
  particular option is not discussed further.
\end{enumerate}

\addimg{dnac-geoview.png}{0.7}{DNA-C Geographic View}

The Network Settings tab warrants some additional discussion. In this tab,
there are additional options to further customize your network. Brief
descriptions and provided below. Recall that these settings can be configured
at the global, regional, or site level.

\begin{enumerate}
  \item \textbf{Network:} Basic network settings such as DHCP/DNS server
  addresses and domain name. It might be sensible to define the domain name at
  the global level and DHCP/DNS servers at the regional or site level, for example.
  \item \textbf{Device Credentials:} Because DNA-C can directly manage network
  devices, it must know the credentials to access them. Options including SSH,
  SNMP, and HTTP protocols.
  \item \textbf{IP Address Pools:} Discussed briefly earlier, this is where
  administrators defined the IP ranges used at the global, regional, and site
  levels. DNA-C helps manage these IP pools to reduce that manual burden from
  network operators.
  \item \textbf{SP Profiles:} Many carriers use different QoS models. For
  example, some use a 3-class model (gold, silver, bronze) while others use
  granular 8-class or 12-class models. By assigned specific SP profiles to
  regions or sites, DNA-C helps keep QoS configuration consistent to improve
  the user experience.
  \item \textbf{Wireless:} DNA-C can tie into Cisco Mobile eXperiences (CMX)
  family of products to manage large wireless networks. It is particularly
  useful for those with extensive mobility/roaming. The administrator can set
  up both enterprise and guest wireless LANs, RF profiles, and more. DNA-C
  also supports integration with Meraki products without an additional license requirement.
\end{enumerate}

\addimg{dnac-netsettings.png}{0.7}{DNA-C Network Setings}

Additionally, the Network Profiles tab is particularly interesting for this
demonstration as VNFs are being provisioned on remote ENCS platforms running
NFVIS\@. On a global, regional, or per site basis, the administrator can
identify which VNFs should run on which NFVIS-enabled sites. For example,
sites in one region may only have access to high-latency WAN transport, and
thus could benefit from WAN optimization VNFs. Such an expense may not be
required in other regions where all transports are relatively low-latency. The
screenshot below shows an example. Note the similarities with the NFVIS
drag-and-drop GUI\@; in this solution, the administrator checks boxes on the
left hand side of the screen to add or remove VNFs. The virtual networking
between VNFs is defined elsewhere in the profile and is not discussed in
detail here.

\addimg{dnac-netprofile-vnf.png}{0.7}{DNA-C Network Profile for VNFs}

After configuring all of the network settings, administrators can populate
their \textbf{Image Repository.} This contains a list of all virtual and physical
images currently loaded onto DNA-C. There are two screenshots below. The first
shows the physical platform images, in this case, the NFVIS hypervisor.
Appliance software, such as a router IOS image, could also appear here. The
second screenshot shows the virtual network functions (VNFs) that are present
in DNA-C. In this example, there is a Viptela vEdge SD-WAN router and ASAv image.

\addimg{dnac-imagerepop.png}{0.7}{DNA-C Images for Physical Devices}

\addimg{dnac-imagerepov.png}{0.7}{DNA-C Images for Virtual Devices}

After completing all of the design steps (for brevity, several were not
discussed in detail here), navigate back to the main screen and explore the
\textbf{Policy} section. The policy section is SDA-focused and provides
security enhancements through traffic filtering and network segmentation
techniques. The dashboard provides a summary of the current policy
configuration. In this example, SDA was not configured, since the ENCS/NFVIS
provisioning demonstration does not include a campus environment. The policy
options are summarized below:

\begin{enumerate}
  \item \textbf{Group-Based Access Control:} This performs ACL style filtering based
  on the SGTs defined earlier. This is the core element of Cisco's
  Trustsec model, which is a technique for deployment stateless traffic filters
  throughout the network without the operational burden that normally follows
  it. This option requires Cisco ISE integration.
  \item \textbf{IP Based Access Control:} When Cisco ISE is absent or the switches
  in the network do not support Trustsec, DNA-C can still help manage traditional
  IP access list support on network devices. This can improve security without
  needing cutting-edge Cisco hardware and software products.
  \item \textbf{Traffic Copy:} This feature uses ERSPAN to capture network traffic
  and tunnel it inside GRE to a collector. This can be useful for troubleshooting
  large networks and provide improved visibility to network operators.
  \item \textbf{Virtual Networks:} This feature provides logical separation between
  and users at layer-2 or layer-3. This requires ISE integration and, upon
  authenticating to the network, ISE and DNA-C team up to assign users to a
  particular virtual network. This logical separation is another method of
  increasing security through segmentation. By default, all end users in a
  virtual network can communicate with one another unless explicitly blocked by
  a blacklist policy.
\end{enumerate}

\addimg{dnac-policymain.png}{0.7}{DNA-C Policy Main Page}

After applying any SDA-related security policies into the network, it's time
to provision the VNFs on the remote ENCS platforms running NFVIS\@. The
screenshot below targets site 200. For the initial day 0 configuration
bootstrapping, the administrator must tell DNA-C what the publicly-accessible
IP address of the remote NFVIS is. This management IP could change as the ENCS
is placed behind NAT devices or in different SP-provided DHCP pools. In this
example, bogus IPs are used as an illustration.

Note that the screenshot is on the second step of the provisioning process.
The first step just confirms the network profile created earlier, which
identifies the VNFs to be deployed at a specific level in the network
hierarchy (global, regional, or site). The third step allows the user to
specific access port configuration, such as VLAN membership and interface
descriptions. The summary tab gives the administrator a review of the
provisioning process before deployment.

\addimg{dnac-provsite.png}{0.7}{DNA-C Site Topology Viewer}

The screenshot that follows shows a log of the provisioning process. This
gives the administrator confidence that all the necessary steps were
completed, and also provides a mechanism for troubleshooting any issues that
arise. Serial numbers and public IP addresses are masked for security.

\addimg{dnac-provlog.png}{0.7}{DNA-C Site Event Logging}

In summary, DNA-C is a powerful tool that unifies network design, SDA policy
application, and VNF provisioning across an enterprise environment.

\subsubsection{Kubernetes Orchestration with minikube Demonstration}
Kubernetes is an open-source container orchestration platform. It is commonly
used to abstract resources like compute, network, and storage away from the
containerized applications that run on top. Kubernetes is to VMware vCenter as
Docker is to VMware virtual machines; Docker abstracts individual application
components and Kubernetes allows the application to scale, be made highly
available, and be centrally managed/monitored. Kubernetes is not a CI/CD
system for deploying code, but managing the containers in which the code has
already been deployed.

Kubernetes introduces many new terms which are critical to understand its
operation. The most important terms, at least for the demonstration in this
section, are discussed next.

\textbf{A pod} is the smallest building block of a Kubernetes deployment. Pods
contain application containers and are managed a single entity. It is common
to place exactly one container in each pod, giving the administrator granular
control over each container. However, it is possible to place multiple
containers in a pod, and makes sense when multiple containers are needed to
provide a single service. A pod cannot be split, which implies that all
containers within a pod ``move together'' between resources in a Kubernetes
cluster. Like Docker containers, pods get one IP address and can have volumes
for data storage. Scaling pods is of particular interest, and using replica
sets is a common way to do this. This creates more copies of a pod within a
deployment.

\textbf{A deployment} is an overarching term to define the entire application
in its totality. This typically includes multiple pods communicating between
one another to make the application functional. Newly created deployments are
placed into servers in the cluster to be executed. High availability is built
into Kubernetes as any failure of the server running the application would
prompt Kubernetes to move the application elsewhere. A deployment can define a
desired state of an application and all of its components (pods, replica sets,
etc.) \\

\textbf{A node} is a worker machine in Kubernetes, which can be physical or
virtual. Where the pods are components of a deployment/application, nodes are
components of a cluster. Although an administrator can just ``create'' nodes in
Kubernetes, this creation is just a representation of a node. The
usability/health of a node depends on whether the Kubernetes master can
communicate with the node. Because nodes can be virtual platforms and
hostnames can be DNS-resolvable, the definition of these nodes can be portable
between physical infrastructures.

\textbf{A cluster} is a collection of nodes that are capable of running pods,
deployments, replica sets, etc. The Kubernetes master is a special type of
node which facilitates communications within the cluster. It is responsible
for scheduling pods onto nodes and responding to events within the cluster. A
node-down event, for example, would require the master to reschedule pods
running on that node elsewhere.

\textbf{A service} is concept used to group pods of similar functionality
together. For example, many database containers contain content for a web
application. The database group could be scaled up or down (i.e.\ they change
often), and the application servers must target the correct database
containers to read/write data. The service often has a label, such as
``database'', which would also exist on pods. Whenever the web application
communicates to the service over TCP/IP, the service communicates to any pod
with the ``database'' tag. Services could include node-specific ports, which is
a simple port forwarding mechanism to access pods on a node. Advanced load
balancing services are also available but are not discussed in detail in this
book.

\textbf{Labels} are an important Kubernetes concept and warrant further
discussion. Almost any resource in Kubernetes can carry a collection of
labels, which is a key/value pair. For example, consider the blue/green
deployment model for an organization. This architecture has two identical
production-capable software instances (blue and green), and one is in
production while the other is upgraded/changed. Using JSON syntax, one set of
pods (or perhaps an entire deployment) might be labeled as \verb|{"color": "blue"}|
while the other is \verb|{"color": "green"}|. The key of ``color'' is the same so
the administrator can query for ``color'' label to get the value, and then make
a decision based on that. One Cisco engineer described labels as \textit{flexible and
extensible source of metadata. They can reference releases of code, locations,
or any sort of logical groupings. There is no limitation of how many labels
can be applied.} In this way, labels are similar to tags in Ansible which can
be used to pick-and-choose certain tasks to execute or skip, depending.

The \verb|minikube| solution provides a relatively easy way to get started
with Kubernetes. It is a VM that can run on Linux, Windows, or Mac OS using a
variety of underlying hypervisors. It represents a tiny Kubernetes cluster for
learning the basics. The command line utility used to interact with Kubernetes
is known as \verb|kubectl| and is installed independently of \verb|minikube|.

The installation of \verb|kubectl| and \verb|minikube| on Mac OS is
well-documented. The author recommends using VirtualBox, not xhyve or VMware
Fusion. Despite being technically supported, the author was not able to get
the latter options working. After installation, ensure both binaries exist and
are in the shell \verb|PATH| environment variable.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# which minikube kubectl
/usr/local/bin/minikube
/usr/local/bin/kubectl
\end{minted}

Starting minikube is as easy as the command below. Check the status of the
Kubernetes cluster to ensure there are no errors. Note that a local IP address
is allocated to minikube to support outside-in access to pods and the cluster
dashboard.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# minikube start
Starting local Kubernetes v1.10.0 cluster...
Starting VM...
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.
Loading cached images from config file.

Nicholass-MBP:localkube nicholasrusso# minikube status
minikube: Running
cluster: Running
kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100
\end{minted}

Next, check on the cluster to ensure it resolves to the minikube IP address.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
KubeDNS is running at https://192.168.99.100:8443/api/v1/
  namespaces/kube-system/services/kube-dns:dns/proxy
\end{minted}

We are ready to start deploying applications. The \verb|hello-minikube| application
is the equivalent of ``hello world'' and is a good way to get started. Using the
command below, the Docker container with this application is downloaded from
Google's container repository and is accessible on TCP port 8080. The name of
the deployment is \verb|hello-minikube| and, at this point, contains one pod.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl run hello-minikube \
>  --image=gcr.io/google_containers/echoserver:1.4 --port=8080
deployment.apps "hello-minikube" created
\end{minted}

As discussed earlier, there is a variety of port exposing techniques. The
``NodePort'' option allows outside access into the deployment using TCP port
8080 which was defined when the deployment was created.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl expose deployment \
>  hello-minikube --type=NodePort
service "hello-minikube" exposed
\end{minted}

Check the pod status quickly to see that the pod is still in a state of creating the
container. A few seconds later, the pod is operational.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl get pod
NAME                             READY     STATUS              RESTARTS   AGE
hello-minikube-c8b6b4fdc-nz5nc   0/1       ContainerCreating   0          17s

Nicholass-MBP:localkube nicholasrusso# kubectl get pod
NAME                             READY     STATUS    RESTARTS   AGE
hello-minikube-c8b6b4fdc-nz5nc   1/1       Running   0          51s
\end{minted}

Viewing the network services, Kubernetes reports which resources are reachable
using which IP/port combinations. Actually reaching these IP addresses may be
impossible depending on how the VM is set up on your local machine, and
considering \verb|minikube| is not meant for production, it isn't a big deal.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl get service
NAME             TYPE       CLUSTER-IP      XTERNAL-IP   PORT(S)          AGE
hello-minikube   NodePort   10.98.210.206  <none>        8080:31980/TCP   15s
kubernetes       ClusterIP  10.96.0.1      <none>        443/TCP          7h
\end{minted}

Next, we will scale the application by increasing the replica sets (rs) from 1
to 2. Replica sets, as discussed earlier, are copies of pods typically used to
add capacity to an application in an automated and easy way. Kubernetes has
built-in support for load balancing to replica sets as well.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl get rs
NAME                       DESIRED   CURRENT   READY     AGE
hello-minikube-c8b6b4fdc   1         1         1         1m
\end{minted}

The command below creates a replica of the original pod, resulting in two total pods.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl scale \
>  deployments/hello-minikube --replicas=2
deployment.extensions "hello-minikube" scaled
\end{minted}

Get the pod information to see the new replica up and running. Theoretically,
the capacity of this application has been doubled and can now handle twice the
workload (again, assuming load balancing has been set up and the application
operates in such a way where this is useful).

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl get pod
NAME                             READY     STATUS    RESTARTS   AGE
hello-minikube-c8b6b4fdc-l5jgn   1/1       Running   0          6s
hello-minikube-c8b6b4fdc-nz5nc   1/1       Running   0          1m
\end{minted}

The minikube cluster comes with a GUI interface accessible via HTTP\@. The
Kubernetes web dashboard can be quickly verified from the shell. First, you
can see the URL using the command below, then feed the output from this
command into curl to issue an HTTP GET request.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# minikube service hello-minikube --url
http://192.168.99.100:31980

Nicholass-MBP:localkube nicholasrusso# curl \
>  $(minikube service hello-minikube --url)/health
CLIENT VALUES:
client_address=172.17.0.1
command=GET
real path=/health
query=nil
request_version=1.1
request_uri=http://192.168.99.100:8080/health

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=192.168.99.100:31980
user-agent=curl/7.43.0
BODY:
-no body in request-
\end{minted}

The command below opens up a web browser to the Kubernetes dashboard.

\begin{minted}{text}
Nicholass-MBP:localkube nicholasrusso# minikube dashboard
Opening kubernetes dashboard in default browser...
\end{minted}

The screenshot below shows the overview dashboard of Kubernetes, focusing on
the number of pods that are deployed. At present, there is 1 deployment called
\verb|hello-minikube| which has 2 total pods.

\addimg{k8s-dashboard.png}{0.7}{Kubernetes Main Dashboard}

We can scale the application further from the GUI by increasing the replicas
from 2 to 3. On the far right of the \textbf{deployments} window, click the
three vertical dots, then \textbf{scale}. Enter the number of replicas
desired. The screenshot below shows the prompt window. The screen reminds the
user that there are currently 2 pods, but we desire 3 now.

\addimg{k8s-scale.png}{0.7}{Kubernetes Application Scaling}

After scaling this application, the dashboard changes to show new pods being
added in the diagram that follows. After a few seconds, the dashboard reflects 3
healthy pods (not shown for brevity). During this state, the third replica set
is still being initialized and is not available for workload processing yet.

\addimg{k8s-workload-status.png}{0.7}{Kubernetes Application Scaling}

\addimg{k8s-workload-status.png}{0.7}{Kubernetes Workload Status}

Scrolling down further in the dashboard, the individual pods and replica sets
are listed. This is similar to the output displayed earlier from the
\verb|kubectl get pods| command.

\addimg{k8s-pods.png}{0.7}{Kubernetes Pods Summary}

Checking the CLI again, the new replica set (ending in \verb|cxxlg|) created
from the dashboard appears here.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl get pods
NAME                             READY     STATUS    RESTARTS   AGE
hello-minikube-c8b6b4fdc-cxxlg   1/1       Running   0          21s
hello-minikube-c8b6b4fdc-l5jgn   1/1       Running   0          8m
hello-minikube-c8b6b4fdc-nz5nc   1/1       Running   0          10m
\end{minted}

To delete the deployment when testing is complete, use the command below. The
entire deployment (application) and all associated pods are removed.

\begin{minted}{bash}
Nicholass-MBP:localkube nicholasrusso# kubectl delete deployment hello-minikube
deployment.extensions "hello-minikube" deleted

Nicholass-MBP:localkube nicholasrusso# kubectl get pods
No resources found.
\end{minted}

Kubernetes can also run as-a-service in many public cloud providers. For
example, Google Kubernetes Engine (GKE), AWS Elastic Container Service for
Kubernetes (EKS), and Microsoft Azure Kubernetes Service (AKS). The author has
done a brief investigation into EKS in particular, but all of these SaaS
services are similar in their core concept. The main driver for Kubernetes
as-a-service was to avoid building clusters manually using IaaS building
blocks, such as AWS EC2, S3, VPC, etc. Achieving high availability is
difficult due to coordination between multiple masters in a common cluster.
With the SaaS offerings, the cloud providers offer a fully managed service
with which users interface directly. Specifically for EKS, the hostname
provided to a customer would look something like
\verb|mycluster.eks.amazonaws.com|. Administrators can SSH to this hostname and
issue \verb|kubectl| commands as usual, along with all dashboard functionality
one would expect.

\subsubsection{Amazon Web Services (AWS) CLI Demonstration}
The AWS command line interface (CLI) is a simple way to interact with AWS
programmatically. Like most APIs, consumers can both read and write data,
which simplifies interaction. Initially setting up the AWS CLI is relatively
simple and many tutorials exist, so this book covers the main points using
some AWS console screenshots.

First, create a user and group with permissions to, at a minimum, create and
delete EC2 instances. For demonstration purposes, the ``terraform'' user is
placed in the ``terraform'' group which has full EC2 access (create, delete,
change power state, etc.) Note that the word ``terraform'' is used because
this section serves as a primer for the Terraform demo in the following
section. Take note of the user Amazon Resource Name (ARN) as this can be used
for verifying AWS CLI connectivity.

\addimg{tf-user-group.png}{0.7}{AWS User/Group Assignments for Terraform}

\addimg{tf-ec2-fullperm.png}{0.7}{AWS EC2 Permissions for Terraform}

Next, generate specific programmatic credentials for the ``terraform'' user. The
access key is used by AWS to communicate the username and other unique data
about your AWS account, and the secret key is a password that should not be
shared.

Once the new ``terraform'' user exists in the proper group with the proper
permissions and a valid access key, run \verb|aws configure| from the shell. The
\verb|aws| binary can be installed via Python pip, but if you are like the author
and are using an EC2 instance to run the AWS CLI, it comes pre-installed on
Amazon Linux. Simply answer the questions as they appear, and always
copy/paste the access and secret keys to avoid typos. Choose a region near you
and use ``json'' for the output format, which is the most programmatically
appropriate answer.

\begin{minted}{text}
[ec2-user@devbox ~]# aws configure
AWS Access Key ID [None]: AKIAJKRONVDHHQ3GJYGA
AWS Secret Access Key [None]: [hidden]
Default region name [None]: us-east-1
Default output format [None]: json
\end{minted}

To quickly test whether AWS CLI is set up correctly, use the command below. Be
sure to match up the \verb|Arn| number and username to what is shown in the
screenshots above.

\begin{minted}{text}
[ec2-user@devbox ~]# aws sts get-caller-identity
\end{minted}

\begin{minted}{json}
{
    "Account": "043535020805", 
    "UserId": "AIDAINLWE2QY3Q3U6EVF4", 
    "Arn": "arn:aws:iam::043535020805:user/terraform"
}
\end{minted}

The goal of this short demonstration is to deploy a Cisco CSR1000v into the
default VPC within the availability zone us-east-1a. Building out a whole new
virtual environment using the AWS CLI manually is not terribly difficult but
would be time consuming (and likely boring) for readers. Many of the AWS CLI
``getter'' commands are prefixed with the word \verb|describe|. To get information
about VPCs, use \verb|describe-vpcs| shown below. The current environment has two
VPCs: the default VPC and a custom Ansible VPC used for Ansible development.
The VPC without a name is the default. Record the \verb|VpcId| of the default VPC
which is \verb|vpc-889b03ee|.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 describe-vpcs
\end{minted}

\begin{minted}{json}
{
    "Vpcs": [
        {
            "VpcId": "vpc-7d5a7b1b", 
            "InstanceTenancy": "default", 
            "Tags": [
                {
                    "Value": "VPC_Ansible", 
                    "Key": "Name"
                }
            ], 
            "CidrBlockAssociationSet": [
                {
                    "AssociationId": "vpc-cidr-assoc-7d5c0815", 
                    "CidrBlock": "10.125.0.0/16", 
                    "CidrBlockState": {
                        "State": "associated"
                    }
                }
            ], 
            "State": "available", 
            "DhcpOptionsId": "dopt-4d2cb42a", 
            "CidrBlock": "10.125.0.0/16", 
            "IsDefault": false
        }, 
        {
            "VpcId": "vpc-889b03ee", 
            "InstanceTenancy": "default", 
            "CidrBlockAssociationSet": [
                {
                    "AssociationId": "vpc-cidr-assoc-c66fe2ae", 
                    "CidrBlock": "172.31.0.0/16", 
                    "CidrBlockState": {
                        "State": "associated"
                    }
                }
            ], 
            "State": "available", 
            "DhcpOptionsId": "dopt-4d2cb42a", 
            "CidrBlock": "172.31.0.0/16", 
            "IsDefault": true
        }
    ]
}
\end{minted}

Armed with the VPC ID from above, ask for the subnets available in this VPC\@.
By default, every AZ within this region has a default subnet, but since this
demonstration is focused on us-east-1a, we can apply some filters. First, we
filter subnets only contained in the default VPC, then additionally only on
the us-east-1a AZ subnets. One subnet is returned with \verb|SubnetId|
of \verb|subnet-f1dfa694|.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 describe-subnets --filters \
>  'Name=vpc-id,Values=vpc-889b03ee' 'Name=availability-zone,Values=us-east-1a'
\end{minted}

\begin{minted}{json}
{
    "Subnets": [
        {
            "AvailabilityZone": "us-east-1a", 
            "AvailableIpAddressCount": 4091, 
            "DefaultForAz": true, 
            "Ipv6CidrBlockAssociationSet": [], 
            "VpcId": "vpc-889b03ee", 
            "State": "available", 
            "MapPublicIpOnLaunch": true, 
            "SubnetId": "subnet-f1dfa694", 
            "CidrBlock": "172.31.64.0/20", 
            "AssignIpv6AddressOnCreation": false
        }
    ]
}
\end{minted}

Armed with the proper subnet for the CSR1000v, an Amazon Machine Image (AMI)
must be identified to deploy. Since there are many flavors of CSR1000v
available, such as bring your own license (BYOL), maximum performance, and
security, apply a filter to target the specific image desired. The example
below shows a name-based filter searching for a string containing 16.09 as the
version followed later by BYOL, the lowest cost option. Record the \verb|ImageId|,
which is \verb|ami-0d1e6af4c329efd82|, as this is the image to deploy.
Note: Cisco images require the user to accept the terms of a license agreement
before usage. One must navigate to the following page first,
subscribe, and accept the terms prior to attempting to start this instance or
launch will result in an error. Visit this
\href{https://aws.amazon.com/marketplace/pp/B00NF48FI2}{link} for details.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 describe-images --filters \
>  'Name=name,Values=cisco-CSR-.16.09*BYOL*'
\end{minted}

\begin{minted}{json}
{
    "Images": [
        {
            "ProductCodes": [
                {
                    "ProductCodeId": "5tiyrfb5tasxk9gmnab39b843", 
                    "ProductCodeType": "marketplace"
                }
            ], 
            "Description": "cisco-CSR-trhardy-20180727122305.16.09.01-BYOL-HVM", 
            "VirtualizationType": "hvm", 
            "Hypervisor": "xen", 
            "ImageOwnerAlias": "aws-marketplace", 
            "EnaSupport": true, 
            "SriovNetSupport": "simple", 
            "ImageId": "ami-0d1e6af4c329efd82", 
            "State": "available", 
            "BlockDeviceMappings": [
                {
                    "DeviceName": "/dev/xvda", 
                    "Ebs": {
                        "Encrypted": false, 
                        "DeleteOnTermination": true, 
                        "VolumeType": "standard", 
                        "VolumeSize": 8, 
                        "SnapshotId": "snap-010a7ddb206eb016e"
                    }
                }
            ], 
            "Architecture": "x86_64", 
            "ImageLocation": "aws-marketplace/cisco-CSR-.16.09.01-BYOL-HVM-[snip]", 
            "RootDeviceType": "ebs", 
            "OwnerId": "679593333241", 
            "RootDeviceName": "/dev/xvda", 
            "CreationDate": "2018-09-19T00:59:25.000Z", 
            "Public": true, 
            "ImageType": "machine", 
            "Name": "cisco-CSR-.16.09.01-BYOL-[snip]"
        }
    ]
}
\end{minted}

Two other minor pieces of information are needed. First, capture the available
key chains and choose the most appropriate one for this instance. One key pair
is available. The name ``EC2-key-pair'' will be used when deploying the CSR1000v.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 describe-key-pairs
\end{minted}

\begin{minted}{json}
{
    "KeyPairs": [
        {
            "KeyName": "EC2-key-pair", 
            "KeyFingerprint": "fc:41:d4:[snip]"
        }
    ]
}
\end{minted}

Next, capture the available security groups and choose one. Be sure to filter
on the default VPC to avoid cluttering output with any Ansible VPC related
security groups. The default security group, in this case, is wide open and
permits all traffic. The \verb|GroupId| of \verb|sg-4d3a5c31| can be used
when deploying the CSR1000v.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 describe-security-groups --filter \
>  'Name=vpc-id,Values=vpc-889b03ee'
\end{minted}

\begin{minted}{json}
{
    "SecurityGroups": [
        {
            "IpPermissionsEgress": [
                {
                    "IpProtocol": "-1", 
                    "PrefixListIds": [], 
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ], 
                    "UserIdGroupPairs": [], 
                    "Ipv6Ranges": []
                }
            ], 
            "Description": "default VPC security group", 
            "IpPermissions": [
                {
                    "IpProtocol": "-1", 
                    "PrefixListIds": [], 
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ], 
                    "UserIdGroupPairs": [], 
                    "Ipv6Ranges": []
                }
            ], 
            "GroupName": "default", 
            "VpcId": "vpc-889b03ee", 
            "OwnerId": "043535020805", 
            "GroupId": "sg-4d3a5c31"
        }
    ]
}
\end{minted}

With all the key information collected, use the command below with the
appropriate inputs to create the new EC2 instance. After running the command,
a string is returned with the instance ID of the new instance; this is why the
\verb|--query| argument is handy when deploying new instances using AWS CLI\@. The
CSR1000v will take a few minutes to fully power up.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 run-instances --image-id ami-0d1e6af4c329efd82 \
>                              --subnet-id subnet-f1dfa694 \
>                              --security-group-ids sg-4d3a5c31 \
>                              --count 1 \
>                              --instance-type t2.medium \
>                              --key-name EC2-key-pair \
>                              --query "Instances[0].InstanceId"
"i-08808ba7abf0d2242"
\end{minted}

In the meantime, collect information about the instance using the command
below. Use the \verb|--instance-ids| option to supply a list of strings, each
containing a specific instance ID\@. The value returned above is pasted below.
The status is still ``initializing''.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 describe-instance-status --instance-ids 'i-08808ba7abf0d2242'
\end{minted}

\begin{minted}{json}
{
    "InstanceStatuses": [
        {
            "InstanceId": "i-08808ba7abf0d2242", 
            "InstanceState": {
                "Code": 16, 
                "Name": "running"
            }, 
            "AvailabilityZone": "us-east-1a", 
            "SystemStatus": {
                "Status": "ok", 
                "Details": [
                    {
                        "Status": "passed", 
                        "Name": "reachability"
                    }
                ]
            }, 
            "InstanceStatus": {
                "Status": "initializing", 
                "Details": [
                    {
                        "Status": "initializing", 
                        "Name": "reachability"
                    }
                ]
            }
        }
    ]
}
\end{minted}

You can continue running the above command every few minutes until the status
changes to \verb|ok|. Some extra information has been removed from the output.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 describe-instance-status \
>  --instance-ids 'i-08808ba7abf0d2242'
\end{minted}

\begin{minted}{json}
{
            "InstanceStatus": {
                "Status": "ok", 
                "Details": [
                    {
                        "Status": "passed", 
                        "Name": "reachability"
                    }
			    ]
            }
}
\end{minted}

In order to connect to the instance to configure it, the public IP or public
DNS hostname is required. The command below targets this specific information
without a massive JSON dump. Simply feed in the instance ID\@. Without the
complex query, one could manually scan the JSON to find the address, but this
solution is more targeted and elegant.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 describe-instances \
>  --instance-ids i-08808ba7abf0d2242 --output text \
>  --query 'Reservations[*].Instances[*].PublicIpAddress' 
34.201.13.127
\end{minted}

Assuming your private key is already present with the proper permissions
(read-only for owner), SSH into the instance using the newly-discovered public
IP address. A quick check of the IOS XE version suggests that the deployment
succeeded.

\begin{minted}{text}
[ec2-user@devbox ~]# ls -l privkey.pem 
-r-------- 1 ec2-user ec2-user 1670 Jan  1 16:54 privkey.pem

[ec2-user@devbox ~]# ssh -i privkey.pem ec2-user@34.201.13.127

ip-172-31-66-99#show version | include IOS XE
Cisco IOS XE Software, Version 16.09.01
\end{minted}

Termination is simple as well. The only challenge is that, generally, one
would have to rediscover the instance ID assuming the termination happened
long after the instance was created. The alternative is manually writing some
kind of shell script to store that data in a file, which must be manually read
back in to delete the instance. The next section on Terraform helps overcome
these state problems in a simple way, but for now, simply delete the CSR1000v
using the command below. The JSON output confirms that the instance is
shutting down.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 terminate-instances --instance-ids i-08808ba7abf0d2242
\end{minted}

\begin{minted}{json}
{
    "TerminatingInstances": [
        {
            "InstanceId": "i-08808ba7abf0d2242", 
            "CurrentState": {
                "Code": 32, 
                "Name": "shutting-down"
            }, 
            "PreviousState": {
                "Code": 16, 
                "Name": "running"
            }
        }
    ]
}
\end{minted}

This \verb|CurrentState| of \verb|shutting-down| will remain for a few minutes
until the instance is gone. Running the command again confirms the instance no
longer exists as the state is \verb|terminated|.

\begin{minted}{text}
[ec2-user@devbox ~]# aws ec2 terminate-instances --instance-ids i-08808ba7abf0d2242
\end{minted}

\begin{minted}{json}
{
    "TerminatingInstances": [
        {
            "InstanceId": "i-08808ba7abf0d2242", 
            "CurrentState": {
                "Code": 48, 
                "Name": "terminated"
            }, 
            "PreviousState": {
                "Code": 48, 
                "Name": "terminated"
            }
        }
    ]
}
\end{minted}

\subsubsection{Infrastructure as Code using Terraform}
Terraform, like Ansible (discussed later in this book), is relatively easy to
get started using. Understanding Terraform's value is best understood by
contrasting it with the AWS CLI demonstrated in the previous section. While
the AWS CLI provides a simple and powerful method to interact with AWS, it has
several drawbacks. Think of a traditional shell script that simply runs
commands and has basic logical constructs like conditionals, loops, and
variables. Suppose one wants to make the script state-aware so that it only
takes the necessary actions. For example, it doesn't create EC2 instances that
already exist and doesn't try to delete non-existent instances. To accomplish
this, the programmer would have to constantly test for the presence or absence
of certain characteristics (the presence of an instance, the presence of a
line of a text in a file, etc.) before taking action. This makes the script
complex and quickly gets out of control for any non-trivial problem.

Terraform solves this problem through abstraction using a domain-specific
language (DSL), like Ansible. This simplified pseudo-code allows programmers
to declare their intent/endstate and Terraform implements the plan. Like many
automation tools, it is often used as ``infrastructure as code'' whereby the
desired system is described in its entirety, checked into version control, and
centrally enforced. Terraform has a collection of providers, which are
specific libraries used to interact with a variety of platforms. For example,
the forthcoming demonstration will use several AWS-specific providers. Because
Terraform is an abstraction layer, it does not reinvent the AWS CLI, but
rather relies on it behind the scenes.

Terraform's DSL is a completely new format, known as Hashicorp Configuration
Language (HCL). The language resembles a simplified JSON format with the
addition of single and multi line comments. It is designed to be both human
and machine friendly.

In this demonstration, Terraform will provision a new virtual networking
environment within AWS known as a virtual private cloud (VPC) that has a large
IP supernet from which all subnets must be contained. A new subnet will be
created which represents a DMZ for public facing enterprise services offered
by a fictitious company. A Cisco ASAv serves as the Internet edge firewall.
Within the DMZ, a Cisco CSR1000v serves as a VPN concentrator for site-to-site
VPNs. These devices won't be configured at a CLI-level by Terraform, but will
be provisioned and properly connected using AWS networking constructs.
Subsequent configuration management using Ansible, Nornir, or homemade scripts
would generally occur after provisioning by Terraform.

Armed with basic knowledge about Terraform and the task at hand, the
demonstration will provision several AWS resources:

\begin{enumerate}
  \item	Build a new VPC (region us-east-1) for our DMZ devices using the
  \verb|aws_vpc| resource
  \item	Build a new DMZ subnet using the \verb|aws_subnet| resource in the
  us-east-1a availability zone
  \item	Deploy an unlicensed Cisco CSR1000v using the \verb|aws_instance| resource
  \item	Deploy an unlicensed Cisco ASAv using the \verb|aws_instance| resource
\end{enumerate}

Note that the preparatory work described in the AWS CLI section must be
completed before continuing. The author strongly recommends completing that
demonstration first before jumping into Terraform. This ensures that Terraform
can use the AWS CLI credentials to access AWS programmatically.

Installing Terraform requires downloading the proper package for your
operating system from here. For this demonstration, the Linux 64-bit package
is downloaded via wget below.

\begin{minted}{text}
[ec2-user@devbox ~]# wget \
>  https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip
[snip, downloading file]
2019-01-01 15:26:18 (53.2 MB/s) - ‘terraform_0.11.11_linux_amd64.zip’ saved

[ec2-user@devbox ~]# ls -l
-rw-rw-r-- 1 ec2-user ec2-user 20971661 Dec 14 21:21 terraform_0.11.11_linux_amd64.zip
\end{minted}

Unzip the package to reveal a single binary. At this point, Terraform
operators have 3 options:

\begin{enumerate}
  \item	Move the binary to a directory in your \verb|PATH|. This is the
  author's preferred choice and what is done below.
  \item	Add the current directory (where the terraform binary exists) to the
  shell \verb|PATH|.
  \item	Prefix the binary with \verb|./| every time you want to use it.
\end{enumerate}

\begin{minted}{text}
 [ec2-user@devbox ~]# unzip terraform_0.11.11_linux_amd64.zip
Archive:  terraform_0.11.11_linux_amd64.zip
  inflating: terraform

[ec2-user@devbox ~]# file terraform
terraform: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, stripped

[ec2-user@devbox ~]# echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/ec2-user/.local/bin:/home/ec2-user/bin

[ec2-user@devbox ~]# sudo mv terraform /usr/local/bin/
\end{minted}

Test to ensure your shell recognizes \verb|terraform| as a command before continuing.

\begin{minted}{text}
[ec2-user@devbox ~]# which terraform
/usr/local/bin/terraform

[ec2-user@devbox ~]# terraform --version
Terraform v0.11.11
\end{minted}

Last, the author recommends creating a directory for this particular Terraform
project as shown below. Change into that directly and create a new text file
called ``network.tf''. Open the file in your favorite editor to begin creating
the Terraform plan.

\begin{minted}{text}
[ec2-user@devbox ~]# mkdir tf-demo && cd tf-demo
[ec2-user@devbox tf-demo]#
\end{minted}

First, invoke the AWS provider using the code below. While this is technically
not needed, specifying the region in the Terraform plan means that Terraform
will not interactively prompt to hand-type a region every time. Note that the
access and secret keys are not needed because AWS CLI has already been configured.

\begin{minted}{terraform}
# This avoids interaction prompting. The rest of the AWS CLI
# parameters (access and secret keys) should already be defined.
provider "aws" {
  region = "us-east-1"
}
\end{minted}

Next, use the \verb|aws_vpc| resource to create a new VPC\@. The documentation
suggests that only the \verb|cidr_block| argument is required. The author
suggests adding a \verb|Name| tag to help organize resources as well. Note
that there is a large list of ``attribute'' fields on the documentation page.
These are the pieces of data returned by Terraform, such as the VPD ID and
Amazon Resource Name (ARN). These are dynamically allocated at runtime and
referencing these values can simply the Terraform plan later.

\begin{minted}{terraform}
# Create a new VPC for DMZ services
resource "aws_vpc" "tfvpc" {
  cidr_block = "203.0.113.0/24"
  tags = {
    Name = "tfvpc"
  }
}
\end{minted}

Next, use the \verb|aws_subnet| resource to create a new IP subnet. The
documentation indicates that \verb|cidr_block| and \verb|vpc_id| arguments are needed.
The former is self-explanatory as it represents a subnet within the VPC
network of 203.0.113.0/24; this demonstration uses 203.0.113.64/26. The VPC ID
is returned from the \verb|aws_vpc| resource and can be referenced using the \verb|${}|
syntax shown below. The name \verb|tfvpc| has an attribute called \verb|id| that
identifies the VPC in which this new subnet should be created. Like the
\verb|aws_vpc| resource, \verb|aws_subnet| also returns an ID which can be referenced
later when creating EC2 instances.

\begin{minted}{terraform}
# Create subnet within the new VPC for the DMZ
resource "aws_subnet" "dmz" {
  vpc_id            = "${aws_vpc.tfvpc.id}"
  cidr_block        = "203.0.113.64/26"
  availability_zone = "us-east-1a"
  tags = {
    Name = "dmz"
  }
}
\end{minted}

Now that the basic network constructs have been configured, its time to add
EC2 instances to construct the DMZ\@. One could just add a few more resource
invocations to the existing network.tf file. For variety, the author is going
to create a second file for the EC2 compute devices. When multiple *.tf
configuration files exist, they are loaded in alphabetical order, but that's
largely irrelevant since Terraform is smart enough to create/destroy resources
in the appropriate sequence regardless of the file names.

Edit a file called ``services.tf'' in your favorite text editor and apply the
following configuration to deploy a Cisco ASAv and CSR1000v within the
us-east-1a AZ\@. The AMI for the CSR1000v is the same one used in the AWS CLI
demonstration. The AMI for the ASAv is the BYOL version, which was derived
using the AWS CLI \verb|describe-instances|. Both instances are placed in the newly
created subnet within the newly created VPC, keeping everything separate from
any existing AWS resources. Just like with the CSR1000v images, Cisco requires
the user to accept the terms of a license agreement before usage. 
One must navigate the the following page first, subscribe, and accept the
terms prior to attempting to start this instance or launch will result in an
error. Visit this
\href{https://aws.amazon.com/marketplace/pp/B00WRGASUC}{link} for details.

\begin{minted}{terraform}
# Cisco ASAv BYOL
resource "aws_instance" "dmz_asav" {
  ami           = "ami-4fbf3c30"
  instance_type = "m4.large"
  subnet_id     = "${aws_subnet.dmz.id}"
  tags = {
    Name = "dmz_asav"
  }
}

# Cisco CSR1000v BYOL
resource "aws_instance" "dmz_csr1000v" {
  ami           = "ami-0d1e6af4c329efd82"
  instance_type = "t2.medium"
  subnet_id     = "${aws_subnet.dmz.id}"
  tags = {
    Name = "dmz_csr1000v"
  }
}
\end{minted}

Once the Terraform plan files have been configured, use \verb|terraform init|.
This scans all the plan files for any required plugins. In this case, the AWS
provider is needed given the types of resource invocations present. To keep
the initial Terraform binary small, individual provider plugins are not
included and are downloaded as-needed. Like most good tools, Terraform is very
verbose and provides hints and help along the way. The output below represents
a successful setup.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform init

Initializing provider plugins...
- Checking for available provider plugins on https://releases.hashicorp.com...
- Downloading plugin for provider "aws" (1.54.0)...

The following providers do not have any version constraints in configuration,
so the latest version was installed.

To prevent automatic upgrades to new major versions that may contain breaking
changes, it is recommended to add version = "..." constraints to the
corresponding provider blocks in configuration, with the constraint strings
suggested below.

* provider.aws: version = "~> 1.54"

Terraform has been successfully initialized!
[snip]
\end{minted}

Now, run \verb|terraform plan| which loads all the HCL files (.tf) and determines
what changes are needed. Since there is no state already and this plan hasn't
been written to a file, its best to use this output as an opportunity to
review the plan. The fields labeled as \verb|<computed>| are automatically generated
and are available for use by the Terraform operator later. The output is very
long, and future iterations of this output will be snipped for brevity.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform plan
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

------------------------------------------------------------------------

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  + aws_instance.dmz_asav
      id:                               <computed>
      ami:                              "ami-4fbf3c30"
      arn:                              <computed>
      associate_public_ip_address:      <computed>
      availability_zone:                <computed>
      cpu_core_count:                   <computed>
      cpu_threads_per_core:             <computed>
      ebs_block_device.#:               <computed>
      ephemeral_block_device.#:         <computed>
      get_password_data:                "false"
      host_id:                          <computed>
      instance_state:                   <computed>
      instance_type:                    "m4.large"
      ipv6_address_count:               <computed>
      ipv6_addresses.#:                 <computed>
      key_name:                         <computed>
      network_interface.#:              <computed>
      network_interface_id:             <computed>
      password_data:                    <computed>
      placement_group:                  <computed>
      primary_network_interface_id:     <computed>
      private_dns:                      <computed>
      private_ip:                       <computed>
      public_dns:                       <computed>
      public_ip:                        <computed>
      root_block_device.#:              <computed>
      security_groups.#:                <computed>
      source_dest_check:                "true"
      subnet_id:                        "${aws_subnet.dmz.id}"
      tags.%:                           "1"
      tags.Name:                        "dmz_asav"
      tenancy:                          <computed>
      volume_tags.%:                    <computed>
      vpc_security_group_ids.#:         <computed>

  + aws_instance.dmz_csr1000v
      id:                               <computed>
      ami:                              "ami-0d1e6af4c329efd82"
      arn:                              <computed>
      associate_public_ip_address:      <computed>
      availability_zone:                <computed>
      cpu_core_count:                   <computed>
      cpu_threads_per_core:             <computed>
      ebs_block_device.#:               <computed>
      ephemeral_block_device.#:         <computed>
      get_password_data:                "false"
      host_id:                          <computed>
      instance_state:                   <computed>
      instance_type:                    "t2.medium"
      ipv6_address_count:               <computed>
      ipv6_addresses.#:                 <computed>
      key_name:                         <computed>
      network_interface.#:              <computed>
      network_interface_id:             <computed>
      password_data:                    <computed>
      placement_group:                  <computed>
      primary_network_interface_id:     <computed>
      private_dns:                      <computed>
      private_ip:                       <computed>
      public_dns:                       <computed>
      public_ip:                        <computed>
      root_block_device.#:              <computed>
      security_groups.#:                <computed>
      source_dest_check:                "true"
      subnet_id:                        "${aws_subnet.dmz.id}"
      tags.%:                           "1"
      tags.Name:                        "dmz_csr1000v"
      tenancy:                          <computed>
      volume_tags.%:                    <computed>
      vpc_security_group_ids.#:         <computed>

  + aws_subnet.dmz
      id:                               <computed>
      arn:                              <computed>
      assign_ipv6_address_on_creation:  "false"
      availability_zone:                "us-east-1a"
      availability_zone_id:             <computed>
      cidr_block:                       "203.0.113.64/26"
      ipv6_cidr_block:                  <computed>
      ipv6_cidr_block_association_id:   <computed>
      map_public_ip_on_launch:          "false"
      owner_id:                         <computed>
      tags.%:                           "1"
      tags.Name:                        "dmz"
      vpc_id:                           "${aws_vpc.tfvpc.id}"

  + aws_vpc.tfvpc
      id:                               <computed>
      arn:                              <computed>
      assign_generated_ipv6_cidr_block: "false"
      cidr_block:                       "203.0.113.0/24"
      default_network_acl_id:           <computed>
      default_route_table_id:           <computed>
      default_security_group_id:        <computed>
      dhcp_options_id:                  <computed>
      enable_classiclink:               <computed>
      enable_classiclink_dns_support:   <computed>
      enable_dns_hostnames:             <computed>
      enable_dns_support:               "true"
      instance_tenancy:                 "default"
      ipv6_association_id:              <computed>
      ipv6_cidr_block:                  <computed>
      main_route_table_id:              <computed>
      owner_id:                         <computed>
      tags.%:                           "1"
      tags.Name:                        "tfvpc"


Plan: 4 to add, 0 to change, 0 to destroy.

------------------------------------------------------------------------

Note: You didn't specify an "-out" parameter to save this plan, so Terraform
can't guarantee that exactly these actions will be performed if
"terraform apply" is subsequently run.
\end{minted}

Running the command again and specifying an optional output file allows the
plan to be saved to disk.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform plan -out=plan.tfstate
[snip]
  + aws_instance.dmz_asav
      [snip]

  + aws_instance.dmz_csr1000v
      [snip]

  + aws_subnet.dmz
      [snip]

  + aws_vpc.tfvpc
      [snip]

Plan: 4 to add, 0 to change, 0 to destroy.

------------------------------------------------------------------------

This plan was saved to: plan.tfstate

To perform exactly these actions, run the following command to apply:
    terraform apply "plan.tfstate"
\end{minted}

Executing \verb|terraform apply plan.tfstate| instructs Terraform to make this plan
(the intended configuration) become the new reality. Terraform is smart enough
to deploy the resources in the correct sequence when dependencies exist, such
as the subnet referencing the VPC, and the EC2 instances referencing the
subnet. The output from the \verb|apply| command is similar to \verb|plan| in its
formatting and display, but because it is running in realtime, it provides
status updates. Also note that the newly-created subnet
\verb|subnet-01461157fed507e7b| was correctly referenced by the EC2 instances.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform apply plan.tfstate
aws_vpc.tfvpc: Creating...
  arn:                              "" => "<computed>"
  assign_generated_ipv6_cidr_block: "" => "false"
  cidr_block:                       "" => "203.0.113.0/24"
  [snip]
  tags.%:                           "" => "1"
  tags.Name:                        "" => "tfvpc"
aws_vpc.tfvpc: Creation complete after 1s (ID: vpc-0edde0f2f198451e1)
aws_subnet.dmz: Creating...
  arn:                             "" => "<computed>"
  assign_ipv6_address_on_creation: "" => "false"
  availability_zone:               "" => "us-east-1a"
  availability_zone_id:            "" => "<computed>"
  cidr_block:                      "" => "203.0.113.64/26"
  [snip]
  tags.%:                          "" => "1"
  tags.Name:                       "" => "dmz"
  vpc_id:                          "" => "vpc-0edde0f2f198451e1"
aws_subnet.dmz: Creation complete after 1s (ID: subnet-01461157fed507e7b)
aws_instance.dmz_csr1000v: Creating...
  ami:                          "" => "ami-0d1e6af4c329efd82"
  arn:                          "" => "<computed>"
  [snip]
  source_dest_check:            "" => "true"
  subnet_id:                    "" => "subnet-01461157fed507e7b"
  tags.%:                       "" => "1"
  tags.Name:                    "" => "dmz_csr1000v"
  tenancy:                      "" => "<computed>"
  volume_tags.%:                "" => "<computed>"
  vpc_security_group_ids.#:     "" => "<computed>"
aws_instance.dmz_asav: Creating...
  ami:                          "" => "ami-4fbf3c30"
  arn:                          "" => "<computed>"
  [snip]
  source_dest_check:            "" => "true"
  subnet_id:                    "" => "subnet-01461157fed507e7b"
  tags.%:                       "" => "1"
  tags.Name:                    "" => "dmz_asav"
  tenancy:                      "" => "<computed>"
  volume_tags.%:                "" => "<computed>"
  vpc_security_group_ids.#:     "" => "<computed>"
aws_instance.dmz_csr1000v: Still creating... (10s elapsed)
aws_instance.dmz_asav: Still creating... (10s elapsed)
aws_instance.dmz_asav: Creation complete after 15s (ID: i-03ac772e458bb9282)
aws_instance.dmz_csr1000v: Still creating... (20s elapsed)
aws_instance.dmz_csr1000v: Still creating... (30s elapsed)
aws_instance.dmz_csr1000v: Creation complete after 32s (ID: i-04e2992781578b002)

Apply complete! Resources: 4 added, 0 changed, 0 destroyed.
\end{minted}

Quickly verify that the instances were successfully created and are powering
up. It's best to do this verification outside of Terraform just to confirm
from multiple sources that the infrastructure is working as expected\@. Using
the AWS CLI with a detailed query, one can limit the output to just a few
lines, effectively only collecting the \verb|Status| value. Note that the two
instance IDs specified here are annotated above in the output from Terraform.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# aws ec2 describe-instance-status \
>  --instance-ids 'i-03ac772e458bb9282' 'i-04e2992781578b002' \
>  --query InstanceStatuses[*].InstanceStatus.Status
\end{minted}

\begin{minted}{json}
[
    "initializing",
    "initializing"
]
\end{minted}

For those preferring visual confirmation, below is a screenshot from the AWS
console showing these particular instances running. Note that both instances
are in the correct AZ of us-east-1a as well.

\addimg{tf-create.png}{0.7}{Verifying EC2 Instances Made By Terraform}

Quickly checking the subnet details in the AWS console confirm that the subnet
is in the correct VPC, AZ, and has the right IPv4 CIDR range.

\addimg{tf-subnet.png}{0.7}{Verifying VPC Subnet Made By Terraform}

Going back to Terraform, notice that a new \verb|terraform.tfstate| file has
been created. This represents the new infrastructure state after the Terraform
plan was applied. Use \verb|terraform show| to view the file, which contains
all the \verb|computed| fields filled in, such as the ARN value.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# ls -l
total 28
-rw-rw-r-- 1 ec2-user ec2-user   533 Jan  1 18:54 network.tf
-rw-rw-r-- 1 ec2-user ec2-user  7437 Jan  1 19:00 plan.tfstate
-rw-rw-r-- 1 ec2-user ec2-user   417 Jan  1 18:59 services.tf
-rw-rw-r-- 1 ec2-user ec2-user 10917 Jan  1 19:01 terraform.tfstate

[ec2-user@devbox tf-demo]# terraform show
aws_instance.dmz_asav:
  id = i-03ac772e458bb9282
  ami = ami-4fbf3c30
  arn = arn:aws:ec2:us-east-1:043535020805:instance/i-03ac772e458bb9282
  associate_public_ip_address = false
  availability_zone = us-east-1a
  cpu_core_count = 1
  cpu_threads_per_core = 2
  credit_specification.# = 1
  credit_specification.0.cpu_credits = standard
  [snip]
\end{minted}

Running \verb|terraform plan| again provides a diff-like report on what changes
need to be made to the infrastructure to implement the plan. Since no new
changes have been made manually to the environment (outside of Terraform), no
updates are needed.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform plan
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

aws_vpc.tfvpc: Refreshing state... (ID: vpc-0edde0f2f198451e1)
aws_subnet.dmz: Refreshing state... (ID: subnet-01461157fed507e7b)
aws_instance.dmz_csr1000v: Refreshing state... (ID: i-04e2992781578b002)
aws_instance.dmz_asav: Refreshing state... (ID: i-03ac772e458bb9282)

------------------------------------------------------------------------

No changes. Infrastructure is up-to-date.

This means that Terraform did not detect any differences between your
configuration and real physical resources that exist. As a result, no
actions need to be performed.
\end{minted}

Suppose a clumsy user accidentally deletes the CSR1000v as shown below. Wait
for the instance to be \verb|terminated|.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# aws ec2 terminate-instances \
>  --instance-ids i-04e2992781578b002
\end{minted}

\begin{minted}{json}
{
    "TerminatingInstances": [
        {
            "InstanceId": "i-04e2992781578b002",
            "CurrentState": {
                "Code": 32,
                "Name": "shutting-down"
            },
            "PreviousState": {
                "Code": 16,
                "Name": "running"
            }
        }
    ]
}
\end{minted}

Using \verb|terraform plan| now detects a change and suggests needing to add 1 more
resource to the infrastructure make the intended plan a reality. Simple use
\verb|terraform apply| to update the infrastructure and answer \verb|yes| to confirm.
Note that you cannot simply rerun \verb|plan.tfstate| because it was created
against an old state (ie, an old diff between intended and actual states).

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform plan
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

aws_vpc.tfvpc: Refreshing state... (ID: vpc-0edde0f2f198451e1)
aws_subnet.dmz: Refreshing state... (ID: subnet-01461157fed507e7b)
aws_instance.dmz_asav: Refreshing state... (ID: i-03ac772e458bb9282)
aws_instance.dmz_csr1000v: Refreshing state... (ID: i-04e2992781578b002)

------------------------------------------------------------------------

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  + aws_instance.dmz_csr1000v
      id:                           <computed>
      ami:                          "ami-0d1e6af4c329efd82"
      arn:                          <computed>
      [snip]

Plan: 1 to add, 0 to change, 0 to destroy.


[ec2-user@devbox tf-demo]# terraform apply
aws_vpc.tfvpc: Refreshing state... (ID: vpc-0edde0f2f198451e1)
aws_subnet.dmz: Refreshing state... (ID: subnet-01461157fed507e7b)
aws_instance.dmz_asav: Refreshing state... (ID: i-03ac772e458bb9282)
aws_instance.dmz_csr1000v: Refreshing state... (ID: i-04e2992781578b002)

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  + aws_instance.dmz_csr1000v
      id:                           <computed>
      ami:                          "ami-0d1e6af4c329efd82"
      arn:                          <computed>
      [snip]
      source_dest_check:            "true"
      subnet_id:                    "subnet-01461157fed507e7b"
      tags.%:                       "1"
      tags.Name:                    "dmz_csr1000v"
      tenancy:                      <computed>
      volume_tags.%:                <computed>
      vpc_security_group_ids.#:     <computed>


Plan: 1 to add, 0 to change, 0 to destroy.

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

aws_instance.dmz_csr1000v: Creating...
  ami:                          "" => "ami-0d1e6af4c329efd82"
  arn:                          "" => "<computed>"
  [snip]
  source_dest_check:            "" => "true"
  subnet_id:                    "" => "subnet-01461157fed507e7b"
  tags.%:                       "" => "1"
  tags.Name:                    "" => "dmz_csr1000v"
  tenancy:                      "" => "<computed>"
  volume_tags.%:                "" => "<computed>"
  vpc_security_group_ids.#:     "" => "<computed>"
aws_instance.dmz_csr1000v: Still creating... (10s elapsed)
aws_instance.dmz_csr1000v: Still creating... (20s elapsed)
aws_instance.dmz_csr1000v: Still creating... (30s elapsed)
aws_instance.dmz_csr1000v: Creation complete after 32s (ID: i-05d5bb841cf4e2ad1)

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.
\end{minted}

The new instance is currently initializing, and Terraform plan says all is well.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# aws ec2 describe-instance-status \
>  --instance-ids 'i-05d5bb841cf4e2ad1' \
>  --query InstanceStatuses[*].InstanceStatus.Status
\end{minted}

\begin{minted}{json}
[
    "initializing"
]
\end{minted}

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform plan
[snip]
No changes. Infrastructure is up-to-date.
\end{minted}

To cleanup, use \verb|terraform plan -destroy| to view a plan to remove all of the
resources added by Terraform. This is a great way to ensure no residual AWS
resources are left in place (and costing money) long after they are needed.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform plan -destroy
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

aws_vpc.tfvpc: Refreshing state... (ID: vpc-0edde0f2f198451e1)
aws_subnet.dmz: Refreshing state... (ID: subnet-01461157fed507e7b)
aws_instance.dmz_csr1000v: Refreshing state... (ID: i-05d5bb841cf4e2ad1)
aws_instance.dmz_asav: Refreshing state... (ID: i-03ac772e458bb9282)

------------------------------------------------------------------------

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  - destroy

Terraform will perform the following actions:

  - aws_instance.dmz_asav

  - aws_instance.dmz_csr1000v

  - aws_subnet.dmz

  - aws_vpc.tfvpc


Plan: 0 to add, 0 to change, 4 to destroy.
\end{minted}

The command above serves as a good preview into what \verb|terraform destroy| will
perform. Below, the infrastructure is torn down in the reverse order it was
created. Note that \verb|-auto-approve| can be appended to both \verb|apply| and
\verb|destroy| actions to remove the interactive prompt asking for \verb|yes|.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform destroy -auto-approve
aws_vpc.tfvpc: Refreshing state... (ID: vpc-0edde0f2f198451e1)
aws_subnet.dmz: Refreshing state... (ID: subnet-01461157fed507e7b)
aws_instance.dmz_csr1000v: Refreshing state... (ID: i-05d5bb841cf4e2ad1)
aws_instance.dmz_asav: Refreshing state... (ID: i-03ac772e458bb9282)
aws_instance.dmz_csr1000v: Destroying... (ID: i-05d5bb841cf4e2ad1)
aws_instance.dmz_asav: Destroying... (ID: i-03ac772e458bb9282)
aws_instance.dmz_asav: Still destroying... (ID: i-03ac772e458bb9282, 10s elapsed)
aws_instance.dmz_csr1000v: Still destroying... (ID: i-05d5bb841cf4e2ad1, 10s elapsed)
aws_instance.dmz_csr1000v: Still destroying... (ID: i-05d5bb841cf4e2ad1, 20s elapsed)
aws_instance.dmz_asav: Still destroying... (ID: i-03ac772e458bb9282, 20s elapsed)
aws_instance.dmz_asav: Still destroying... (ID: i-03ac772e458bb9282, 30s elapsed)
aws_instance.dmz_csr1000v: Still destroying... (ID: i-05d5bb841cf4e2ad1, 30s elapsed)
aws_instance.dmz_asav: Destruction complete after 40s
aws_instance.dmz_csr1000v: Still destroying... (ID: i-05d5bb841cf4e2ad1, 40s elapsed)
[snip, waiting for CSR1000v to terminate]
aws_instance.dmz_csr1000v: Still destroying... (ID: i-05d5bb841cf4e2ad1, 2m50s elapsed)
aws_instance.dmz_csr1000v: Destruction complete after 2m51s
aws_subnet.dmz: Destroying... (ID: subnet-01461157fed507e7b)
aws_subnet.dmz: Destruction complete after 1s
aws_vpc.tfvpc: Destroying... (ID: vpc-0edde0f2f198451e1)
aws_vpc.tfvpc: Destruction complete after 0s

Destroy complete! Resources: 4 destroyed.
\end{minted}

Using \verb|terraform plan -destroy| again says there is nothing left to destroy,
indicating that everything has been cleaned up. Further verification via AWS
CLI or AWS console may be desirable, but for brevity, the author excludes it here.

\begin{minted}{text}
[ec2-user@devbox tf-demo]# terraform plan -destroy
[snip]
No changes. Infrastructure is up-to-date.
\end{minted}

\subsubsection{Flask Application Monitoring with Prometheus}
Prometheus is \textit{an open-source monitoring system with a dimensional
data model, flexible query language, efficient time series database and modern
alerting approach.} To instrument a Python application, a Prometheus 
\href{https://prometheus.io/docs/instrumenting/clientlibs/}{client library}
is installed and is accessed within the application's source code. This allows
Prometheus to collect and export metrics about the application's performance,
improving observability and overall application awareness. Python Flask is
a lightweight web services framework that simplifies the creation of web
applications. Instrumenting a simple Flask application is a great way
to demonstrate Prometheus, and to do that, we'll install both the
\verb|flask| and \verb|prometheus-flask-exporter| packages.

\begin{minted}{text}
[ec2-user@devbox prom_test]# pip install flask prometheus-flask-exporter
Collecting flask
Collecting prometheus_flask_exporter
(snip)
Successfully installed flask-1.1.2 prometheus-flask-exporter-0.18.1 (snip)
\end{minted}

Prometheus offers four main metric types to monitor an application:

\begin{enumerate}
  \item \textbf{counter}: This metric counts the number of times a certain
    operation occurs, such as a function being called or an exception being
    raised. Counters can only increase and always start from 0.
  \item \textbf{gauge}: Like a counter, a gauge measures a specific numeric
    value, but can rise and fall arbitrarily. Gauges can be used to monitor
    CPU utilization, memory usage, and the number of currently executing jobs.
  \item \textbf{histogram}: Histograms are complex metric types that typically
    measure request durations or response sizes. These values are placed into
    buckets which can be viewed as a time-series, making them good candidates
    for detailed statistical analysis (beyond the scope of this document).
  \item \textbf{summary}: This metric preceded the histogram and largely behaves
    the same with some low-level technical differences regarding how quantiles
    are calculated. Prometheus published a comparison chart with more details
    \href{https://prometheus.io/docs/practices/histograms/}{here.}
\end{enumerate}

In this demo, we'll test first three metric types (summary metrics aren't
relevant for this book). The Flask application has four URLs available which
are well-commented in the code below. The application is a trivial and
function-less product ordering and fulfillment system that uses random
sleep timers to simulate complex tasks being accomplished by the system.
The \verb|@metrics.XYZ| decorator where \verb|XYZ| is the metric type is
the manner in which metrics are associated with each function. The two
positional arguments correspond to the metric's name and description.

\begin{minted}{python}
#!/usr/bin/env python

"""
Author: Nick Russo
Purpose: Trivial Flask app to demonstrate Prometheus metric types.
"""

import random
import time
from flask import Flask
from prometheus_flask_exporter import PrometheusMetrics

# Create Flask app and pass it into Prometheus for monitoring
# Individual Flask routes (HTTP resources) are decorated with metrics
app = Flask(__name__)
metrics = PrometheusMetrics(app)

@app.route("/")
def index():
    """Main page; just for connectivity testing"""
    return "OK"

@app.route("/orders")
@metrics.counter("counter_orders", "Number of orders placed")
def orders():
    """Place a new order and track using a counter (only goes up)"""
    return "Thanks for placing an order!"

@app.route("/fulfillment")
@metrics.gauge("gauge_fulfillment", "Concurrent fulfillment processes running")
def fulfillment():
    """Measure concurrent order fulfillment using a gauge (goes up and down)"""
    sleep_time = random.uniform(5.0, 10.0)
    time.sleep(sleep_time)
    return f"Last order was fulfilled in {round(sleep_time, 2)} seconds."

@app.route("/service")
@metrics.histogram("histogram_service", "Customer service wait times")
def service():
    """Measure caller wait times using a histogram (bucket-based runtimes)"""
    sleep_time = random.uniform(0.0, 11.0)
    time.sleep(sleep_time)
    return f"Customer service wait time is: {round(sleep_time, 2)} seconds."

if __name__ == "__main__":
    app.run(host="0.0.0.0")
\end{minted}

To keep things simple, a basic HTTP GET to each resource will be adequate
to generate the necessary metrics. This makes it easy to test with web
browsers, desktop tools (e.g. Postman), and CLI tools. We'll start the
web server on the devbox, then use \verb|curl| to validate connectivity
from a second machine that will soon be running the Prometheus server.

\begin{minted}{text}
[ec2-user@devbox prom_test]# python app.py
 * Debug mode: off
 * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)

[centos@prometheus ~]# curl http://devbox.njrusmc.net:5000/
OK
\end{minted}

We can also send a GET request to the \verb|/metrics| endpoint
which reveals the structured data that Prometheus interprets
(also known as ``scrapes''). There is a ton of data here, so
the author has omitted much of it in order to highlight the
most important parts. The first block are default metrics
that are exported with Flask thanks to the Python package
in use. These metrics capture request processing times along with
the HTTP method, path, and status code. For this demo, we'll focus
more on our custom measurements which were named
\verb|counter_orders|, \verb|gauge_fulfillment|, and \verb|histogram_service|.
They are separated by line breaks in the output below for cleanliness, and
some of these metrics have multiple measurements with slightly different names.

\begin{minted}{text}
[centos@prometheus ~]# curl http://devbox.njrusmc.net:5000/metrics
# HELP flask_http_request_duration_seconds Flask HTTP request duration in seconds
# TYPE flask_http_request_duration_seconds histogram
flask_http_request_duration_seconds_bucket{le="0.005",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="0.01",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="0.025",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="0.05",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="0.075",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="0.1",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="0.25",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="0.5",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="0.75",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="1.0",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="2.5",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="5.0",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="7.5",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="10.0",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_bucket{le="+Inf",method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_count{method="GET",path="/",status="200"} 1.0
flask_http_request_duration_seconds_sum{method="GET",path="/",status="200"} 0.00011417699897720013

# HELP counter_orders_total Number of orders placed
# TYPE counter_orders_total counter
counter_orders_total 0.0

# HELP gauge_fulfillment Concurrent fulfillment processes running
# TYPE gauge_fulfillment gauge
gauge_fulfillment 0.0

# HELP histogram_service Customer service wait times
# TYPE histogram_service histogram
histogram_service_bucket{le="0.005"} 0.0
histogram_service_bucket{le="0.01"} 0.0
histogram_service_bucket{le="0.025"} 0.0
histogram_service_bucket{le="0.05"} 0.0
histogram_service_bucket{le="0.075"} 0.0
histogram_service_bucket{le="0.1"} 0.0
histogram_service_bucket{le="0.25"} 0.0
histogram_service_bucket{le="0.5"} 0.0
histogram_service_bucket{le="0.75"} 0.0
histogram_service_bucket{le="1.0"} 0.0
histogram_service_bucket{le="2.5"} 0.0
histogram_service_bucket{le="5.0"} 0.0
histogram_service_bucket{le="7.5"} 0.0
histogram_service_bucket{le="10.0"} 0.0
histogram_service_bucket{le="+Inf"} 0.0
histogram_service_count 0.0
histogram_service_sum 0.0
\end{minted}

Now that we are confident that the Prometheus server and client (Flask) have
connectivity, we can start configuring Prometheus. According to the Prometheus
\href{https://prometheus.io/docs/introduction/first_steps/}{documentation},
we should create a YAML file that identifies the Prometheus targets (clients)
and how often to scrape metrics from them. We'll use a 5 second scrape and
evaluation interval, giving us relatively fast feedback regarding our
application's performance. Then, we specify the host and port information
as a static target for this simple demonstration.

\begin{minted}{text}
[centos@prometheus ~]# cat prometheus.yml
\end{minted}

\begin{minted}{yaml}
---
global:
  scrape_interval: "5s"
  evaluation_interval: "5s"

scrape_configs:
  - job_name: "etech"
    static_configs:
      - targets: ["devbox.njrusmc.net:5000"]
...
\end{minted}

Next, we can use the command below to pull and run the official container.
Note that we use a bind-mount to map our local \verb|prometheus.yml|
file to the Prometheus configuration file on the server with the same name.

\begin{minted}{text}
[centos@prometheus ~]# docker run \
  --publish 9090:9090 \
  --volume /home/centos/prometheus.yml:/etc/prometheus/prometheus.yml \
  --detach prom/prometheus
c1b35f65b70b0292e1e58e9d6ce74a155a229940231b910d6c529d0415a0d330
\end{minted}

Once Prometheus is running, we can simulate a ``high volume'' of customer
interaction using a simple Bash script. First, the script places some orders,
which is fast and runs synchronously as there is no sleep timer. Then,
the script checks fulfillment and service statuses in the background to
allow concurrent issuance of multiple requests.

\begin{minted}{text}
[centos@prometheus ~]# cat generate_activity.sh
\end{minted}

\begin{minted}{bash}
#!/bin/bash
for i in {1..5}; do
  curl http://devbox.njrusmc.net:5000/orders
done
for i in {1..3}; do
  curl http://devbox.njrusmc.net:5000/fulfillment &
done
for i in {1..20}; do
  curl http://devbox.njrusmc.net:5000/service &
done
\end{minted}

Next, we'll run the script and wait for it to finish.

\begin{minted}{text}
[centos@prometheus ~]# ./generate_activity.sh 
Thanks for placing an order!
Thanks for placing an order!
Thanks for placing an order!
Thanks for placing an order!
Thanks for placing an order!
Customer service wait time is: 0.09 seconds.
Customer service wait time is: 0.19 seconds.
Customer service wait time is: 0.81 seconds.
Customer service wait time is: 0.86 seconds.
Customer service wait time is: 1.71 seconds.
Customer service wait time is: 2.91 seconds.
Customer service wait time is: 3.11 seconds.
Customer service wait time is: 3.32 seconds.
Customer service wait time is: 3.91 seconds.
Customer service wait time is: 4.39 seconds.
Customer service wait time is: 5.17 seconds.
Customer service wait time is: 6.06 seconds.
Customer service wait time is: 6.13 seconds.
Customer service wait time is: 6.36 seconds.
Customer service wait time is: 6.78 seconds.
Last order was fulfilled in 6.96 seconds.
Customer service wait time is: 7.12 seconds.
Last order was fulfilled in 7.69 seconds.
Last order was fulfilled in 8.47 seconds.
Customer service wait time is: 8.76 seconds.
Customer service wait time is: 9.93 seconds.
Customer service wait time is: 10.92 seconds.
Customer service wait time is: 10.97 seconds.
\end{minted}

As a quick confirmation, we can use \verb|curl| again to the \verb|/metrics|
endpoint to ensure our metrics have changed. We see 5 orders and 20 service
calls, but 0 fulfillment requests. That's because the gauge only measures
point-in-time concurrent requests, which reached up to 3 at one point. That
peak should be reflected in the Prometheus UI which we'll check soon.

\begin{minted}{text}
[centos@prometheus ~]# curl http://devbox.njrusmc.net:5000/metrics
(snipped in various places)
counter_orders_total 5.0
gauge_fulfillment 0.0
histogram_service_bucket{le="0.005"} 0.0
histogram_service_bucket{le="0.01"} 0.0
histogram_service_bucket{le="0.025"} 0.0
histogram_service_bucket{le="0.05"} 0.0
histogram_service_bucket{le="0.075"} 0.0
histogram_service_bucket{le="0.1"} 1.0
histogram_service_bucket{le="0.25"} 2.0
histogram_service_bucket{le="0.5"} 2.0
histogram_service_bucket{le="0.75"} 2.0
histogram_service_bucket{le="1.0"} 4.0
histogram_service_bucket{le="2.5"} 5.0
histogram_service_bucket{le="5.0"} 10.0
histogram_service_bucket{le="7.5"} 16.0
histogram_service_bucket{le="10.0"} 18.0
histogram_service_bucket{le="+Inf"} 20.0
histogram_service_count 20.0
histogram_service_sum 99.58115865300897
\end{minted}

Next, open a web browser to the Prometheus server on port 9090 as specified
in our \verb|docker container run| command. Under ``Status'' and ``Targets'',
our devbox target is fully operational as expected. If the target is not up,
Prometheus cannot scrape metrics.

\addimg{prom-target.png}{0.8}{Prometheus Target Status}

Next, head to ``Graph'' and enter a metric to track. We'll start with
\verb|counter_orders_total| which, as the name implies, counts the
total number of orders placed. Our activity script generated 5 orders
and we see them reflected in the bottom right corner of the graphic.

\addimg{prom-count-t.png}{0.8}{Prometheus Counter Metric --- Table}

Rather than view this in table form, click the ``Graph'' tab within
the panel to see a graphical representation of the counter. In our case,
we generated 5 requests in short succession, leading to a rapid, one-time
increase in orders.

\addimg{prom-count-g.png}{0.8}{Prometheus Counter Metric --- Graph}

We can repeat the process for the gauge metric. To prove that Prometheus
really saw 3 concurrent fulfillment requests, we can set a time range during
the processing peak before the gauge decreased back to 0.

\addimg{prom-gauge-t.png}{0.8}{Prometheus Gauge Metric --- Table}

This is further proved by exploring the graph, shown below. Unlike a counter,
the gauge rose to 3 then fell to 0 once the processing was complete.

\addimg{prom-gauge-g.png}{0.8}{Prometheus Gauge Metric --- Graph}

Last, we can collect the histogram data, and the most interesting measurements
are often the completion time buckets. Each bucket is successively larger using
a ``less than'' operator, allowing operators to track performance over time and
using various tiers. The table format is shown below, indicating the number
of matches per bucket. Don't worry about the exact values quite yet.

\addimg{prom-hist-t.png}{0.8}{Prometheus Histogram Metric --- Table}

The graph view is very appealing as it shows all of the different buckets
as a stacked graph using different colors. In the context of measuring
real-life call center performance, such a chart would be quite useful.

\addimg{prom-hist-g.png}{0.8}{Prometheus Histogram Metric --- Graph}

While the Prometheus GUI is very useful for validating the collection
of metrics and building some basic visualizations, it is inadequate
for production operations by itself. Most professional organizations
prefer to export these logs to dashboard applications like
\href{https://grafana.com/}{Grafana} and
\href{https://www.elastic.co/kibana}{Kibana}, and many tutorials
exist on the precise technical steps to enable those integrations.
Additionally, a more generic Prometheus client for Python exists named
\href{https://pypi.org/project/prometheus-client}{prometheus-client}.
This package is a dependency of the 
\href{https://pypi.org/project/prometheus-flask-exporter}{prometheus-flask-exporter}
package used in this demo as the latter is Flask-specific. If you are working
in a non-Flask environment, be sure to explore the generic package.
