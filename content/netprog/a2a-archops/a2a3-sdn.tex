\subsection{Controller based network design}
Software-Defined Networking (SDN) is a concept that networks can be both
programmable and disaggregated concurrently, ultimately providing additional
flexibility, intelligence, and customization for the network administrators.
Because the definition of SDN varies so widely within the network community,
it should be thought of as a continuum of different models rather than a
single, prescriptive solution.

\subsubsection{SDN Models}
There are four main SDN models as defined in
\href{http://www.ciscopress.com/store/art-of-network-architecture-business-driven-design-9780133259230}{The Art of Network Architecture: Business-Driven Design} by
\href{https://twitter.com/rtggeek}{Russ White} and
\href{ihttps://twitter.com/LadyNetwkr}{Denise Donohue} (Cisco Press 2014).
The models are discussed briefly below.

\begin{enumerate}
  \item \textbf{Distributed:} Although not really an ``SDN'' model at all, it
  is important to understand the status quo. Network devices today each have
  their own control-plane components which rely on distributed routing
  protocols (such as OSPF, BGP, etc). These protocols form paths in the
  network between all relevant endpoints (IP prefixes, etc). Devices typically
  do not influence one another’s routing decisions individually as traffic is
  routed hop-by-hop through the network without centralized oversight. This
  model totally distributes the control-plane across all devices. Such
  control-planes are also autonomous; with minimal administrative effort, they
  often form neighborships and advertise topology and/or reachability
  information. Some of the drawbacks include potential routing loops
  (typically transient ones during periods of convergence) and complex routing
  schemes in poorly designed/implemented networks. The diagram that follows depicts
  several routers each with their own control-plane and no centralization.

  \addimg{sdn-distributed.jpg}{0.7}{SDN Model --- Distributed}

  \item \textbf{Augmented:} This model relies on a fully distributed
  control-plane by adding a centralized controller that can apply policy to
  parts of the network at will. Such a controller could inject shorter-match
  IP prefixes, policy-based routing (PBR), security features (ACL), or other
  policy objects. This model is a good compromise between distributing
  intelligence between nodes to prevent singles points of failure (which a
  controller introduces) by using a known-good distributed control-plane
  underneath. The policy injection only happens when it ``needs to'', such as
  offloading traffic from an overloaded link in a DC fabric or traffic from a
  long-haul fiber link between two points of presence (POPs) in an SP core.
  Cisco’s Performance Routing (PfR) is an example of the augmented model which
  uses the Master Controller (MC) to push policy onto remote forwarding nodes.
  Another example includes offline path computation element (PCE) servers for
  automated MPLS TE tunnel creation. In both cases, a small set of routers
  (PfR border routers or TE tunnel head-ends) are modified, yet the remaining
  routers are untouched. This model has a lower impact on the existing network
  because the wholesale failure of the controller simply returns the network
  to the distributed model, which is a viable solution for many businses
  cases. The diagram that follows depicts the augmented SDN model.

  \addimg{sdn-augmented.jpg}{0.7}{SDN Model --- Augmented}

  \item \textbf{Hybrid:} This model is very similar to the augmented model
  except that controller-originated policy can be imposed anywhere in the
  network. This gives additional granularity to network administrators; the
  main benefit over the augmented model is that the hybrid model is always
  topology-independent. The topological restrictions of which nodes the
  controller can program/affect are not present in this model. Cisco’s
  Application Centric Infrastructure (ACI) is a good example of this model.
  ACI separates reachability from policy, which is critical from both
  survivability and scalability perspectives. This solution uses the
  Application Policy Infrastructure Controller (APIC) as the policy
  application tool. The failure of the centralized controller in these models
  has an identical effect to that of a controller in the augmented model; the
  network falls back to a distributed model. The impact of a failed controller
  is a more significant since more devices are affected by the controller’s
  policy when compared to the augmented model. The diagram that follows
  depicts the augmented SDN model.

  \addimg{sdn-hybrid.jpg}{0.7}{SDN Model --- Hybrid}

  \item \textbf{Centralized:} This is the model most commonly referenced when
  the phrase ``SDN'' is used. It relies on a single controller, which hosts
  the entire control-plane. Ultimately, this device commands all of the
  devices in the forwarding-plane. These controllers push their forwarding
  tables with the proper information (which doesn’t necessarily have to be an
  IP-based table, it could be anything) to the forwarding hardware as
  specified by the administrators. This offers very granular control, in many
  cases, of individual flows in the network. The hardware forwarders can be
  commoditized into white boxes (or branded white boxes, sometimes called
  brite boxes) which are often inexpensive. Another value proposition of
  centralizing the control-plane is that a ``device'' can be almost anything:
  router, switch, firewall, load-balancer, etc. Emulating software functions
  on generic hardware platforms can add flexibility to the business.
  
  The most significant drawback is the newly-introduced single point of
  failure and the inability to create failure domains as a result. Some SDN
  scaling architectures suggest simply adding additional controllers for fault
  tolerance or to create a hierarchy of controllers for larger networks. While
  this is a valid technique, it somewhat invalidates the ``centralized'' model
  because with multiple controllers, the distributed control-plane is reborn.
  The controllers still must synchronize their configuration, and in some
  cases, state information such as routing and flow data. This occurs via a
  network-based protocol and the possibility of inconsistencies between the
  controllers is real. When using this multi-controller architecture, the
  network designer must understand that there is, in fact, a distributed
  control-plane in the network; it has just been moved around. The failure of
  all controllers means the entire failure domain supported by those
  controllers will be at the very best static (unable to adjust to changes),
  and at the very worst inoperable. The failure of the communication paths
  between controllers could likewise cause inconsistent/intermittent problems
  with forwarding, just like a fully distributed control-plane. OpenFlow is a
  good example of a fully-centralized model. Nodes colored gray in the diagram
  that follows have no standalone control plane of their own, relying
  entirely on the controller.

  \addimg{sdn-centralized.jpg}{0.7}{SDN Model --- Centralized}
\end{enumerate}

These SDN designs warrant additional discussion, specifically around the
communications channels that allow them to function. An SDN controller sits
``in the middle'' of the SDN notional architecture. It uses \textbf{northbound}
and \textbf{southbound} communication paths to operate with other components
of the architecture.

The \textbf{northbound} interfaces are considered APIs which are interfaces to existing
business applications. This is generally used so that applications can make
requests of the network, which could include specific performance requirements
(bandwidth, latency, etc). Because the controller ``knows'' this information
by communicating with the infrastructure devices via management agents, it can
determine the best paths through the network to satisfy these constraints.
This is loosely analogous to the original intent of the Integrated Services
QoS model using Resource Reservation Protocol (RSVP) where applications would
reserve bandwidth on a per-flow basis. It is also similar to MPLS TE
constrained SPF (CSPF) where a single device can source-route traffic through
the network given a set of requirements. The logic is being extended to
applications with a controller ``shim'' in between, ultimately providing a
full network view for optimal routing. A REST API is an example of a
northbound interface.

The \textbf{southbound} interfaces include the control-plane protocol between the
centralized controller and the network forwarding hardware. These are the less
intelligent network devices used for forwarding only (assuming a centralized
model). A common control-plane used for this purpose would be OpenFlow; the
controller determines the forwarding tables per flow per network device,
programs this information, and then the devices obey it. Note that OpenFlow is
not synonymous with SDN\@; it is just an example of one southbound control-plane
protocol. Because the SDN controller is sandwiched between the northbound and
southbound interfaces, it can be considered ``middleware'' in a sense. The
controller is effectively able to evaluate application constraints and produce
forwarding-table outputs.

The image that follows depicts a very high-level diagram of the SDN layers as
it relates to interaction between components.

\addimg{sdn-high-level.png}{0.5}{SDN Communications Channels}

There are many trade-offs between the different SDN models. The table that follows
attempts to capture the most important ones. Looking at the SDN market at the
time of this writing, many solutions seem to be either hybrid or augmented
models. SD-WAN solutions, such as Cisco Viptela, only make changes at the edge
of the network and use overlays/tunnels as the primary mechanism to implement policy.

\begin{longtable}{LLLLL}
\toprule
% top left cell is blank
&
\textbf{Distributed}
&
\textbf{Augmented}
&
\textbf{Hybrid}
&
\textbf{Centralized}
\\ \midrule
\textbf{Availability}
&
Dependent on the protocol convergence times and redundancy in the network.
Highly automonous and heals itself without a central brain
&
Dependent on the protocol convergence times and redundancy in the network.
Doesnt matter how bad the SDN controller is its failure is tolerable
&
Dependent on the protocol convergence times and redundancy in the network.
Doesnt matter how bad the SDN controller is  its failure is tolerable
&
Heavily reliant on a single SDN controller, unless one adds controllers to
split failure domains or to make one failure domain resilient
(both introduce a distributed control-plane)
\\ \midrule
\textbf{Granularity / control}
&
Generally low for IGPs but better for BGP\@. All devices generally need a common
view of the network to prevent loops independently. MPLS TE helps somewhat.
&
Better than distributed since policy injection can happen at the network edge,
or a small set of nodes. Can be combined with MPLS TE for more granular selection.
&
Moderately granular since SDN policy decisions are extended to all nodes. Can
influence decisions based on any arbitrary information within a datagram
&
Very highly granular; complete control over all routing decisions based on any
arbitrary information within a datagram
\\ \midrule
\textbf{Scalability}
&
Very high in a properly designed network (failure domain isolation, topology
summarization, reachability aggregation, etc)
&
High, but gets worse with more policy injection. Policies are generally
limited to key nodes (such as border routers)
&
Moderate, but gets worse with more policy injection. Policy is proliferated
across the network to all nodes (exact quantity may vary per node)
&
Depends; all devices retain state for all transiting flows. Hardware-dependent
on TCAM and whether it can use other tables such as L4 ports or IPv6 flow
labels
\\
\bottomrule
\end{longtable}

\subsubsection{Centralized SDN using OpenFlow and Faucet}
When people think of ``real'' SDN (which often implies a fully centralized,
open-standards technology suite), OpenFlow is commonly used as the
southbound protocol between the SDN controller and the network devices.
This section demonstrates how to use the \href{https://faucet.nz/}{Faucet}
controller, built on the \href{https://osrg.github.io/ryu-book/en/html/}{Ryu}
framework, and the \href{https://www.openvswitch.org/}{Open vSwitch (OVS)}
device, a Linux-based network device that is lightweight and easy to deploy.

The topology used in this demonstration is shown below. OVS will interconnect
two standard Cisco IOS routers which are managed autonomously. Behind
each router is a simulated LAN segment with ``users'' behind R1 and
``servers'' behind R2. The lab uses IPv6 only with OSPFv3 routing
enabled between R1 and R2 across OVS\@. The OVS
\href{https://gns3.com/marketplace/appliances/open-vswitch}{GNS3 appliance}
runs inside Docker (hidden from GNS3 users) and connects over the Internet
to a Faucet instance running in the cloud. One could run Faucet within
GNS3 on a Linux machine, but testing it in the cloud improved
accessibility and helped tie in additional evolving technologies.

\addimg{faucet-topo.png}{0.7}{OpenFlow Testbed Topology in GNS3}

Configuring OVS for the first time is a bit tricky because the
exact commands required depend upon the initial OVS image. The
GNS3 appliance comes with 4 Linux bridges, each of which is effectively
its own virtual switch. All 16 ports are initially in the \verb|br0| bridge.
Each port has a number which is dynamically assigned by OVS during
initialization; these numbers become important later. The output
below shows that \verb|eth0| is port 1, \verb|eth1| is port 2,
and \verb|eth2| is port 3. According to the topology, \verb|eth0| will
be used for OpenFlow management back to Faucet, while \verb|eth1| and
\verb|eth2| will connect to R1 and R2, respectively. The OVS version
is also included below for completeness.

\begin{minted}{text}
/ # ovs-vsctl -V
ovs-vsctl (Open vSwitch) 2.4.0
Compiled Apr  6 2016 14:08:48
DB Schema 7.12.1

/ # ovs-ofctl show br0
OFPT_FEATURES_REPLY (xid=0x2): dpid:0000a2bbc9e0024f
n_tables:254, n_buffers:256
capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP
actions: output enqueue set_vlan_vid set_vlan_pcp strip_vlan mod_dl_src 
         mod_dl_dst mod_nw_src mod_nw_dst mod_nw_tos mod_tp_src mod_tp_dst
 1(eth0): addr:8a:1d:41:af:df:a4
     config:     0
     state:      0
     current:    10MB-FD COPPER
     speed: 10 Mbps now, 0 Mbps max
 2(eth1): addr:ce:d2:75:9f:f1:ed
     config:     0
     state:      0
     current:    10MB-FD COPPER
     speed: 10 Mbps now, 0 Mbps max
 3(eth2): addr:ce:26:d0:13:7a:7d
     config:     0
     state:      0
     current:    10MB-FD COPPER
     speed: 10 Mbps now, 0 Mbps max
 (snip, more interfaces)
\end{minted}

Before continuing with the OVS setup, the author recommends completing
the Faucet setup. This document does not recite every Faucet setup command
as they are well-documented in their Installing Faucet for the
\href{https://docs.faucet.nz/en/latest/tutorials/first_time.html}{first time}
tutorial. The packages, which require a Debian-based distribution, also
include Gauge (for measuring port statistics), Prometheus (for measuring
system health), and Grafana (for visualizing the results of both). The
remainder of this demonstration assumes that the basic setup steps
have been completed.

Now that we have the interface numbers, we can populate the main
\verb|/etc/faucet/faucet.yaml| Faucet configuration file. The file
includes another file named \verb|acls.yaml| which we'll explore shortly.
Next, it defines a single VLAN named \verb|transport| which uses an 802.1Q
VLAN ID of 12. This VLAN interconnects R1 and R2 across OVS\@.

Then, the file defines the OpenFlow datapaths, or DPs for short. A datapath
is a 64-bit field whereby the low-order 48 bits are usually used for the switch
MAC address and the high-order 16 bits can be anything. This demo uses the
number 1 to keep things simple, but in short, each DP represents an OpenFlow
instance on a device. That means each Linux bridge interface would have
exactly one DP assigned. We'll need to configure a DPID of 1 on our OVS
\verb|br0| bridge later.

After specifying the correct hardware type for our \verb|sw1| device, we
enumerate the interfaces. YAML comments are included to map the output
from the \verb|ovs-ofctl show br0| command issued earlier, which revealed
the interface numbers. These numbers are the dictionary keys 
and must match the OVS numbers we recorded. In addition to a
self-explanatory name and description for each device, we specify the
\verb|native_vlan| which is equivalent to the Cisco IOS command of
\verb|switchport access vlan|. OVS will expect untagged Ethernet frames
on these ports and will internally map them to VLAN 12 per our configuration.

\begin{minted}{text}
root@faucet:/etc/faucet# cat faucet.yaml
\end{minted}

\begin{minted}{yaml}
---
include:
  - "acls.yaml"

vlans:
  transport:
    vid: 12
    description: "R1-R2 link"

dps:
  sw1:
    dp_id: 0x1  # datapath-id=0000000000000001
    hardware: "Open vSwitch"
    interfaces:
      2:  #  2(eth1): addr:ce:d2:75:9f:f1:ed
        name: "R1"
        description: "User Gateway"
        native_vlan: "transport"
        acl_in: "R1_INBOUND"
      3:  #  3(eth2): addr:ce:26:d0:13:7a:7d
        name: "R2"
        description: "Server Gateway"
        native_vlan: "transport"
...
\end{minted}

By itself, the configuration file above is adequate to establish connectivity
between R1 and R2. However, the promise of OpenFlow is that it can provide
more granular policy matching rather than just basic VLAN management. The
\verb|/etc/faucet/acls.yaml| file, which was included in the
\verb|/etc/faucet/faucet.yaml| file, allows us to specify access-control
lists (ACLs) to restrict traffic flow much like a traditional router would.

A single ACL is defined which is named \verb|R1_INBOUND| and has three rules.
The ACL is a list of nested dictionaries and is relatively easy to interpret.
First, all IPv4 traffic will be dropped by matching the IPv4 Ethertype.
Second, IPv6 Telnet is blocked from users to servers, presumably because
Telnet is insecure and we don't want it on the network at all. This complex
rule matches the IPv6 Ethertype, TCP protocol, Telnet port, and IPv6
source/destination ranges. The final rule permits all remaining traffic,
matching nothing in particular.

\begin{minted}{text}
root@faucet:/etc/faucet# cat acls.yaml
\end{minted}

\begin{minted}{yaml}
---
acls:
  R1_INBOUND:
    # Deny all IPv4 traffic (because we are 'next-gen')
    - rule:
        dl_type: 0x0800
        actions:
          allow: false

    # Prevent users from using IPv6 Telnet to reach servers
    - rule:
        dl_type: 0x86dd
        nw_proto: 6
        tcp_dst: 23
        ipv6_src: "2001:db8:1::/64"  # R1 Users
        ipv6_dst: "2001:db8:2::/64"  # R2 Servers
        actions:
          allow: false

    # Permit all other traffic
    - rule:
        actions:
          allow: true
...
\end{minted}

After making these changes, you must restart both Faucet and Gauge,
in that order, for changes to take effect. If OpenFlow devices have
already established connectivity to Faucet, the SDN controller is smart
enough to update the devices with only the minimum number of rules to
implement the desired policy. This is useful because it preserves
computing resources on the OpenFlow device while also minimizing any
micro-outages that occur during the rule reprogramming.

\begin{minted}{text}
root@faucet:/etc/faucet# systemctl restart faucet
root@faucet:/etc/faucet# systemctl restart gauge
\end{minted}

As a final step, be sure to record Faucet's IPv4 address so that OVS can
connect to it. Since our Faucet is running in AWS as an EC2 instance,
it has an elastic IP associated with it, and we can use \verb|curl| to
discover Faucet's public IP\@.

\begin{minted}{text}
root@faucet:/etc/faucet# curl https://api64.ipify.org
34.207.175.9
\end{minted}

To establish an OpenFlow connection to Faucet, OVS needs an IPv4 address.
GNS3 allows you to pre-configure the \verb|eth0| configuration by hand-editing
the \verb|/etc/network/interfaces| file as you'd normally do. Simply uncomment
the lines below to enable DHCP, allowing the internal GNS3 DHCP server to
issue an IPv4 address to OVS\@. GNS3 can also provide NAT service to OVS so
that it can reach the Internet.

\begin{minted}{text}
/ # grep eth0 /etc/network/interfaces
auto eth0
iface eth0 inet dhcp
\end{minted}

To confirm that OVS has connectivity to Faucet, perform a ping test before
trying to establish an OpenFlow connection. This will help isolate a basic
IP addressing or routing problem from an OpenFlow-specific problem.

\begin{minted}{text}
/ # ping -c 3 34.207.175.9
PING 34.207.175.9 (34.207.175.9): 56 data bytes
64 bytes from 34.207.175.9: seq=0 ttl=128 time=12.072 ms
64 bytes from 34.207.175.9: seq=1 ttl=128 time=63.384 ms
64 bytes from 34.207.175.9: seq=2 ttl=128 time=90.134 ms

--- 34.207.175.9 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 12.072/55.196/90.134 ms
\end{minted}

Next, configure OVS so that it is able to connect to Faucet. The
commands below are individually commented to explain what they do.
Most importantly, note that the \verb|br0| bridge is configured
with DPID 1 to match the Faucet configuration. This is how OVS
identifies itself to Faucet.

\begin{minted}{text}
# Remove eth0 from br0; used for connection to faucet
ovs-vsctl del-port br0 eth0

# Delete unnecessary bridges; created with GNS3 OVS appliance
ovs-vsctl del-br br1
ovs-vsctl del-br br2
ovs-vsctl del-br br3

# Identify DP ID; must match "dp_id: 0x1" in faucet.yaml
ovs-vsctl set bridge br0 other-config:datapath-id=0000000000000001

# Management should only be out-of band for cleanliness
ovs-vsctl set bridge br0 other-config:disable-in-band=true

# If controllers fail, OVS stops forwarding (no autonomous failover)
ovs-vsctl set bridge br0 fail_mode=secure

# TCP 6653 to faucet; actual SDN control and policy application
# TCP 6654 to gauge; read-only for metric collection, does not apply policy
ovs-vsctl set-controller br0 tcp:34.207.175.9:6653 tcp:34.207.175.9:6654
\end{minted}

None of the commands above provide any output (unless they fail). To
verify OpenFlow connectivity, use the \verb|ovs-vsctl show| command.
The command does not reveal everything, but does confirm that
the OpenFlow connections to Faucet (6653) and Gauge (6654) are functional.

\begin{minted}{text}
/ # ovs-vsctl show
5fbd00bd-6899-49ab-bb89-38f247ac6b6b
    Bridge "br0"
        Controller "tcp:34.207.175.9:6654"
            is_connected: true
        Controller "tcp:34.207.175.9:6653"
            is_connected: true
        fail_mode: secure
        Port "br0"
            Interface "br0"
                type: internal
        Port "eth1"
            Interface "eth1"
        Port "eth2"
            Interface "eth2"
        (snip, more interfaces)
\end{minted}

Everything should be working at this point. We can assume that the OpenFlow
controller has sent down the proper ruleset to OVS once the OSPFv3 neighbors
formed between R1 and R2 as shown below. Both routers have OSPFv3-learned IPv6
routes to one another's Loopback (LAN simulation) networks, as expected.

\begin{minted}{text}
R1#show ospfv3 ipv6 neighbor | begin ^Neighbor
Neighbor ID     Pri   State           Dead Time   Interface ID    Interface
10.1.2.2          0   FULL/  -        00:00:33    5               Ethernet0/1

R1#show ipv6 route ospf | begin ^O
O   2001:DB8:2::2/128 [110/10]
     via FE80::2, Ethernet0/1


R2#show ospfv3 ipv6 neighbor | begin ^Neighbor
Neighbor ID     Pri   State           Dead Time   Interface ID    Interface
10.1.2.1          0   FULL/  -        00:00:33    4               Ethernet0/2

R2#show ipv6 route ospf | begin ^O
O   2001:DB8:1::1/128 [110/10]
     via FE80::1, Ethernet0/2
\end{minted}

Next, let's ensure that the ACL works correctly. Trying to ping R2's
IPv4 address across OVS fails, even though the ARP resolution succeeds.
This is a clear indication that IPv4 ARP traffic (Ethernet \verb|0x0806|) is
permitted while actual IPv4 traffic (Ethernet \verb|0x0800|) is not. In a real
IPv6-only environment, you'd probably block IPv4 ARP as well. The author
left it enabled just to highlight how granular the ACL ruleset can be.

\begin{minted}{text}
R1#ping 10.1.2.2
Type escape sequence to abort.
Sending 5, 100-byte ICMP Echos to 10.1.2.2, timeout is 2 seconds:
.....
Success rate is 0 percent (0/5)

R1#show arp
Protocol  Address          Age (min)  Hardware Addr   Type   Interface
Internet  10.1.2.1                -   aabb.cc00.0110  ARPA   Ethernet0/1
Internet  10.1.2.2                1   aabb.cc00.0220  ARPA   Ethernet0/1
\end{minted}

Next, let's attempt to Telnet from a simulated user to a simulated server
using IPv6. We expect this to time out as it is dropped by the ACL\@. On
the other hand, OSPFv3 is already working between R1 and R2 link-local
addresses, so clearly IPv6 is not blocked entirely. Ping traffic between
R1 and R2 loopbacks works as expected while Telnet is blocked.

\begin{minted}{text}
R1#telnet 2001:db8:2::2 /source-interface Loopback0
Trying 2001:DB8:2::2 ...
> Connection timed out; remote host not responding

R1#ping 2001:db8:2::2 source Loopback0
Type escape sequence to abort.
Sending 5, 100-byte ICMP Echos to 2001:DB8:2::2, timeout is 2 seconds:
Packet sent with a source address of 2001:DB8:1::1
!!!!!
Success rate is 100 percent (5/5), round-trip min/avg/max = 1/5/7 ms
\end{minted}

When troubleshooting OpenFlow, OVS provides many useful (but hard to read)
commands. You can examine the flow table using \verb|ovs-ofctl dump-flows br0|
as shown below. For readability, the author has deleted many low-relevance fields,
such as the hexademical cookie, table ID, and more. These are important, low-level
details but are beyond the scope of this book. Individual flow entries have been
been spread across multiple lines for additional brevity. The table is ordered
by priority with higher values taking precedence, which means more specific
ACL entries are correctly processed first. The first two entries have logged
the dropped IPv4 and IPv6 Telnet attempts. Then, there is an entry that matches
all IPv6 multicast traffic with a destination MAC address beginning with 33:33.
Traffic with this destination will have its VLAN stripped and flooded out
of ports 2 and 3, which correspond to \verb|eth1| (to R1) and \verb|eth2|
(to R2), respectively. The final flow with the lowest priority permits all
other traffic.

\begin{minted}{text}
/ # ovs-ofctl dump-flows br0

# Drops all IPv4
n_packets=5, n_bytes=570, idle_age=972,priority=20480,ip,in_port=2
actions=drop

# Drops IPv6 Telnet from user to server VLAN
n_packets=2, n_bytes=156, idle_age=888, priority=20479,
tcp6,in_port=2,ipv6_src=2001:db8:1::/64,ipv6_dst=2001:db8:2::/64,tp_dst=23
actions=drop

# Allows IPv6 multicast (dynamic entry; there are more like this)
n_packets=245, n_bytes=23038, idle_age=5,priority=8208,
dl_vlan=12,dl_dst=33:33:00:00:00:00/ff:ff:00:00:00:00
actions=strip_vlan,output:2,output:3

# Permit all other traffic
n_packets=4, n_bytes=308, idle_age=522,priority=8192,dl_vlan=12
actions=strip_vlan,output:2,output:3
\end{minted}

As mentioned earlier, the Faucet installation comes with Grafana as well.
This is used to display metrics collected by Prometheus, which monitors
the local controller, and Gauge, which collects port statistics from
remote OpenFlow devices. The screenshow below is an example of a
dashboard used to track connected OpenFlow devices. Our demonstrated
used a single OVS instance named ``sw1'' with a DPID of 1.

\addimg{faucet-inv.png}{0.8}{Grafana Inventory Dashboard}

Faucet also supplies a pre-made dashboard for reviewing Gauge-collected
port statistics. The image below shows the traffic across OVS in a relatively
steady state. Because Faucet and OVS do not have GUIs by which they can
be managed, Grafana serves as a useful substitute for performance monitoring.

\addimg{faucet-grafana.png}{0.8}{Grafana Port Statistics Dashboard}

The ``first time'' Faucet tutorial discussed earlier provides links to these
dashboards for those interested in exploring them.
