\subsection{Configuration management tools and version control systems}
This section discussions a variety of configuration management tools, typically
ones that enable ``infrastructure as code''. It also contains specific
version control systems with a high level comparison to help coders decide
which is best for them.

% CM tools
\subsubsection{Agent-based Summary}
Management agents are typically on-box, add-on software components that allow
an automation, orchestration, or monitoring tool to communicate with the
managed device. The agent exposes an API that would have otherwise not been
available. On the topic of monitoring, the agents allow the device to report
traffic conditions back to the controller (telemetry). Given this information,
the controller can sense (or, with analytics, predict) congestion, route
around failures, and perform all manner of fancy traffic-engineering as
required by the business applications. Many of these agents perform the same
general function as SNMP yet offer increased flexibility and granularity as
they are programmable.

Agents could also be used for non-management purposes. Interface to the
Routing System (I2RS) is an SDN technique where a specific control-plane agent
is required on every data-plane forwarder. This agent is effectively the
control-plane client that communicates northbound towards the controller. This
is the channel by which the controller consults its RIB and populates the FIB
of the forwarding devices. The same is true for OpenFlow (OF) which is a fully
centralized SDN model. The agent can be considered an interface to a
data-plane forwarder for a control-plane SDN controller.

A simple categorization method is to quantify management strategies as ``agent
based'' or ``agent-less based''. Agent is pull-based, which means the agent
connects with master. Changes made on master are pulled down when agent is
``ready''. This can be significant since if a network device is currently
experiencing a microburst, the management agent can wait until the contention
abates before passing telemetry data to the master. Agent-less is push-based
like SNMP traps, where the triggering of an event on a network device creates
a message for the controller in unsolicited fashion. The other direction also
holds true; a master can use SSH to access a device for programming whenever
the master is ``ready''.

Although not specific to ``agents'', there are several common
applications/frameworks that are used for automation. Some of them rely on
agents while others do not. Three of them are discussed briefly below as these
are found in Cisco’s NX-OS DevNet Network Automation Guide. Note that subsets
of the exact definitions are added here. Since these are third-party products,
the author does not want to misrepresent the facts or capabilities as
understood by Cisco.

\begin{enumerate}
  \item \textbf{Puppet (by Puppet Labs):} The Puppet software package is an
  open source automation toolset for managing servers and other resources by
  enforcing device states, such as configuration settings. Puppet components
  include a puppet agent which runs on the managed device (client) and a
  puppet master (server) that typically runs on a separate dedicated server
  and serves multiple devices. The Puppet master compiles and sends a
  configuration manifest to the agent. The agent reconciles this manifest with
  the current state of the node and updates state based on differences. A
  puppet manifest is a collection of property definitions for setting the
  state on the device. Manifests are commonly used for defining configuration
  settings, but they can also be used to install software packages, copy
  files, and start services.
  
  In summary, Puppet is agent-based (requiring software installed on the
  client) and pushes complex data structures to managed nodes from the master
  server. Puppet manifests are used as data structures to track node state and
  display this state to the network operators. Puppet is not commonly used for
  managing Cisco devices as most Cisco products, at the time of this writing,
  do not support the Puppet agent. The follow products support Puppet today:

  \begin{enumerate}
    \item Cisco Nexus 7000 and 7700 switches running NX-OS \verb|7.3(0)D1(1)| or later
    \item Cisco Nexus 9300, 9500, 3100, and 300 switches running NX-OS
 	\verb|7.3(0)I2(1)| or later
    \item Cisco Network Convergence System (NCS) 5500 running IOS-XR \verb|6.0| or later
    \item Cisco ASR9000 routers running IOS-XR \verb|6.0| or later
  \end{enumerate}

  \item \textbf{Chef (by Chef Software):} Chef is a systems and cloud
  infrastructure automation framework that deploys servers and applications to
  any physical, virtual, or cloud location, no matter the size of the
  infrastructure. Each organization is comprised of one or more workstations,
  a single server, and every node that will be configured and maintained by
  the chef-client. A cookbook defines a scenario and contains everything that
  is required to support that scenario, including libraries, recipes, files,
  and more. A Chef recipe is a collection of property definitions for setting
  state on the device. While recipes are commonly used for defining
  configuration settings, they can also be used to install software packages,
  copy files, start services, and more.

  In summary, Chef is very similar to Puppet in that it requires agents and
  manages devices using complex data structures. The concepts of cookbooks and
  recipes are specific to Chef (hence the name) which contribute to a
  hierarchical data structure management system. A Chef cookbook is loosely
  equivalent to a Puppet manifest. Like Puppet, Chef is not commonly used to
  manage Cisco devices due to requiring the installation of an agent. Below is
  a list of supported platforms that support being managed by Chef:

  \begin{enumerate}
    \item Cisco Nexus 7000 and 7700 switches running NX-OS \verb|7.3(0)D1(1)| or later
    \item Cisco Nexus 9300, 9500, 3100, and 300 switches running NX-OS
 	\verb|7.3(0)I2(1)| or later
    \item Cisco Network Convergence System (NCS) 5500 running IOS-XR \verb|6.0| or later
    \item Cisco ASR9000 routers running IOS-XR \verb|6.0| or later
  \end{enumerate}
\end{enumerate}

\subsubsection{Agent-less Summary}
The concept of agent-less software was briefly discussed in the previous
section. Simply put, no special client-side software is needed on the managed
entity. This typically makes agent-less solutions faster to deploy and easier
to learn. The main drawback is their limited power and often lack of
visibility, but since many network devices deployed in production today do not
support modern APIs (especially in small/medium businesses), agent-less
solutions can be quite popular. This section focuses on Ansible, a common task
execution engine for network devices.

\begin{enumerate}
  \item \textbf{Ansible (by Red Hat):} Ansible is an open source IT
  configuration management and automation tool. Unlike Puppet and Chef,
  Ansible is agent-less, and does not require a software agent to be installed
  on the target node (server or switch) in order to automate the device. By
  default, Ansible requires SSH and Python support on the target node, but
  Ansible can also be easily extended to use any API\@. Ansible
  operators
  write most of their code in YAML, a format discussed earlier in the book.
  \item \textbf{Nornir (community product):} Nornir has quite a lot in
  common with Ansible at a conceptual level. It's open source and agent-less,
  based on Python, and doesn't usually require special software on its
  managed targets. Nornir runbooks are written in Python. Unlike many
  other CM tools, Nornir was written primarily for network automation.
\end{enumerate}

In summary, agent-less tools tend to be lighter-weight than their agent-based
counterparts. No custom software needs to be installed on any
device provided that it supports SSH\@. This can be a drawback since individual
device CLIs must be exposed to network operators (or, at best, the agent-less
automation engine) instead of using a more abstract API design. Ansible is
very commonly used to manage Cisco network devices as it requires no agent
installation on the managed devices. Nornir is rapidly gaining popularity, too.
Any Cisco device that can be accessed using SSH can be managed by these
agent-less tools. This includes Cisco ASA firewalls, older Cisco ISRs, and
older Cisco Catalyst switches.

\subsubsection{Agent-less Demonstration with Ansible (SSH/CLI)}
The author has deployed Ansible in production and is most familiar with
Ansible when compared against Puppet or Chef. This section will illustrate the
value of automation using a simple but powerful playbook. These tests were
conducted on a Linux machine in Amazon Web Services (AWS) which was targeting
a Cisco CSR1000v. Before beginning, all of the relevant version information is
shown below for reference.

\begin{minted}{text}
RTR_CSR1#show version | include RELEASE  
Cisco IOS Software, CSR1000V Software (X86_64_LINUX_IOSD-UNIVERSALK9-M),
  Version 15.5(3)S4a, RELEASE SOFTWARE (fc1)

[ec2-user@devbox ansible]# uname -a
Linux ip-10-125-0-100.ec2.internal 3.10.0-514.16.1.el7.x86_64 #1 SMP
  Fri Mar 10 13:12:32 EST 2017 x86_64 x86_64 x86_64 GNU/Linux

[ec2-user@devbox ansible]# ansible-playbook --version
ansible-playbook 2.3.0.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.5 (default, Aug  2 2016, 04:20:16) [GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]
\end{minted}

Ansible playbooks are collections of plays. Each play targets a specific set
of hosts and contains a list of tasks. In YAML, arrays/lists are denoted with
a hyphen (\verb|-|) character. The first play in the playbook begins with a hyphen
since it’s the first element in the array of plays. The play has a name,
target hosts, and some other minor options. Gathering facts can provide basic
information like time and date, which are used in this script. When
connection: local is used, the python commands used with Ansible are executed
on the control machine (Linux) and not on the target. This is required for
many Cisco devices being managed by the CLI\@.

The first task defines a credentials dictionary. This contains transport
information like SSH port (default is 22), target host, username, and
password. The \verb|ios_config| and \verb|ios_command| modules, for example,
require this to log into the device. The second task uses the
\verb|ios_config| module to issue specific commands. The commands will specify
the SNMPv3 user/group and update the auth/priv passwords for that user. For
accountability reasons, a timestamp is written to the configuration as well
using the ``facts'' gathered earlier in the play. Minor options to the
\verb|ios_config| module, such as \verb|save_when: always| and match: none are
optional. The first option saves the configuration after the commands are
issued while the second does not care about what the router already has in its
configuration. The commands in the task will forcibly overwrite whatever is
already configured; this is not typically done in production, but is done to
illustrate a simple example. The \verb|changed_when: false| option tells
Ansible to always report a status of \verb|ok| rather than \verb|changed|
which makes the script ``succeed'' from an operations perspective. The \verb|>|
operator is used in YAML to denote folded text for readability, and the input
is assumed to always be a string. This particular example is not idempotent.
\textbf{Idempotent} is a term used to describe the behavior of only making the
necessary changes. This implies that when no changes need to be made, the tool
does nothing. Although considered a best practice, achieving idempotence is
not a prerequisite for creating effective Ansible playbooks.

\begin{minted}{text}
[ec2-user@devbox ansible]# cat snmp.yml 
\end{minted}

\begin{minted}{yaml}
---
- name: "Updating SNMPv3 pre-shared keys"
  hosts: csr1
  gather_facts: true
  connection: local
  tasks:
    - name: "SYS >> Define router credentials"
      set_fact:
        CREDENTIALS:
          host: "{{ inventory_hostname }}" 
          username: "ansible"
          password: "ansible"

    - name: "IOS >> Issue commands to update SNMPv3 passwords, save config"
      ios_config:
        provider: "{{ CREDENTIALS }}"
        commands:
          - >
            snmp-server user {{ snmp.user }} {{ snmp.group }} v3 auth
            sha {{ snmp.authpass }} priv aes 256 {{ snmp.privpass }}
          - >
            snmp-server contact PASSWORDS UPDATED
            {{ ansible_date_time.date }} at {{ ansible_date_time.time }}
        save_when: always
        match: none
      changed_when: false
...
\end{minted}

The playbook above makes a number of assumptions that have not been reconciled
yet. First, one should verify that \verb|csr1| is defined and reachable. It is
configured as a static hostname-to-IP mapping in the system hosts file.
Additionally, it is defined in the Ansible hosts file as a valid host. Last,
it is valuable to ping the host to ensure that it is powered on and responding
over the network. The verification for all aforementioned steps is below.

\begin{minted}{text}
[ec2-user@devbox ansible]# grep csr1 /etc/hosts
10.125.1.11 csr1

[ec2-user@devbox ansible]# grep csr1 /etc/ansible/hosts
csr1

[ec2-user@devbox ansible]# ping csr1 -c 3
PING csr1 (10.125.1.11) 56(84) bytes of data.
64 bytes from csr1 (10.125.1.11): icmp_seq=1 ttl=255 time=3.41 ms
64 bytes from csr1 (10.125.1.11): icmp_seq=2 ttl=255 time=2.85 ms
64 bytes from csr1 (10.125.1.11): icmp_seq=3 ttl=255 time=2.82 ms

--- csr1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 2.821/3.028/3.411/0.278 ms
\end{minted}

Next, Ansible needs to populate variables for things like snmp.user and
snmp.group. Ansible is smart enough to look for file names matching the target
hosts in a folder called \verb|host_vars/| and automatically add all variables
to the play. These files are in YAML format and items can be nested as shown
below. This makes it easier to organize variables for different features. Some
miscellaneous BGP variables are shown in the file below even though our script
doesn't care about them. Note that if groups are used in the Ansible hosts
file, variable files can contain the names of those groups inside the
\verb|group_vars/| directly for similar treatment. Note that there are secure
ways to deal with plain-text passwords with Ansible, such as Ansible Vault.
This feature is not demonstrated in this document.

\begin{minted}{text}
[ec2-user@devbox ansible]# cat host_vars/csr1.yml
\end{minted}

\begin{minted}{yaml}
---
# Host variables for csr1
snmp:
  user: USERV3
  group: GROUPV3
  authpass: ABC123
  privpass: DEF456
bgp:
  asn: 65021  
  rid: 192.0.2.1
...
\end{minted}

The final step is to execute the playbook. Debugging is enabled so that the
generated commands are shown in the output below, which normally does not
happen. Note that the variable substitution, as well as the Ansible timestamp,
appears to be working. The play contained three tasks, all of which succeed.
Although \verb|gather_facts| didn't look like a task in the playbook, behind the
scenes the \verb|setup| module was executed on the control machine, which
counts as a task.

\begin{minted}{text}
[ec2-user@devbox ansible]# ansible-playbook snmp.yml -v
Using /etc/ansible/ansible.cfg as config file

PLAY [Updating SNMPv3 pre-shared keys] **************************************

TASK [Gathering Facts] ******************************************************
ok: [csr1]

TASK [SYS >> Define router credentials] *************************************
ok: [csr1] => {"ansible_facts": {"provider": {"host": "csr1",
"password": "ansible", "username": "ansible"}}, "changed": false}

TASK [IOS >> Issue commands to update SNMPv3 passwords, save config] ********
ok: [csr1] =>
{
 "banners": {}, "changed": false, "commands":
 [
  "snmp-server user USERV3 GROUPV3 v3 auth sha ABC123 priv aes 256 DEF456",
  "snmp-server contact PASSWORDS UPDATED 2017-05-07 at 18:05:27"
 ],
 "updates":
 [
  "snmp-server user USERV3 GROUPV3 v3 auth sha ABC123 priv aes 256 DEF456",
  "snmp-server contact PASSWORDS UPDATED 2017-05-07 at 18:05:27"
 ]
}

PLAY RECAP ******************************************************************
csr1                       : ok=3    changed=0    unreachable=0    failed=0
\end{minted}

To verify that the configuration was successfully applied, log into the target
router to manually verify the configuration. To confirm that the configuration
was saved, check the startup-configuration manually as well. The verification
is shown below.

\begin{minted}{text}
RTR_CSR1#show snmp contact
PASSWORDS UPDATED 2017-05-07 at 18:05:27

RTR_CSR1#show snmp user USERV3
User name: USERV3
Engine ID: 800000090300126BF529F95A
storage-type: nonvolatile	 active
Authentication Protocol: SHA
Privacy Protocol: AES256
Group-name: GROUPV3

RTR_CSR1#show startup-config | include contact
snmp-server contact PASSWORDS UPDATED 2017-05-07 at 18:05:27
\end{minted}

This simple example only scratches the surface of Ansible. The author has
written a comprehensive OSPF troubleshooting playbook which is simple to set
up, executes quickly, and is 100\% free. The link to the Github repository
where this playbook is hosted is provided below, and in the references
section. There are many other, unrelated Ansible playbooks available at the
author's Github page as well.

Nick's OSPF TroubleShooter (nots) --- \url{https://github.com/nickrusso42518/nots}

\subsubsection{NETCONF-based Infrastructure as Code with Ansible}
Earlier sections of this book introduced NETCONF, both as a protocol and
integrated into a Python programmability demonstration. Ansible can also
utilize  NETCONF for managing network devices, and this is quickly becoming
a common infrastructure-as-code alternative to legacy SSH/CLI administration.

This demonstration will solve the same problem as my popular open-source
\href{https://github.com/nickrusso42518/vpnm}{vpnm} repository available on Github.
The playbook ensures that the correct MPLS layer-3 VPN route-targets are configured,
intelligently adding and removing import and export route-targets where needed.
The playbook above is SSH/CLI based, which makes it universally consumable by
devices of any age, but is quite complex to understand and maintain. Using
NETCONF, operators can simplify the maintenance of their desired state.

Ansible allows for any arbitrary NETCONF RPC calls using the \verb|netconf_rpc|
module, but effectively using this module is tricky. The author recommends
first trying \verb|netconf_get| and \verb|netconf_config| modules for read and
write operations, respectively, and falling back to \verb|netconf_rpc| for
more customized actions if the wrapper modules don't work.

Presumably, readers already have some familiarity with Ansible at this point, so
I won't explain every detail. The variables file contains a list of VRFs that
should exist on a target router, such as an MPLS provider edge (PE). Each item
in the list is a dictionary, which contains two keys of interest,
\verb|route_import| and \verb|route_export|. These are lists of strings where
each element is a route-target. If an RT is present in this list, it will be
present on the device. If an RT is absent from this list, it will be removed
from the device. Operators can determine RT membership simply by editing this
file and running the Ansible playbook, which is how infrastructure as code
is supposed to work.

\begin{minted}{yaml}
---
# host_vars/csr1.yml
vrfs:
  - name: "VPN1"
    description: "FIRST VRF"
    rd: "1:1"
    route_import:
      - "100:1"
    route_export:
      - "100:2"
  - name: "VPN2"
    description: "SECOND VRF"
    rd: "2:2"
    route_import:
      - "200:1"
      - "200:2"
    route_export: []
\end{minted}

Let's explore the playbook next to see how the modules are used. Thankfully,
Ansible makes this very easy. All the operator must do is specify the XML
text to pass in. Coming up with the XML can be challenging, but we will visit
that soon. For now, assume that we ``just know'' it. Just like using jinja2 for
plain-text templating, it works well for XML templates, too.

\begin{minted}{yaml}
---
# nc_update.yml
- name: "Infrastructure-as-code using NETCONF"
  hosts: routers
  connection: netconf
  tasks:
    - name: "Update VRF config with NETCONF from XML template"
      netconf_config:
        content: "{{ lookup('template', 'templates/vpn.j2') }}"
\end{minted}

Admittedly, the template is the most complex part of the solution, but such is
the price (sometimes) paid for having nicely structured data. This structure
is based on the ``native'' YANG model, as opposed to something like OpenConfig,
so that needs to be specified. Notice the jinja2 \verb|for| loops. The outer
loop iterates over each VRF, creating a new \verb|<definition>| block for each.
Basic data, such as \verb|name|, \verb|description|, and \verb|rd| are applied
for each VRF\@. Then, a pair of nested \verb|for| loops iterate over the export
and import route targets, adding the appropriate XML blocks for each one. As
such, the size and composition of the template changes every time the operator
changes the ``desired state'' in the YAML variables files.

\begin{minted}{xml}
<config>
  <native xmlns="http://cisco.com/ns/yang/Cisco-IOS-XE-native">
    <vrf operation="replace">
{% for vrf in vrfs %}
      <definition>
        <name>{{ vrf.name }}</name>
        <description>{{ vrf.description }}</description>
        <rd>{{ vrf.rd }}</rd>
        <address-family>
          <ipv4/>
          <ipv6/>
        </address-family>
        <route-target>
{% for rte in vrf.route_export %}
          <export>
            <asn-ip>{{ rte }}</asn-ip>
          </export>
{% endfor %}
{% for rti in vrf.route_import %}
          <import>
            <asn-ip>{{ rti }}</asn-ip>
          </import>
{% endfor %}
        </route-target>
      </definition>
{% endfor %}
    </vrf>
  </native>
</config>
\end{minted}

First, check the router configuration. There are no VRFs on the device at all.
Be sure \verb|netconf-yang| is enabled, not \verb|netconf|, in order for
this technology to work correctly.

\begin{minted}{text}
CSR1#show vrf
[no output]
CSR1#show running-config | include netconf-yang
netconf-yang
\end{minted}

Next, run the playbook. Notice that the system reports \verb|changed|. At present,
the author cannot find an obvious way to report exactly what changed, but these
modules are rather new and are likely to be updated over time. At least we know
that a change was made, and in Ansible land, that means notifying handlers and
other useful activities.

\begin{minted}{text}
[centos@devbox netconf]# ansible-playbook nc_update.yml

PLAY [Infrastructure-as-code using NETCONF] *******************************

TASK [Update VRF config with NETCONF] *************************************
changed: [csr1]

PLAY RECAP ****************************************************************
csr1                       : ok=1    changed=1    unreachable=0    failed=0
\end{minted}

Run the playbook once more and the task reports \verb|ok|, implying that there
were no necessary changes since the state did not change.

\begin{minted}{text}
[centos@devbox netconf]# ansible-playbook nc_update.yml

PLAY [Infrastructure-as-code using NETCONF] *******************************

TASK [Update VRF config with NETCONF] *************************************
ok: [csr1]

PLAY RECAP ****************************************************************
csr1                       : ok=1    changed=0    unreachable=0    failed=0
\end{minted}

Using SSH, log into the router's CLI and check the VRF configuration. Notice
that both VPNs have the exactly correct VPN configuration. Any changes to
this configuration will be reverted anytime the playbook runs again.

\begin{minted}{text}
CSR1#show running-config | section vrf_definition
vrf definition VPN1
 description FIRST VRF
 rd 1:1
 route-target export 100:1
 route-target import 100:2
 !
 address-family ipv4
 exit-address-family
 !
 address-family ipv6
 exit-address-family
vrf definition VPN2
 description SECOND VRF
 rd 2:2
 route-target import 200:1
 route-target import 200:2
 !
 address-family ipv4
 exit-address-family
 !
 address-family ipv6
 exit-address-family
\end{minted}

Let's grab the current VRF configuration using NETCONF\@. This is how the author
grabbed the initial XML snippet to build the jinja2 template above. Another
approach could be converting the native YANG model to XML using \verb|pyang| or
something like it. This playbook is a little more involved since there are
some post-processing steps needed to beautify the XML for human readability
and write it to disk. Using the \verb|filter| option with \verb|netconf_get|
can limit the output just to a certain section, in this case, just VRFs.
Omitting this option captures the entire configuration.

\begin{minted}{yaml}
---
# nc_get.yml
- name: "Update VRF state via NETCONF"
  hosts: routers
  connection: netconf
  tasks:
    - name: "Get VRF config with NETCONF"
      netconf_get:
        source: running
        lock: always
        filter: "<native><vrf></vrf></native>"
      register: nc_vrf

    - name: "Format XML for easy viewing"
      xml:
        xmlstring: "{{ nc_vrf.stdout }}"
        pretty_print: true
      register: pretty_config
      changed_when: false

    - name: "Ensure vrf_configs/ folder exists"
      file:
        path: "{{ playbook_dir }}/vrf_configs"
        state: directory

    - name: "Write XML to disk"
      copy:
        content: "{{ pretty_config.xmlstring }}"
        dest: "vrf_configs/{{ inventory_hostname }}_netconf.xml"
\end{minted}

Running this playbook grabs the VRF configuration as represented by YANG
and encoded as XML data.

\begin{minted}{text}
[centos@devbox netconf]# ansible-playbook nc_get.yml

PLAY [Update VRF state via NETCONF] **********************************

TASK [Get VRF config with NETCONF] ****************************************
ok: [csr1]

TASK [Format XML for easy viewing] ****************************************
ok: [csr1]

TASK [Ensure vrf_configs/ folder exists] **********************************
ok: [csr1]

TASK [Write XML to disk] **************************************************
changed: [csr1]

PLAY RECAP ****************************************************************
csr1                       : ok=4    changed=1    unreachable=0    failed=0
\end{minted}

Look at the contents of the file to see how the pieces fit together. Also notice
how the proper route-target state is in place. The \verb|ns| numbering is
referencing XML namespaces, which like programming namespaces, can provide uniqueness
when same-named constructs are referenced from a single program. The namespaces
shouldn't be included when building XML templates, though. Administrators
can use these NETCONF captures as a way of doing configuration state
backups also.

\begin{minted}{text}
[centos@devbox netconf]# cat vrf_configs/csr1_netconf.xml
\end{minted}

\begin{minted}{xml}
<?xml version='1.0' encoding='UTF-8'?>
<ns0:data xmlns:ns0="urn:ietf:params:xml:ns:netconf:base:1.0"
  xmlns:ns1="http://cisco.com/ns/yang/Cisco-IOS-XE-native">
  <ns1:native>
    <ns1:vrf>
      <ns1:definition>
        <ns1:name>VPN1</ns1:name>
        <ns1:description>FIRST VRF</ns1:description>
        <ns1:rd>1:1</ns1:rd>
        <ns1:address-family>
          <ns1:ipv4/>
          <ns1:ipv6/>
        </ns1:address-family>
        <ns1:route-target>
          <ns1:export>
            <ns1:asn-ip>100:2</ns1:asn-ip>
          </ns1:export>
          <ns1:import>
            <ns1:asn-ip>100:1</ns1:asn-ip>
          </ns1:import>
        </ns1:route-target>
      </ns1:definition>
      <ns1:definition>
        <ns1:name>VPN2</ns1:name>
        <ns1:description>SECOND VRF</ns1:description>
        <ns1:rd>2:2</ns1:rd>
        <ns1:address-family>
          <ns1:ipv4/>
          <ns1:ipv6/>
        </ns1:address-family>
        <ns1:route-target>
          <ns1:import>
            <ns1:asn-ip>200:1</ns1:asn-ip>
          </ns1:import>
          <ns1:import>
            <ns1:asn-ip>200:2</ns1:asn-ip>
          </ns1:import>
        </ns1:route-target>
      </ns1:definition>
    </ns1:vrf>
  </ns1:native>
</ns0:data>
\end{minted}

\subsubsection{RESTCONF-based Infrastructure as Code with Ansible}
Suppose you love the idea of using something better than SSH/CLI but
find the XML templating within NETCONF to be rather confusing. While it is
possible to write some kind of translation from YAML/JSON Ansible variables
directly into XML, this would be rather complex for the average network
automation engineer. RESTCONF offers an alternative. Using Ansible's
generic \verb|uri| module to run HTTP-based operations on network devices,
operators can pass variables directly into the message body of an HTTP PUT
to configure a device as JSON data.

The variables structure has to change a bit to fit the YANG model we saw
above, except using YAML (or JSON) formatting. I'll use YAML for brevity,
and assuming operators are willing to restructure the state variables files,
this data can be passed straight into \verb|uri| by referencing the topmost
dictionary key of \verb|vrfs| from the \verb|body| option.

\begin{minted}{yaml}
---
# host_vars/csr1.yml
vrfs:
  vrf:
    definition:
      - name: "VPN1"
        description: "FIRST VRF"
        rd: "1:1"
        address-family:
          ipv4: {}
          ipv6: {}
        route-target:
          export:
            - asn-ip: "100:2"
          import:
            - asn-ip: "100:1"
      - name: "VPN2"
        description: "SECOND VRF"
        rd: "2:2"
        address-family:
          ipv4: {}
          ipv6: {}
        route-target:
          import:
            - asn-ip: "200:1"
            - asn-ip: "200:2"
\end{minted}

Next, examine the playbook. Like NETCONF, there is only one task to perform
the update. This module requires quite a bit more data, including login
information given \verb|connection: local| at the play level. The other
fields help construct the correct HTTP headers needed to configure the
device via RESTCONF\@. There are no jinja2 templates required at all.

\begin{minted}{yaml}
---
# rc_update.yml
- name: "Infrastructure-as-code using RESTCONF"
  hosts: routers
  connection: local
  tasks:
    - name: "Update VRF config with HTTP PUT"
      uri:
        # YAML folded syntax won't work here, shown for readability only
        url: >-
          https://{{ ansible_host }}/restconf/data/
          Cisco-IOS-XE-native:native/Cisco-IOS-XE-native:vrf
        user: "ansible"
        password: "ansible"
        method: PUT
        headers:
          Content-Type: "application/yang-data+json"
          Accept: "application/yang-data+json, application/yang-data.errors+json"
        body_format: json
        body: "{{ vrfs }}"
        validate_certs: false
        return_content: true
        status_code:
          - 200  # OK
          - 204  # NO CONTENT
\end{minted}

The device has no VRFs on it, just like before. RESTCONF will add them.
Be sure \verb|restconf| is enabled!

\begin{minted}{text}
CSR1#show vrf
[no output]
CSR1#show running-config | include restconf
restconf
\end{minted}

Run the playbook, and notice that the task reports \verb|ok|. Like NETCONF,
RESTCONF is idempotent and easy to program using Ansible. Unlike NETCONF,
there is no notification in the HTTP response message that indicates whether
a change was made or not. This could be problematic if there are handlers
requiring notification, but often times is not a big issue. Administrators can
see if changes were made using an HTTP GET operation which is coming up next.
It is possible that Cisco will update their RESTCONF API to include this
in the future.

\begin{minted}{text}
[centos@devbox restconf]# ansible-playbook rc_update.yml

PLAY [Infrastructure-as-code using RESTCONF] ******************************

TASK [Update VRF config with HTTP PUT] ************************************
ok: [csr1]

PLAY RECAP ****************************************************************
csr1                       : ok=1    changed=0    unreachable=0    failed=0
\end{minted}

Quickly check the VRF configuration on the CLI to ensure it matches
the declarative state from the variables file. This output should be
identical to the output what NETCONF returned, since both methods
do the exact same thing.

\begin{minted}{text}
CSR1#show run | section vrf definition
vrf definition VPN1
 description FIRST VRF
 rd 1:1
 route-target export 100:2
 route-target import 100:1
 !
 address-family ipv4
 exit-address-family
 !
 address-family ipv6
 exit-address-family
vrf definition VPN2
 description SECOND VRF
 rd 2:2
 route-target import 200:1
 route-target import 200:2
 !
 address-family ipv4
 exit-address-family
 !
 address-family ipv6
 exit-address-family
\end{minted}

In case operators don't know what the correct data structure looks like,
use the \verb|uri| module again for the HTTP GET operation. The playbook
below allows operators to execute an HTTP GET, collect data, and write it
to a file. It doesn't require quite as much post-processing as XML since
Ansible can beautify JSON rather easily.

\begin{minted}{yaml}
---
# rc_get.yml
- name: "Collect VRF config with RESTCONF"
  hosts: routers
  connection: local
    - name: "Get VRF config with RESTCONF"
      uri:
        # YAML folded syntax won't work here, shown for readability only
        url: >-
          https://{{ ansible_host }}/restconf/data/
          Cisco-IOS-XE-native:native/Cisco-IOS-XE-native:vrf
        user: "{{ ansible_user }}"
        password: "{{ ansible_password }}"
        method: GET
        return_content: true
        headers:
          Accept: 'application/yang-data+json'
        validate_certs: false
      register: rc_vrf

    - name: "Ensure vrf_configs/ folder exists"
      file:
        path: "{{ playbook_dir }}/vrf_configs"
        state: directory

    - name: "Write JSON to disk"
      copy:
        content: "{{ rc_vrf.json | to_nice_json }}"
        dest: "vrf_configs/{{ inventory_hostname }}_restconf.json"
\end{minted}

Quickly run the playbook to gather the current VRF state and store it
as a JSON file.

\begin{minted}{text}
[centos@devbox restconf]# ansible-playbook rc_get.yml

PLAY [Collect VRF config with RESTCONF] ***********************************

TASK [Get VRF config with RESTCONF] ***************************************
ok: [csr1]

TASK [Ensure vrf_configs/ folder exists] **********************************
ok: [csr1]

TASK [Write JSON to disk] *************************************************
changed: [csr1]

PLAY RECAP ****************************************************************
csr1                       : ok=3    changed=1    unreachable=0    failed=0
\end{minted}

Check the contents of the file to see the JSON returned from RESTCONF\@.
Operators can use this as their variables template starting point. Simply
modify this JSON structure, optionally converting to YAML first if that
is easier, and pass the result into Ansible to manage your infrastructure
as code using JSON instead of CLI commands.

\begin{minted}{text}
[centos@devbox restconf]# cat vrf_configs/csr1_restconf.json
\end{minted}

\begin{minted}{json}
{
    "Cisco-IOS-XE-native:vrf": {
        "definition": [
            {
                "address-family": {
                    "ipv4": {},
                    "ipv6": {}
                },
                "description": "FIRST VRF",
                "name": "VPN1",
                "rd": "1:1",
                "route-target": {
                    "export": [
                        {
                            "asn-ip": "100:2"
                        }
                    ],
                    "import": [
                        {
                            "asn-ip": "100:1"
                        }
                    ]
                }
            },
            {
                "address-family": {
                    "ipv4": {},
                    "ipv6": {}
                },
                "description": "SECOND VRF",
                "name": "VPN2",
                "rd": "2:2",
                "route-target": {
                    "import": [
                        {
                            "asn-ip": "200:1"
                        },
                        {
                            "asn-ip": "200:2"
                        }
                    ]
                }
            }
        ]
    }
}
\end{minted}

\subsubsection{Agent-less Demonstration with Nornir}
Nornir is an open source project created by
\href{https://twitter.com/dbarrosop/}{David Barroso} and is maintained by
several well-known network programmability experts. Nornir uses many common,
open-source projects under the hood, such as textfsm, NAPALM, and netmiko.
This makes it easily consumable by organizations already using these libraries
for other purposes.  Nornir was formerly known as Brigade and is a task
execution engine, like Ansible, with a few key differences:

\begin{enumerate}
  \item	No domain specific language (DSL). Yes, Nornir makes you write Python,
  while Ansible lets you write simpler YAML\@. Doing simple things is easy in
  DSL, but complex hard things is extremely challenging. Even moderately
  complex nested iteration requires multiple files in Ansible, but doing so in
  Python is trivial. With Nornir, you get pure Python without complex
  integrations via DSL\@.
  \item	Python debugger (\verb|pdb|) works natively, simplifying debugging. In
  Ansible, your best tools are verbosity options from the shell (i.e.\
  \verb|ansible-playbook test.yml -vvv|) or the \verb|debug| module, neither
  of which have the power of \verb|pdb|.
  \item	The number of supplemental Python support tools (such as
  \verb|pylint|, \verb|bandit|, and \verb|black|) is enormous. These can
  easily be leveraged for Nornir runbook maintenance, typically within CI/CD pipelines.
  \item	Nornir tends to be faster than Ansible, given that it does not need to
  serialize/deserialize between YAML/JSON and Python continuously. More data
  referenced within Ansible means more processing time, and thus slower execution.
\end{enumerate}

Because the author has extensive experience with Ansible across a variety of
production use cases, comparisons between Nornir and Ansible are common
throughout this section. Given Ansible's popularity and market penetration at
the time of this writing, it is likely that readers will be able to compare
and contrast, too.

Installing Nornir is simple using pip. The author recommends using Python 3.6
or newer and Nornir 2.0.0 or newer. The Linux and Cisco CSR1000v versions are
the same as those shown in the previous Ansible demonstration, and thus are not repeated.

\begin{minted}{text}
[ec2-user@devbox ~]# python3 --version
Python 3.6.5

[ec2-user@devbox ~]# python3 -m pip install nornir
[snip, installation output]

[ec2-user@devbox nornir-test]# python3 -m pip list | grep nornir
nornir (2.0.0)
\end{minted}

Nornir is comprised of several main components. First, an optional
configuration file is used to specify global parameters, typically default
settings for the execution of Nornir runbooks, which can simplify Nornir
coding later. The same concept exists in Ansible. Exploring the configuration
file is not terribly important to understanding Nornir basics and is not
covered in this demonstration.

Also like Ansible, Nornir supports robust options for managing inventory,
which is a collection of hosts and groups. Nornir can even consume existing
Ansible inventories for those looking to migrate from Ansible to Nornir. The
inventory file is called \verb|hosts.yaml| and is required when using Nornir's
default inventory plugin. The groups file is called \verb|groups.yaml| and is
optional, though often used. Many more advanced inventory options exist, but
this demonstration uses the ``simple'' inventory method, which is the default.

The simplest possible hosts.yaml file is shown below. There are many other
minor options for host fields, such as a site identifier, role, and group
list. This demonstration uses only a single CSR1000v, named as such in the
inventory as a top level key. The variables specific to this host are the
subways listed under it.

\begin{minted}{yaml}
---
# hosts.yaml
csr1000v:
  hostname: "csr1000v.lab.local"  # or IP address
  username: "cisco"
  password: "cisco"
  platform: "ios"
\end{minted}

For the sake of a more interesting example, consider the case of multiple
CSR1000v routers with the same login information. Copy/pasting host-level
variables such as usernames and passwords is undesirable, especially at scale,
so using group-level variables via \verb|groups.yaml| is a better design. Each
CSR is assigned to group \verb|csr| which contains the common login
information as group-level variables. While the format differs from Ansible's
YAML inventory, the general logic of data inheritance is the same. More
generic variable definitions, such as group variables, can be overridden on a
per-host basis if necessary.

\begin{minted}{yaml}
---
# hosts.yaml
csr1000v_1:
  hostname: "172.16.1.1"
  groups: ["csr"]
csr1000v_2:
  hostname: "172.16.1.2"
  groups: ["csr"]
csr1000v_3:
  hostname: "172.16.1.3"
  groups: ["csr"]
csr1000v_4:
  hostname: "172.16.1.4"
  groups: ["csr"]

---
# groups.yaml
csr:
  username: "cisco"
  password: "cisco"
  platform: "ios"
\end{minted}

The demonstration below is a simple runbook from
\href{https://twitter.com/networklore}{Patrick Ogenstad}, one of the Nornir
developers. The author has adapted it slightly to fit this book's format and
added comments to briefly explain each step. The Python file below is
named \verb|get_facts_ios.py|.

\begin{minted}{python}
from nornir import InitNornir
from nornir.plugins.tasks.networking import napalm_get
from nornir.plugins.functions.text import print_result

# Initialize a Nornir object.
nr = InitNornir()

# Execute a task against the hosts defined in the inventory.
# Specifically, gather basic router facts using NAPALM getters
# behind the scenes, much like Ansible's "ios_facts" module.
facts = nr.run(
    napalm_get,
    getters=['get_facts'])

# Pretty-print the result to stdout in a colorful JSON-style format.
print_result(facts)
\end{minted}

Running this code yields the following output. Like Ansible, individual tasks
are printed in easy-to-delineate stanzas which contains specific output from
that task. Here, the data returned by the device is printed, along with many
of the dictionary keys needed to access individual fields, if necessary. This
simple method is great for troubleshooting but often times, programmers will
have to perform specific actions on specific pieces of data.

\begin{minted}{text}
[ec2-user@devbox nornir-test]# python3 get_facts_ios.py 

napalm_get**************************************************************
* csr1000v ** changed : False ******************************************
vvvv napalm_get ** changed : False vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv INFO
{ 'get_facts': { 'fqdn': 'CSR1000v.ec2.internal',
                 'hostname': 'CSR1000v',
                 'interface_list': ['GigabitEthernet1', 'VirtualPortGroup0'],
                 'model': 'CSR1000V',
                 'os_version': 'Virtual XE Software '
                               '(X86_64_LINUX_IOSD-UNIVERSALK9-M), Version '
                               '16.9.1, RELEASE SOFTWARE (fc2)',
                 'serial_number': '9RJTDVAF3DP',
                 'uptime': 5160,
                 'vendor': 'Cisco'}}
^^^^ END napalm_get ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\end{minted}

The \verb|result| object is a key component in Nornir, albeit a complex one.
The general structure is as follows, shown in pseudo-YAML format with some
minor technical inaccuracies intentionally. This quick visual indication can
help those new to Nornir to understand the general structure of data returned
by a Nornir run.

\begin{minted}{yaml}
result_from_nornir:
  host1:
    - task1:
      other_stuff1: interesting values here
      other_stuff2: ...
        more_details_a: ...
        more_details_b: ...
    - task2: ...
  host2:
    - task1: ...
    - task2: ...
\end{minted}

More accurately, the \verb|result_from_nornir| is not a pure dictionary but is
a dict-like object called \verb|AggregatedResult|, which combines all of the
results across all hosts. Each host is referenced by hostname as a dictionary
key, which returns a \verb|MultiResult| object. This is a list-like structure
which can be indexed by integer, iterated over, sliced, etc. The elements of
these lists are \verb|Result| objects which contain extra interesting data
that is be accessible from a given task. This extra interesting data is
wrapped in a dictionary which is accessible through the \verb|result|
attribute of the object, NOT indexable as a dictionary key. The pseudo-YAML
below is slightly more accurate in showing the object structure used for
Nornir results.

\begin{minted}{yaml}
AggregatedResult:
  MultiResult:
    - Result:
        changed: !!bool
        failed: !!bool
        name: !!str
        result:
          specific_field1: ...
          specific_field2: ...
    - Result: ...
  MultiResult:
    - Result: ...
    - Result: ...
\end{minted}

If this seems tricky, it is, and the demonstration below helps explain it.
Without digging into the source code of these custom objects, one can use the
Python debugger (\verb|pdb|) to do some basic discovery. This understanding
makes programmatically accessing individual fields easier, which Nornir
automatically parses and stores as structured data. Simply add this line of
code to the end of the Python script above. This is the programming equivalent
of setting a breakpoint; Python calls them traces.

\begin{minted}{python}
import pdb; pdb.set_trace()
\end{minted}

After running the code and seeing the pretty JSON output displayed, a \verb|(Pdb)|
prompt waits for user input. Mastering pdb is outside the scope of this book
and we will not be exploring pdb-specific commands in any depth. What pdb
enables is a real-time Python command line environment, allowing us to inject
arbitrary code at the trace. Just type \verb|facts| to start, the name
of the object returned by the Nornir run. This alone reveals a fair amount of
information.

\begin{minted}{text}
(Pdb) facts
AggregatedResult (napalm_get): {'csr1000v': MultiResult: [Result: "napalm_get"]}
\end{minted}

First, the \verb|facts| object is an \verb|AggregatedResult|, a dict-like
object as annotated by the {curly braces} with key:value mappings inside. It
has one key called \verb|csr1000v|, the name of our test host. The value of
this key is a \verb|MultiResult| object which is a list-like structure as
annotated by the [square brackets]. Thus, pdb should indicate that
\verb|facts['csr1000v']| returns a MultiResult object, which contains a
\verb|Result| object named \verb|napalm_get|.

\begin{minted}{text}
(Pdb) facts['csr1000v']
MultiResult: [Result: "napalm_get"]
\end{minted}

Since there was only 1 task that Nornir ran (getting the IOS facts), the
length of this list-like object should be 1. Quickly test that using the
Python \verb|len()| function.

\begin{minted}{text}
(Pdb) len(facts['csr1000v'])
1
\end{minted}

Index the task results manually by using the [0] index method. This is yields
a \verb|Result| object, which is neither list-like nor dict-like.

\begin{minted}{text}
(Pdb) facts['csr1000v'][0]
Result: "napalm_get"
\end{minted}

The \verb|Result| object has some metadata fields, such as \verb|changed| and
\verb|failed| (much like Ansible) to indicate what happened when a task was
executed. The real meat of the results is buried in a field called
\verb|result|. Using Python's \verb|dir()| function to explore these fields is
useful, as shown below. For brevity, the author has manually removed some
fields not relevant to this discovery exercise.

\begin{minted}{text}
(Pdb) dir(facts['csr1000v'][0])
[..., 'changed', 'diff', 'exception', 'failed', 'host', 'name', 'result', 'severity_level']
\end{minted}

Feel free to casually explore some of these fields by simply referencing them.
For example, since this was a read-only task that succeeded, both
\verb|changed| and \verb|failed| fields should be false. If this were a task
with configuration changes, \verb|changed| could potentially be true if actual
changes were necessary. Also note that the name of this task was
\verb|napalm_get|, the default name as our script did not specify one. Nornir
can consume netmiko and NAPALM connection handlers, which provides expansive
support for many network platforms, and this helps prove it.

\begin{minted}{text}
(Pdb) facts['csr1000v'][0].changed
False
(Pdb) facts['csr1000v'][0].failed
False
(Pdb) facts['csr1000v'][0].name
'napalm_get'
\end{minted}

After digging through all of the custom objects, we can test the \verb|result|
field for its type, which results in a basic dictionary with a top-level key
of \verb|get_facts|. The value is another dictionary with a handful of keys
containing device information. Simply printing out this field displays the
dictionary that was pretty-printed by the \verb|print_result()| function shown earlier.
The long \verb|get_facts| dict output is broken up to fit the screen.

\begin{minted}{text}
(Pdb) type(facts['csr1000v'][0].result)
<class 'dict'>

(Pdb) facts['csr1000v'][0].result
{'get_facts': {'uptime': 2340, 'vendor': 'Cisco',
  'os_version': 'Virtual XE Software (X86_64_LINUX_IOSD-UNIVERSALK9-M),
  Version 16.9.1, RELEASE SOFTWARE (fc2)', 'serial_number': '9RJTDVAF3DP',
  'model': 'CSR1000V', 'hostname': 'CSR1000v', 'fqdn': 'CSR1000v.ec2.internal',
  'interface_list': ['GigabitEthernet1', 'VirtualPortGroup0']}}
\end{minted}

Using \verb|pdb| to reference individual fields, we can add some custom code
to test our understanding. For example, suppose we want to create a string
containing the hostname and serial number in a hyphenated string. Using the
new f-string feature of Python 3.6, this is simple and clean.

\begin{minted}{text}
(Pdb) data = facts['csr1000v'][0].result['get_facts']
(Pdb) important_info = f"{data['hostname']}-{data['serial_number']}"
(Pdb) important_info
'CSR1000v-9RJTDVAF3DP'
\end{minted}

Armed with this new understanding, we can add these exact lines to our
existing runbook and continue development using the \verb|data| dictionary as
a handy shortcut to access the IOS facts.

It is worthwhile to explain Nornir's \verb|run()| function in greater depth. The
\verb|run()| function takes in a task object, which is just another function. Because
everything can be treated like an object in Python, passing functions as
parameters into other functions to be executed later is easy. This parameter
function is a task and contains the logic to perform some action, like run a
command, gather facts, or make configuration changes. The remaining keyword
arguments (kwargs) are the inputs for the parameter function passed into
\verb|run()|. In short, \verb|run()| is a Nornir wrapper to execute the
parameter function with its kwargs, but do so within the framework of Nornir.

To group tasks together, one does not create a ``list of tasks'' as in
Ansible. Instead, use a wrapper function that has many \verb|run()| invocations to
sequence the tasks in the correct order. Nornir consumers can easily insert
additional logic in between \verb|run()| calls, such as printing output, inserting
pdb traces, writing to files, or whatever other things don't directly qualify
as Nornir tasks. This wrapper function is passed into \verb|run()| from the calling
function level as if it were a task itself. Be sure to include any kwargs
needed for this wrapper to operate. The example below expands our previous
Nornir runbook to both collect basic facts and apply configuration. For
cleanliness, the author has added a \verb|main()| function to this runbook.

The \verb|manage_router()| function sequences the tasks to be run. Using
NAPALM to configure network devices introduces a rich feature set of providing
a ``diff'', automatic rollback, and automatic configuration saving. Users
should pass in a \verb|\n| delineated string, which can be assembled by joining a
list of strengths via newline (or a variety of other techniques). Note that
results from individual task calls are not saved inside the wrapper; Nornir
aggregates these results at the calling function level.

In the \verb|main()| function, the calling function in this case,
\verb|manage_router()| is treated like a task and its \verb|config_lines|
kwarg is populated with a list of service strings to apply. This task grouping
wrapper is executed and its results are printed out. The Python file below
is named \verb|manage_router_ios.py|.

\begin{minted}{python}
from nornir import InitNornir
from nornir.plugins.tasks.networking import napalm_get
from nornir.plugins.tasks.networking import napalm_configure
from nornir.plugins.functions.text import print_result

def manage_router(nr, config_lines):

    # Task 1: Get basic information (same as before)
    nr.run(task=napalm_get, getters=['get_facts'])

    # Task 2: Use "napalm_configure" function along with kwargs
    # representing the configuration as a newline-joined string.
    nr.run(task=napalm_configure, configuration='\n'.join(config_lines))

def main():

    # Initialize a Nornir object.
    nr = InitNornir()

    # Define services as a list of strings
    services = [
        'service nagle',
        'service sequence-numbers',
        'no service pad'
    ]

    # Run the grouped task function to get facts and apply config.
    from_tasks = nr.run(task=manage_router, config_lines=services)

    # Pretty-print the result to stdout in a pretty JSON format.
    print_result(from_tasks)

if __name__ == '__main__':
    main()
\end{minted}

Running this code yields the following output. Tasks are printed out in the
sequence in which they were invoked. This particular router required Nagle and
sequence-number services to be enabled, and needed to have the PAD service
disabled, per the diff included in the output.

\begin{minted}{text}
[ec2-user@devbox nornir-test]# python3 manage_router_ios.py

manage_router***********************************************************
* csr1000v ** changed : True ***************************************************
vvvv manage_router ** changed : False vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv INFO
---- napalm_get ** changed : False ------------------------------------- INFO
{ 'get_facts': { 'fqdn': 'CSR1000v.ec2.internal',
                 'hostname': 'CSR1000v',
                 'interface_list': ['GigabitEthernet1', 'VirtualPortGroup0'],
                 'model': 'CSR1000V',
                 'os_version': 'Virtual XE Software '
                               '(X86_64_LINUX_IOSD-UNIVERSALK9-M), Version '
                               '16.9.1, RELEASE SOFTWARE (fc2)',
                 'serial_number': '9RJTDVAF3DP',
                 'uptime': 1560,
                 'vendor': 'Cisco'}}
---- napalm_configure ** changed : True -------------------------------- INFO
+service nagle
+service sequence-numbers
-no service pad
^^^^ END manage_router ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\end{minted}

Because NAPALM is idempotent with respect to IOS configuration management,
running the runbook again should yield no changes when the
\verb|napalm_configure| task is executed. The \verb|changed| return value
changes from \verb|True| in the previous output to \verb|False| below. No diff
is supplied as a result.

\begin{minted}{text}
[ec2-user@devbox nornir-test]# python3 manage_router_ios.py

manage_router***********************************************************
* csr1000v ** changed : False **************************************************
vvvv manage_router ** changed : False vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv INFO
---- napalm_get ** changed : False ------------------------------------- INFO
{ 'get_facts': { 'fqdn': 'CSR1000v.ec2.internal',
                 'hostname': 'CSR1000v',
                 'interface_list': ['GigabitEthernet1', 'VirtualPortGroup0'],
                 'model': 'CSR1000V',
                 'os_version': 'Virtual XE Software '
                               '(X86_64_LINUX_IOSD-UNIVERSALK9-M), Version '
                               '16.9.1, RELEASE SOFTWARE (fc2)',
                 'serial_number': '9RJTDVAF3DP',
                 'uptime': 2040,
                 'vendor': 'Cisco'}}
---- napalm_configure ** changed : False ------------------------------- INFO
^^^^ END manage_router ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\end{minted}

Rerunning the code with a pdb trace applied at the end of the program allows
Nornir users to explore the \verb|from_tasks| variable in more depth. For each
host (in this case \verb|csr1000v|), there is a list of \verb|MultiResult| objects.
This list includes results from the wrapper function, not just the inner tasks, so
its length should be 3: the grouped function followed by the 2 tasks. For
troubleshooting they can be indexed as shown below. Notice the empty-string
diff returned by NAPALM from the second task, an indicator that our network
hasn't experienced any changes since the last Nornir run.

\begin{minted}{text}
[ec2-user@devbox nornir-test]# python3 manage_router_ios.py
> /home/ec2-user/nornir-test/manage_router_ios.py(31)main()
-> print_result(from_tasks)

(Pdb) from_tasks
AggregatedResult (manage_router): {'csr1000v': MultiResult:
  [Result: "manage_router", Result: "napalm_get", Result: "napalm_configure"]}

(Pdb) from_tasks['csr1000v']
MultiResult: [Result: "manage_router", Result: "napalm_get",
  Result: "napalm_configure"]

(Pdb) len(from_tasks['csr1000v'])
3

(Pdb) from_tasks['csr1000v'][0]
Result: "manage_router"

(Pdb) from_tasks['csr1000v'][1]
Result: "napalm_get"

(Pdb) from_tasks['csr1000v'][2]
Result: "napalm_configure"

(Pdb) from_tasks['csr1000v'][2].diff
''
\end{minted}

% VCS tools
\subsubsection{Version Control Overview}
Automation in general is a fundamental topic of an effective automation
design. In all case, a programmer needs to write the code in the first
place, and like all pieces of code, it must be maintained, tested, versioned,
and continuously monitored. Examples of popular repositories for text file
configuration management include Github and Amazon Web Services (AWS) CodeCommit.
The sections that follow include demonstrations using a variety of version
control systems and remote repositories.

\subsubsection{Git with Github}
In the first example, a Google Codejam solution is shown in the code that
follows. The challenge was finding the minimal scalar product between two
vectors of equal length. The solution is to sort both vectors: one sorted
greatest-to-least, and one sorted least-to-greatest. Then, performing the
basic scalar product logic, the problem is solved. This code is not an
exercise in absolute efficiency or optimization as it was written to be
modular and readable. The example below was written in Python 3.5.2 and
the name of the file is \verb|VectorPair.py|.

\begin{minted}{text}
Nicholass-MBP:min-scalar-prod nicholasrusso# python3 --version
Python 3.5.2
\end{minted}

\begin{minted}{python}
class VectorPair:
    """
	Class defining a VectorPair object with helper methods.
	"""

    def __init__(self, v1, v2, n):
        """
        Constructor takes in two vectors and the vector length.
        """
        self.v1 = v1
        self.v2 = v2
        self.n = n

    def _resolve_sp(self, v1, v2):
        """
        Given two vectors of equal length, the scalar product is pairwise
        multiplication of values and the sum of all pairwise products.
        """
        sp = 0

        # Iterate over elements in the array and compute
        #  the scalar product
        for i in range(self.n):
            sp += v1[i] * v2[i]

        return sp

    def resolve_msp(self):
        """
        Given two vectors of equal length, the minimum scalar product is
        the smallest number that exists given all permutations of
        multiplying numbers between the two vectors.
        """

        # Sort v1 low->high and v2 high->low
        # This ensures the smallest values of one list are
        #  paired with the largest values of the other
        v1sort = sorted(self.v1, reverse=False)
        v2sort = sorted(self.v2, reverse=True)

        # Invoke the internal method for resolution
        return self._resolve_sp(v1sort, v2sort)
\end{minted}

This Github account is used to demonstrate a revision
control example. Suppose that a change to the Python script above is required,
and specifically, a trivial comment change. Checking the \verb|git status first|,
the repository is up to date as no changes have been made. It explores git at
a very basic level and does not include branches, forks, pull requests, etc.

\begin{minted}{text}
Nicholass-MBP:min-scalar-prod nicholasrusso# git status
On branch master
Your branch is up-to-date with 'origin/master'.
nothing to commit, working directory clean
\end{minted}

The verbiage of a comment relating to the constructor method is now changed.

\begin{minted}{text}
Nicholass-MBP:min-scalar-prod nicholasrusso# grep Constructor VectorPair.py
    Constructor takes in the vector length and two vectors.

### OPEN THE TEXT EDITOR AND MAKE CHANGES (NOT SHOWN) ###

Nicholass-MBP:min-scalar-prod nicholasrusso# grep Constructor VectorPair.py
    Constructor takes in two vectors and the vector length.
\end{minted}

\verb|git status| now reports that \verb|VectorPair.py| has been modified but
not added to the set of files to be committed to the repository.
The \verb|changes not staged for commit| indicates that the files are
not currently in the staging area.

\begin{minted}{text}
Nicholass-MBP:min-scalar-prod nicholasrusso# git status
On branch master
Your branch is up-to-date with 'origin/master'.
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   VectorPair.py

no changes added to commit (use "git add" and/or "git commit -a")
\end{minted}

Adding this file to the list of changed files effectively stages it for
commitment to the repository. The \verb|changes to be committed| verbiage
word from the terminal indicates this.

\begin{minted}{text}
Nicholass-MBP:min-scalar-prod nicholasrusso# git add VectorPair.py

Nicholass-MBP:min-scalar-prod nicholasrusso# git status
On branch master
Your branch is up-to-date with 'origin/master'.
Changes to be committed:
  (use "git reset HEAD <file>..." to unstage)

	modified:   VectorPair.py
\end{minted}

Next, the file is committed with a comment explaining the change. This command
does not update the Github repository, only the local one. Code contained in
the local repository is, by definition, one programmer's local work. Other
programmers may be contributing to the remote repository while another works
locally for some time. This is why git is considered a ``distributed'' version
control system.

\begin{minted}{text}
Nicholass-MBP:min-scalar-prod nicholasrusso# git commit -m "evolving tech comment update"
[master 74ed39a] evolving tech comment update
 1 file changed, 2 insertions(+), 2 deletions(-)

Nicholass-MBP:min-scalar-prod nicholasrusso# git status
On branch master
Your branch is ahead of 'origin/master' by 1 commit.
  (use "git push" to publish your local commits)
nothing to commit, working directory clean
\end{minted}

To update the remote repository, the committed updates must be pushed. After
this is complete, the \verb|git status| utility informs us that there are no
longer any changes.

\begin{minted}{text}
Nicholass-MBP:min-scalar-prod nicholasrusso# git push -u
Counting objects: 4, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (4/4), done.
Writing objects: 100% (4/4), 455 bytes | 0 bytes/s, done.
Total 4 (delta 2), reused 0 (delta 0)
remote: Resolving deltas: 100% (2/2), completed with 2 local objects.
To https://github.com/nickrusso42518/google-codejam.git
   e8d0c54..74ed39a  master -> master
Branch master set up to track remote branch master from origin.

Nicholass-MBP:min-scalar-prod nicholasrusso# git status
On branch master
Your branch is up-to-date with 'origin/master'.
nothing to commit, working directory clean
\end{minted}

Logging into the Github web page, one can verify the changes were successful. At
the root directory containing all of the Google Codejam challenges, the
comment added to the last commit is visible.

\addimg{github-folders.png}{0.8}{Github Changes --- Summary}

Looking into the min-scalar-prod directory and specifically the \verb|VectorPair.py|
file, git clearly displays the additions/removals from the file. As such, git
is a powerful tool that can be used for scripting, data files (YAML, JSON,
XML, YANG, etc.) and any other text documents that need to be revision
controlled. The screenshot is shown below.

\addimg{github-diff.png}{0.8}{Github Changes --- Detailed Differences}

\subsubsection{Git with AWS CodeCommit and CodeBuild}
Although AWS services are not on the blueprint, a basic understanding of
developer services available in public cloud (PaaS and SaaS options) is worth
examining. This example uses the CodeCommit service, which is comparable to
Github, acting as a remote Git repository. Additionally, CodeBuild CI services
are integrated into the test repository, similar to Travis CI or Jenkins, for
testing the code.

This section does not walk through all of the detailed AWS setup as there are
many tutorials and documents detailing it. However, some key points are worth
mentioning. First, an Identity and Access Management (IAM) group should be
created for any developers accessing the project. The author also created a
user called \verb|nrusso| and added him to the \verb|Development| group.

\addimg{aws-iam-devgroup.png}{0.8}{Creating a New AWS IAM User and Group}

Note that the permissions of the Development group should include
\verb|AWSCodeCommitFullAccess|.

\addimg{aws-iam-permissions.png}{0.8}{Assigning AWS IAM Permissions}

Navigating to the CodeCommit service, create a new repository called
\verb|awsgit| without selecting any other fancy options. This initializes and
empty repository. This is the equivalent of creating a new repository in
Github without having pushed any files to it.

\addimg{aws-create.png}{0.8}{Creating a New AWS CodeCommit Repository}

Next, perform a clone operation from the AWS CodeCommit repository using
HTTPS\@. While the repository is empty, this establishes successful connectivity
with AWS CodeCommit.

\begin{minted}{text}
Nicholass-MBP:projects nicholasrusso# git clone \
>  https://git-codecommit.us-east-1.amazonaws.com/v1/repos/awsgit
Cloning into 'awsgit'...
Username for 'https://git-codecommit.us-east-1.amazonaws.com': nrusso-at-043535020805
Password for 'https://nrusso-at-043535020805@git-codecommit.us-east-1.amazonaws.com':
warning: You appear to have cloned an empty repository.
Checking connectivity... done.

Nicholass-MBP:projects nicholasrusso# ls -l awsgit/
Nicholass-MBP:projects nicholasrusso#
\end{minted}

Change into the directory and check the Git remote repositories. The AWS
CodeCommit repository named \verb|awsgit| has been added automatically after
the clone operation. We can tell this is a Git repository since it contains
the \verb|.git| hidden folder.

\begin{minted}{text}
Nicholass-MBP:projects nicholasrusso# cd awsgit/
Nicholass-MBP:awsgit nicholasrusso# git remote -v
origin https://git-codecommit.us-east-1.amazonaws.com/v1/repos/awsgit (fetch)
origin https://git-codecommit.us-east-1.amazonaws.com/v1/repos/awsgit (push)

Nicholass-MBP:awsgit nicholasrusso# ls -la
total 0
drwxr-xr-x   3 nicholasrusso  staff  102 May  5 14:45 .
drwxr-xr-x   8 nicholasrusso  staff  272 May  5 14:45 ..
drwxr-xr-x  10 nicholasrusso  staff  340 May  5 14:46 .git
\end{minted}

Create a file. Below is an example of a silly \verb|README.md| file in markdown.
Markdown is a simple way of writing HTML code that many repository systems can
render nicely.

\begin{minted}{md}
# DevOps in Cloud
This is pretty cool

## Hopefully markdown works
That would make this file look good

> Note: Important message

```
code
block
```
\end{minted}

Following the basic git workflow, we add the file to the staging area, commit
it to the local repository, then push it to AWS CodeCommit repository called
\verb|awsgit|.

\begin{minted}{text}
Nicholass-MBP:awsgit nicholasrusso# git add .

Nicholass-MBP:awsgit nicholasrusso# git commit -m "added readme"
[master (root-commit) 99bfff2] added readme
 1 file changed, 12 insertions(+)
 create mode 100644 README.md

Nicholass-MBP:awsgit nicholasrusso# git push -u origin master
Counting objects: 3, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 337 bytes | 0 bytes/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To https://git-codecommit.us-east-1.amazonaws.com/v1/repos/awsgit
 * [new branch]      master -> master
Branch master set up to track remote branch master from origin.
\end{minted}

Check the AWS console to see if the file was correctly received by the
repository. It was, and even better, CodeCommit supports Markdown rendering
just like Github, Gitlab, and many other GUI-based systems.

\addimg{aws-readme.png}{0.8}{AWS CodeCommit README File}

To build on this basic repository, we can enable continuous integration (CI)
using AWS CodeBuild service. It ties in seamlessly to CodeCommit which, unlike
other common integrations (Github + Jenkins) which require many manual steps.
The author creates a sample project below based on Fibonacci numbers, which
are numbers whereby the next number is the sum of the previous two. Some
additional error-checking is added to check for non-integer inputs, which
makes the test cases more interesting. The Python file below is called
\verb|fibonacci.py|.

\begin{minted}{python}
#!/bin/python

def fibonacci(n):
    if not isinstance(n, int):
        raise ValueError('Please use an integer')
    elif n < 2:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
\end{minted}

Any good piece of software should come with unit tests. Some software
development methodologies, such as Test Driven Development (TDD), even suggest
writing the unit tests before the code itself! Below are the enumerated test
cases used to test the Fibonacci function defined above. The three test cases
evaluate zero/negative number inputs, bogus string inputs, and valid integer
inputs. The test script below is called \verb|fibotest.py|.

\begin{minted}{python}
#!/bin/python

import unittest
from fibonacci import fibonacci

class fibotest(unittest.TestCase):

    def test_input_zero_neg(self):
        self.assertEqual(fibonacci(0), 0)
        self.assertEqual(fibonacci(-1), -1)
        self.assertEqual(fibonacci(-42), -42)

    def test_input_invalid(self):
        try:
            n = fibonacci('oops')
            self.fail()
        except ValueError:
            pass
        except:
            self.fail()

    def test_input_valid(self):
        self.assertEqual(fibonacci(1), 1)
        self.assertEqual(fibonacci(2), 1)
        self.assertEqual(fibonacci(10), 55)
        self.assertEqual(fibonacci(20), 6765)
        self.assertEqual(fibonacci(30), 832040)
\end{minted}

The test cases above are executed using the unittest toolset which loads in
all the test functions and executes them in a test environment. The file
below is called \verb|runtest.py|.

\begin{minted}{python}
#!/bin/python

import unittest
import sys
from fibotest import fibotest

def runtest():
    testRunner = unittest.TextTestRunner()
    testSuite = unittest.TestLoader().loadTestsFromTestCase(fibotest)
    testRunner.run(testSuite)

runtest()
\end{minted}

To manually run the tests, simply execute the \verb|runtest.py| code. There
are, of course, many different ways to test Python code. A simpler alternative
could have been to use \verb|pytest| but using the \verb|unittest| strategy
is just as effective.

\begin{minted}{text}
Nicholass-MBP:awsgit nicholasrusso# python runtest.py
...
----------------------------------------------------------------------
Ran 3 tests in 0.970s

OK
\end{minted}

However, the goal of CodeBuild is to offload this testing to AWS based on
triggers, which can be manual scheduling, commit-based, time-based, and more.
In order to provide the build specifications for AWS so it knows what to test,
the \verb|buildspec.yml| file can be defined. Below is simple, one-stage CI pipeline
that just runs the test code we developed.

\begin{minted}{yaml}
# buildspec.yml
version: 0.2

phases:
  pre_build:
    commands:
      - python runtest.py
\end{minted}

Add, commit, and push these new files to the repository (not shown). Note that
the author also added a \verb|.gitignore| file so that the Python machine code
(\verb|.pyc|) files would be ignored by git. Verify that the source code files
appear in CodeCommit.

\addimg{aws-repo-files.png}{0.8}{AWS CodeCommit Repository with Files}

Click on the \verb|fibonacci.py| file as a sanity check to ensure the text was
transferred successfully. Notice that CodeCommit does some syntax highlighting
to improve readability.

\addimg{aws-code.png}{0.8}{AWS CodeCommit Fibonacci Source Code}

At this point, you can schedule a build in CodeBuild to test out your code.
The author does not walk through setting up CodeBuild because there are many
tutorials on it, and it is simple. A basic screenshot below shows the process
at a high level. CodeBuild will automatically spin up a test instance of sorts
(in this case, Ubuntu Linux with Python 3.5.2) to execute the buildspec.yml file.

\addimg{aws-ci-start.png}{0.8}{AWS CodeBuild Build Start}

After the manual build (in our case, just a unit test, we didn't ``build''
anything), the detailed results are displayed on the screen. The phases that
were not defined in the \verb|buildspec.yml| file, such as \verb|INSTALL|,
\verb|BUILD|, and \verb|POST_BUILD|, instantly succeed as they do not exist.
Actually testing the code in the \verb|PRE_BUILD| phase only took 1 second.
If you want to see this test take longer, define test cases use larger numbers
for the Fibonacci function input, such as 50.

\addimg{aws-ci-success.png}{0.8}{AWS CodeBuild Build Progress}

Below these results is the actual machine output, which matches the test
output we generated when running the tests manually. This indicates a
successful CI pipeline integration between CodeCommit and CodeBuild. Put
another way, it is a fully integrated development environment without the
manual setup of Github + Jenkins, Bitbucket + Travis CI, or whatever other
combination of SCM + CI you can think of.

\addimg{aws-ci-log.png}{0.8}{AWS CodeBuild Build Log}

Note that build history, as it is in every CI system, is also available. The
author initially failed the first build test due to a configuration problem
within buildspec.yml, which illustrates the value of maintaining build
history.

\addimg{aws-ci-history.png}{0.8}{AWS CodeCommit Build History}

The main drawback of these fully-integrated services is that they are specific
to your cloud provider. Some would call this ``vendor lock-in'', since
portability is limited. To move, you could clone your Git repositories and
move elsewhere, but that may require retooling your CI environment. It may
also be time consuming and risky for large projects with many developers and
many branches, whereby any coordinated work stoppage would be challenging to execute.

\subsubsection{Subversion (SVN) and comparison to Git}
Subversion (SVN) is another version control system, though in the author's
experience, is less commonly used today when compared to git. SVN is a
centralized version control system whereby the \verb|commit| action pushes
changes to the central repository. The \verb|checkout| action pulls changes
down from the repository. In git, these two actions govern activity against
the local repository with additional commands like push, pull (fetch and
merge), and clone being available for interaction with remote repositories.

This section assumes the reader has already set up a basic SVN server. A link
in the references provides simple instructions for building a local SVN server
on CentOS7. The author used this procedure, with some basic modifications for
Amazon Linux, for hosting on AWS EC2. It's public URL is
\verb|http://svn.njrusmc.net/| (the URL is dead at the time of this writing)
for this demonstration. A repository called \verb|repo1| has been created on
the server with a test user of \verb|nrusso| with full read/write permissions.

The screenshots below show the basic username/password login and the blank
repository. Do not continue until, at a minimum, you have achieved this
functionality.

\addimg{svn-login.png}{0.8}{SVN Repository --- Initial Login}

\addimg{svn-empty.png}{0.8}{SVN Repository --- Empty Project}

The remainder of this section is focused on SVN client-side operations, where
the author uses another Amazon Linux EC2 instance to represent a developer's
workstation.

First, SVN must be installed using the command below. Like git, it is a
relatively small program with a few small dependencies. Last, ensure the
\verb|svn| command is in your path, which should happen automatically.

\begin{minted}{text}
[root@devbox ec2-user]# yum install subversion
Loaded plugins: amazon-id, rhui-lb, search-disabled-repos

[snip]

Installed:
  subversion.x86_64 0:1.7.14-14.el7

Complete!

[root@devbox ec2-user]# which svn
/bin/svn
\end{minted}

Use the command below to checkout (similar to git's pull or clone) the empty
repository built on the SVN server. The author put little effort into securing
this environment, as evidenced by using HTTP and without any data protection
on the server itself. Production repositories would likely not see the
authentication warning below.

\begin{minted}{text}
[root@devbox ~]# svn co --username nrusso http://svn.njrusmc.net/svn/repo1 repo1
Authentication realm: <http://svn.njrusmc.net:80> SVN Repos
Password for 'nrusso': 

-----------------------------------------------------------------------
ATTENTION!  Your password for authentication realm:
[snip password warning]
Checked out revision 0.

The SVN system will automatically create a directory called "repo1" in the
working directory where the SVN checkout was performed. There are no
version-controlled files in it, since the repository has no code yet.

[root@devbox ~]# ls -l repo1/
total 0
\end{minted}

Next, change to this repository directory and look at the repository
information. There is nothing particularly interesting, but it is handy in
case you forget the URL or current revision.

\begin{minted}{text}
[root@devbox ~]# cd repo1/

[root@devbox repo1]# svn info
Path: .
Working Copy Root Path: /root/repo1
URL: http://svn.njrusmc.net/svn/repo1
Repository Root: http://svn.njrusmc.net/svn/repo1
Repository UUID: 26c9a9fa-97ad-4cdc-a0ad-9d84bf11e78a
Revision: 0
Node Kind: directory
Schedule: normal
Last Changed Rev: 0
Last Changed Date: 2018-05-05 09:45:25 -0400 (Sat, 05 May 2018)
\end{minted}

Next, create a file. The author created a simple but highly suboptimal
exponentiation function using recursion in Python. A few test cases are
included at the end of the file. The name of the Python file below is
\verb|svn_test.py|.

\begin{minted}{python}
#!/bin/python

def pow(base, exponent):
    if(exponent == 0):
        return 1
    else:
        return base * pow(base, exponent - 1)

print('2^4 is {}'.format(pow(2, 4)))
print('3^5 is {}'.format(pow(3, 5)))
print('4^6 is {}'.format(pow(4, 6)))
print('5^7 is {}'.format(pow(5, 7)))
\end{minted}

Quickly test the code by executing it with the command below (not that the
mathematical correctness matters for this demonstration).

\begin{minted}{text}
[root@devbox repo1]# python svn_test.py 
2^4 is 16
3^5 is 243
4^6 is 4096
5^7 is 78125
\end{minted}

Like git, SVN has a \verb|status| option. The question mark next to the new
Python files suggests SVN does not know what this file is. In git terms, it is
an untracked file that needs to be added to the version control system.

\begin{minted}{text}
[root@devbox repo1]# svn status
?       svn_test.py
\end{minted}

The SVN \verb|add| command is somewhat similar to git \verb|add| with the
exception that files are only added once. In git, \verb|add| moves files from
the working directory to the staging area. In SVN, \verb|add| moves untracked
files into a tracked status. The \verb|A| at the beginning of the line
indicates the file was added.

\begin{minted}{text}
[root@devbox repo1]# svn add svn_test.py 
A         svn_test.py
\end{minted}

In case you missed the output above, you can use the \verb|status| command
(\verb|st| is a built-in alias) to verify that the file was added.

\begin{minted}{text}
[root@devbox repo1]# svn st
A       svn_test.py
\end{minted}

The last step involves the \verb|commit| action to push changes to the SVN
repository. The output indicates we are now on version 1.

\begin{minted}{text}
[root@devbox repo1]# svn commit svn_test.py -m"python recursive exponent function"
Adding         svn_test.py
Transmitting file data .
Committed revision 1.
\end{minted}

The SVN status shows no changes. This similar to a git ``clean working
directory'' but is implicit given the lack of output.

\begin{minted}{text}
[root@devbox repo1]# svn st
[root@devbox repo1]#
\end{minted}

Below are screenshots of the repository as viewed from a web browser. Now, our
new file is present.

\addimg{svn-hasfile.png}{0.8}{SVN Repository --- Files Present}

As in most git-based repository systems with GUIs, such as Github or Gitlab,
you can click on the file to see its contents. While this version of SVN
server is a simple Apache2-based, no-frills implementation, this feature still
works. Clicking on the hyperlink reveals the source code contained in the file.

\addimg{svn-code.png}{0.8}{SVN Repository --- Viewing Code}

Next, make some changes to the file. In this case, remove one test case and
add a new one. Verify the changes were saved.

\begin{minted}{text}
[root@devbox repo1]# tail -4 svn_test.py
print('2^4 is {}'.format(pow(2, 4)))
print('4^6 is {}'.format(pow(4, 6)))
print('5^7 is {}'.format(pow(5, 7)))
print('6^8 is {}'.format(pow(6, 8)))
\end{minted}

SVN \verb|status| now reports the file as modified, similar to git. Use the
\verb|diff| command to view the changes. Plus signs (+) and minus signs (-)
are used to indicate additions and deletions, respectively.

\begin{minted}{text}
[root@devbox repo1]# svn status
M       svn_test.py

[root@devbox repo1]# svn diff
Index: svn_test.py
===================================================================
--- svn_test.py	(revision 1)
+++ svn_test.py	(working copy)
@@ -7,6 +7,6 @@
         return base * pow(base, exponent - 1)

 print('2^4 is {}'.format(pow(2, 4)))
-print('3^5 is {}'.format(pow(3, 5)))
 print('4^6 is {}'.format(pow(4, 6)))
 print('5^7 is {}'.format(pow(5, 7)))
+print('6^8 is {}'.format(pow(6, 8)))
\end{minted}

Unlike git, there is no staging area, so the \verb|add| command used again
fails. The file is already under version control and so can be directly
committed to the repository.

\begin{minted}{text}
[root@devbox repo1]# svn add svn_test.py
svn: warning: W150002: '/root/repo1/svn_test.py' is already under version control
svn: E200009: Could not add all targets because some targets are already versioned
svn: E200009: Illegal target for the requested operation
\end{minted}

Using the built-in \verb|ci| alias for \verb|commit|, push the changes to the
repository. The current code version is incremented to 2.

\begin{minted}{text}
[root@devbox repo1]# svn ci svn_test.py -m"different numbers"
Sending        svn_test.py
Transmitting file data .
Committed revision 2.
 \end{minted}

To view log entries, use the \verb|update| command first to bring changes from
the remote repository into our workspace. This ensures that the subsequent
\verb|log| command works correctly, similar to git's \verb|log| command. Using
the verbose option, one can see all of the relevant history for these code
modifications.

\begin{minted}{text}
[root@devbox repo1]# svn update
Updating '.':
At revision 2.

[root@devbox repo1]# svn log -v
------------------------------------------------------------------------
r2 | nrusso | 2018-05-05 10:52:37 -0400 (Sat, 05 May 2018) | 1 line
Changed paths:
   M /svn_test.py

different numbers
------------------------------------------------------------------------
r1 | nrusso | 2018-05-05 10:47:03 -0400 (Sat, 05 May 2018) | 1 line
Changed paths:
   A /svn_test.py

python recursive exponent function
------------------------------------------------------------------------
\end{minted}

The table that follows briefly compares the git and SVN version control
systems. One is not better than the other; they are simply different. Tools
like git is best suited for highly technical, distributed teams where local
version control and frequent offline development occurs. SVN is generally
simpler and it is easier to do simple tasks, such as manager a single-branch
repository with \verb|checkout| and \verb|commit| actions.

\begin{longtable}{lll}
\toprule
% top left cell is blank
&
\textbf{Git}
&
\textbf{Subversion (SVN)}
\\ \midrule
\textbf{General design}
&
Distributed; local and remote repo
&
Centralized; central repo only
\\ \midrule
\textbf{Staging area?}
&
Yes; can split work across commits
&
No, commit means push
\\ \midrule
\textbf{Learning curve}
&
Hard; many commands to learn
&
Easy; fewer moving pieces
\\ \midrule
\textbf{Branching and merging}
&
Easy, simple, and fast
&
Complex and laborious
\\ \midrule
\textbf{Revisions}
&
None; SHA1 commit IDs instead
&
Simple numbers; easy for non-techs
\\ \midrule
\textbf{Directory support}
&
Tracks only files, not directories
&
Tracks directories (empty ones too)
\\ \midrule
\textbf{Data tracked}
&
Content of the files
&
Files themselves
\\ \midrule
\textbf{Windows support}
&
Generally poor
&
Tortoise SVN plugin is a good option
\\
\bottomrule
\caption{Git and SVN Comparison}
\end{longtable}

\subsubsection{Network Validation with Batfish}
Given a set of network configurations, can you determine how the network
will behave? In the context of CI/CD, engineers will frequently spin up
virtual instances dynamically, interconnect them according to the topological
specifications, then load the configurations. Once complete, some automated
script will test for compliance on the emulated devices. While powerful, this
approach can be time consuming, resource intensive, and complex to build.
Batfish offers a comparable capability except operates offline, ingesting
configurations and inferring the network's behavior sans emulation. Batfish
is a great ``first step'' in a network test pipeline to catch any errors before
the emulations begin. In some environments, Batfish alone may be adequate to
determine the validity of a network, depending on the organizational goals.

The public documentation for Batfish is clear and concise. This demonstration
focuses primary on Batfish with Python using \verb|pybatfish|, linked
\href{https://pybatfish.readthedocs.io/en/latest/}{here}. The first several
steps are straightforward and leverage technologies discussed elsewhere in
this book, such as Python virtual environments and Docker containers. After
creating a new \verb|venv| for Batfish testing, install the \verb|pybatfish|
package. This provides a client interface into the Batfish server, which is
downloaded and run using Docker on the local development machine.

\begin{minted}{text}
[ec2-user@devbox bf]# python3.6 -m venv ~/environments/batfish
[ec2-user@devbox bf]# source ~/environments/batfish/bin/activate

[ec2-user@devbox bf]# pip install pybatfish
Collecting pybatfish
  Downloading https: (snip)
Successfully installed pybatfish-2020.10.8.667 (snip)

[ec2-user@devbox bf]# sudo docker pull batfish/allinone
Using default tag: latest
latest: Pulling from batfish/allinone
(snip)
Status: Downloaded newer image for batfish/allinone:latest
docker.io/batfish/allinone:latest

[ec2-user@devbox bf]# sudo docker run --name batfish \
  -v batfish-data:/data \
  -p 8888:8888 -p 9997:9997 -p 9996:9996 \
  -d batfish/allinone
be9782adbd7e5ec64(snip)
\end{minted}

In a production environment, one might leverage Kubernetes to maintain
several pods, each of which runs one instance of Batfish, to provide
increased scale and availability. Putting all of the Batfish pods behind
a common Kubernetes service (effectively a DNS hostname) is one approach
to building an enterprise-grade Batfish deployment. In the interest
of simplicity, this demo will employ Batfish to analyze two large OSPF
networks. These are Cisco Live presentations that I've delivered in the
past and each one has roughly 20 network devices. The
\href{https://github.com/nickrusso42518/ospf_brkrst3310}{BRKRST-3310}
session focuses on troubleshooting and automation while the
\href{https://github.com/nickrusso42518/ospf_digrst2337}{DIGRST-2337}
session focuses on design and deployment. The hyperlinks lead to the
configuration repositories for each session. Those repositories are
cloned from GitHub below.

\begin{minted}{text}
[ec2-user@devbox bf]# git clone https://github.com/nickrusso42518/ospf_brkrst3310.git
Cloning into 'ospf_brkrst3310'...
remote: Enumerating objects: 133, done.
remote: Total 133 (delta 0), reused 0 (delta 0), pack-reused 133
Receiving objects: 100% (133/133), 342.87 KiB | 0 bytes/s, done.
Resolving deltas: 100% (90/90), done.

[ec2-user@devbox bf]# git clone https://github.com/nickrusso42518/ospf_digrst2337.git
Cloning into 'ospf_digrst2337'...
remote: Enumerating objects: 45, done.
remote: Counting objects: 100% (45/45), done.
remote: Compressing objects: 100% (22/22), done.
remote: Total 45 (delta 27), reused 41 (delta 23), pack-reused 0
Unpacking objects: 100% (45/45), done.
\end{minted}

Batfish consumes information by encapsulating the relevant data into
``snapshots''. A snapshot is represented on the filesystem as a
hierarchical directory structure with a variety of subdirectories. The
only relevant directory in this demo is \verb|configs/| which contains
network device configurations (Batfish does not care about file extensions).
More generally, a snapshot is a collection of configurations for a given
network at a given point in time. Batfish can operate on multiple networks
independently, each with many snapshots. Within a given network, you can
analyze the differences between any pair of snapshots.

\begin{minted}{text}
[ec2-user@devbox bf]# mkdir -p snapshots/brkrst3310/configs
[ec2-user@devbox bf]# cp ospf_brkrst3310/final-configs/*.txt snapshots/brkrst3310/configs/
[ec2-user@devbox bf]# ls -1 snapshots/brkrst3310/configs/
R10.txt
R11.txt
R12.txt
(snip)
\end{minted}

The output below reveals the full tree structure. The snapshot directory
is named \verb|brkrst3310| and the \verb|configs/| subdirectory contains
all of the network device configurations. To add additional snapshots for
other networks, simply create a new directory under the \verb|snapshots/|
parent directory. For now, ignore the other (empty) directories.

\begin{minted}{text}
[ec2-user@devbox bf]# tree snapshots/ --charset==ascii
snapshots/
`-- brkrst3310
    |-- batfish
    |-- configs
    |   |-- R10.txt
    |   |-- R11.txt
    (snip)
    |   |-- R8.txt
    |   `-- R9.txt
    |-- hosts
    `-- iptables
\end{minted}

Next, let's write some Python code to interact with the local Batfish server.
The full script is shown below and is well-commented. In summary, the script
takes in a single command-line argument, which should match the name of the
snapshot directory (``brkrst3310'' in this case). The code connects to the
Batfish server, initializes a snapshot from the proper directory, then asks
a series of OSPF-related questions. A ``question'' is the mechanism by which
an engineer tasks Batfish. Batfish will ``answer'' the question and return a
\verb|pandas| data frame, commonly used for data manipulation and analysis.
The script converts the \verb|pandas| data frame into three common file
formats: JSON, HTML, CSV, and \verb|pandas| data frame as a text string.
These four formats are used for demonstration only; many additional formats
are available per the \verb|pandas| documentation.

\begin{minted}{text}
[ec2-user@devbox bf]# cat bf.py
\end{minted}

\begin{minted}{python}
#!/usr/bin/env python

"""
Author: Nick Russo
Purpose: Tests Batfish on sample Cisco Live sessions focused
on the OSPF routing protocol using archived configurations.
"""

import sys
import json
import pandas
from pybatfish.client.commands import *
from pybatfish.question import bfq, load_questions

# Global pandas formatting for string display
pandas.set_option("display.width", 1000)
pandas.set_option("display.max_columns", 20)
pandas.set_option("display.max_rows", 1000)
pandas.set_option("display.max_colwidth", -1)

def main(directory):
    """
    Tests Batfish logic on a specific snapshot directory.
    """

    # Perform basic initialization per documentation
    bf_session.host = "localhost"
    bf_set_network(directory)
    bf_init_snapshot(f"snapshots/{directory}", name=directory, overwrite=True)
    load_questions()

    # Identify the questions to ask (not calling methods yet)
    bf_questions = {
        "proc": bfq.ospfProcessConfiguration,
        "intf": bfq.ospfInterfaceConfiguration,
        "area": bfq.ospfAreaConfiguration,
        "nbrs": bfq.ospfEdges,
    }

    # Unpack dictionary tuples and iterate over them
    for short_name, bf_question in bf_questions.items():

        # Ask the question and store the response pandas frame
        pandas_frame = bf_question().answer().frame()

        # Assemble the generic file name prefix
        file_name = f"outputs/{short_name}_{directory}"

        # Generate JSON data for programmatic consumption
        json_data = json.loads(pandas_frame.to_json(orient="records"))
        with open(f"{file_name}.json", "w") as handle:
            json.dump(json_data, handle, indent=2)

        # Generate HTML data for web browser viewing
        html_data = pandas_frame.to_html()
        with open(f"{file_name}.html", "w") as handle:
            handle.write(html_data)

        # Generate CSV data using pipe separator (bf data has commas)
        csv_data = pandas_frame.to_csv(sep="|")
        with open(f"{file_name}.csv", "w") as handle:
            handle.write(csv_data)

        # Store string version of pandas data frame (table-like)
        with open(f"{file_name}.pandas.txt", "w") as handle:
            handle.write(str(pandas_frame))

if __name__ == "__main__":
    # Check for at least 2 CLI args; fail if absent
    if len(sys.argv) < 2:
        print("usage: python bf.py <snapshot_dir_name>")
        sys.exit(1)

    # Snapshot directory was specified; pass it into main
    else:
        main(sys.argv[1])
\end{minted}

You'll notice that the script writes all artifacts to the \verb|outputs/|
directory, so we'll quickly create that first. Then, we'll run the
\verb|bf.py| script, passing in ``brkrst3310'' as a CLI argument. By default,
Batfish logs its actions to the console for easy troubleshooting.

\begin{minted}{text}
[ec2-user@devbox bf]# mkdir outputs
[ec2-user@devbox bf]# python bf.py brkrst3310
status: TRYINGTOASSIGN
.... no task information
status: ASSIGNED
.... 2020-12-20 14:42:22.439000+00:00 Parse network configs 0 / 19.
status: ASSIGNED
.... 2020-12-20 14:42:22.439000+00:00 Convert configurations
  to vendor-independent format 1 / 20.
status: TERMINATEDNORMALLY
.... 2020-12-20 14:42:22.439000+00:00 Deserializing objects of type
  'org.batfish.datamodel.Configuration' from files 19 / 19.
Default snapshot is now set to brkrst3310
status: TRYINGTOASSIGN
.... no task information
status: CHECKINGSTATUS
.... no task information
status: TERMINATEDNORMALLY
.... 2020-12-20 14:42:22.955000+00:00 Parse environment BGP tables.
Successfully loaded 65 questions from remote
Successfully loaded 65 questions from remote
status: TRYINGTOASSIGN
.... no task information
status: CHECKINGSTATUS
.... no task information
status: TERMINATEDNORMALLY
.... 2020-12-20 14:42:23.356000+00:00 Begin job.
status: TRYINGTOASSIGN
.... no task information
status: TERMINATEDNORMALLY
.... 2020-12-20 14:42:23.674000+00:00 Begin job.
status: ASSIGNED
.... no task information
status: TERMINATEDNORMALLY
.... 2020-12-20 14:42:23.833000+00:00 Begin job.
\end{minted}

After a few seconds, the script completes, and the \verb|outputs/| directory
contains 16 new files (4 questions asked * 4 output formats). The script asked
Batfish for OSPF area, interface, process, and neighbor information specifically.
The full list of supported Batfish questions is listed in the documentation.

\begin{minted}{text}
[ec2-user@devbox bf]# ls -1 outputs/
area_brkrst3310.csv
area_brkrst3310.html
area_brkrst3310.json
area_brkrst3310.pandas.txt
intf_brkrst3310.csv
intf_brkrst3310.html
intf_brkrst3310.json
intf_brkrst3310.pandas.txt
nbrs_brkrst3310.csv
nbrs_brkrst3310.html
nbrs_brkrst3310.json
nbrs_brkrst3310.pandas.txt
proc_brkrst3310.csv
proc_brkrst3310.html
proc_brkrst3310.json
proc_brkrst3310.pandas.txt
\end{minted}

We'll examine one of each file corresponding to one of each feature. Starting
with the OSPF area JSON file, we see a list of dictionaries. Each dictionary
describes a different OSPF area from the perspective of a network device.
In this case, Batfish says R6 has area 4 configured as an NSSA\@. R4 also has
three interfaces in that area, one of which is passive. Regarding R2, it
has area 1 configured as a standard area with only one active interface
participating in that area. All of these statements are true; you can
check the GitHub configurations or topology diagram yourself if you like.

\begin{minted}{text}
[ec2-user@devbox bf]# head -n 26 outputs/area_brkrst3310.json
\end{minted}

\begin{minted}{json}
[
  {
    "Node": "r6",
    "VRF": "default",
    "Process_ID": "1",
    "Area": "4",
    "Area_Type": "NSSA",
    "Active_Interfaces": [
      "Ethernet0/0",
      "Serial1/1"
    ],
    "Passive_Interfaces": [
      "Loopback0"
    ]
  },
  {
    "Node": "r2",
    "VRF": "default",
    "Process_ID": "1",
    "Area": "1",
    "Area_Type": "NONE",
    "Active_Interfaces": [
      "Ethernet0/0"
    ],
    "Passive_Interfaces": []
  },
\end{minted}

Next, let's examine the OSPF interface HTML file. This uses a table format to
represent the data, making it easy to view for non-technical people to view
using their web browsers. The beginning of the file identifies the column
names and includes common OSPF interface-level parameters.

\begin{minted}{text}
[ec2-user@devbox bf]# head -n 15 outputs/intf_brkrst3310.html
\end{minted}

\begin{minted}{html}
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Interface</th>
      <th>VRF</th>
      <th>Process_ID</th>
      <th>OSPF_Area_Name</th>
      <th>OSPF_Enabled</th>
      <th>OSPF_Passive</th>
      <th>OSPF_Cost</th>
      <th>OSPF_Network_Type</th>
      <th>OSPF_Hello_Interval</th>
      <th>OSPF_Dead_Interval</th>
    </tr>
\end{minted}

Rather than scrub the file, it makes more sense to examine a web browser
screenshot as shown below. Some rows have been deleted for brevity.
Because the table is very wide and will be hard to read in this book, the
author has manually shortened some column names. At a glance, the data
looks correct, as all Ethernet interfaces in the topology typically have
a cost of 10, use standard OSPF hello/dead timers, are not passive
(i.e., links between devices), and use the P2P network type.

\addimg{batfish-html.png}{0.8}{Batfish pandas Data Frame in HTML Format}

Next, let's examine the OSPF process CSV file. Using the \verb|column|
command, an engineer can view a tabular file without needing a spreadsheet
application. Note that this particular ``answer'' embeds commas in the data,
so the Python script used the pipe (\verb:|:) character instead. Again, the
author has shortened some column names to keep the table clean. Like the JSON
and HTML files, this data is correct per the network topology.

\begin{minted}{text}
[ec2-user@devbox bf]# column -s'|' -t outputs/proc_brkrst3310.csv | less -S

Node  vrf      PID  Areas      Reference_BW  Router_ID  Export_Policy_Sources  ABR
r13   default  1    [3]        100000000.0   10.0.0.13  []                     False
r6    default  1    [4]        100000000.0   10.0.0.6   ['RM_EIGRP_TO_OSPF']   False
r15   default  1    [2]        100000000.0   10.0.0.15  []                     False
r4    default  1    [0, 1, 4]  100000000.0   10.0.0.4   []                     True
r7    default  1    [4]        100000000.0   10.0.0.7   ['RM_EIGRP_TO_OSPF']   False
r14   default  1    [0, 3]     100000000.0   10.0.0.14  []                     True
r16   default  1    [2]        100000000.0   10.0.0.16  []                     False
r11   default  1    [0, 1]     100000000.0   10.0.0.11  []                     True
r2    default  1    [0, 1]     100000000.0   10.0.0.2   []                     True
r10   default  1    [0, 1]     100000000.0   10.0.0.10  []                     True
r5    default  1    [0, 4]     100000000.0   10.0.0.5   []                     True
r12   default  1    [3]        100000000.0   10.0.0.12  []                     False
r19   default  1    [1]        100000000.0   10.0.0.19  []                     False
r3    default  1    [0, 2]     100000000.0   10.0.0.3   []                     True
r9    default  1    [0]        100000000.0   10.0.0.9   []                     False
r1    default  1    [0, 3]     100000000.0   10.0.0.1   []                     True
\end{minted}

Last, we can view a string representation of the raw \verb|pandas| data frame,
which is presented in a table-like format. It's a long file (38 lines) so
we'll examine the first several lines for brevity.

\begin{minted}{text}
[ec2-user@devbox bf]# wc outputs/nbrs_brkrst3310.pandas.txt
  38  116 1520 outputs1/nbrs_brkrst3310.pandas.txt

[ec2-user@devbox bf]# head -n 15 outputs1/nbrs_brkrst3310.pandas.txt
           Interface  Remote_Interface
0    r1[Ethernet0/0]  r14[Ethernet0/0]
1   r14[Ethernet0/0]   r1[Ethernet0/0]
2    r1[Ethernet0/1]   r2[Ethernet0/1]
3    r1[Ethernet0/1]   r3[Ethernet0/1]
4    r2[Ethernet0/1]   r1[Ethernet0/1]
5    r2[Ethernet0/1]   r3[Ethernet0/1]
6    r3[Ethernet0/1]   r2[Ethernet0/1]
7    r3[Ethernet0/1]   r1[Ethernet0/1]
8    r1[Ethernet0/2]  r13[Ethernet0/2]
9   r13[Ethernet0/2]   r1[Ethernet0/2]
10   r1[Ethernet0/3]  r12[Ethernet0/3]
11  r12[Ethernet0/3]   r1[Ethernet0/3]
12  r10[Ethernet0/1]   r9[Ethernet0/1]
13   r9[Ethernet0/1]  r10[Ethernet0/1]
\end{minted}

According to the topology, all of this information is correct. Most links are
point-to-point connections, such as those between R1-R14, R1-R13, and R9-R10.
Some links are multi-access and contain many neighbors as seen between
R1, R2, and R3 on Ethernet0/1 specifically. Unlike the area, interface,
and process outputs, testing for neighbors goes beyond just parsing a local
configuration file. Batfish logically determines how the routers are
connected and provides structured data in response, making it easy to
test for compliance with the expected design.

The advantage of writing a general-purpose script to test Batfish is that
you can pass in a variety of snapshot names. Let's run another quick
test using the second OSPF-focused Cisco Live session we cloned earlier.
The output below reviews the basic process for seeding a snapshot with
the proper directories and files.

\begin{minted}{text}
[ec2-user@devbox bf]# mkdir snapshots/digrst2337/configs/  (snip; make other dirs too)
[ec2-user@devbox bf]# cp ospf_digrst2337/configs/*.txt snapshots/digrst2337/configs/
[ec2-user@devbox bf]# tree snapshots/ --charset==ascii
snapshots/
|-- brkrst3310
|   |-- batfish
|   |-- configs
|   |   |-- R10.txt
|   |   |-- R11.txt
|   (snip)
|   |   |-- R8.txt
|   |   `-- R9.txt
|   |-- hosts
|   `-- iptables
`-- digrst2337
    |-- batfish
    |-- configs
    |   |-- R10.txt
    |   |-- R11.txt
    (snip)
    |   |-- R8.txt
    |   `-- R9.txt
    |-- hosts
    `-- iptables
\end{minted}

Then, run the \verb|bf.py| script and pass in ``digrst2337'', the directory
name, as a command-line argument. Some output has been omitted for brevity.

\begin{minted}{text}
[ec2-user@devbox bf]# python bf.py digrst2337
status: TRYINGTOASSIGN
.... no task information
status: ASSIGNED
.... 2020-12-20 14:56:39.149000+00:00 Begin job.
status: ASSIGNED
.... 2020-12-20 14:56:39.149000+00:00 Parse network configs 1 / 20.
status: ASSIGNED
.... 2020-12-20 14:56:39.149000+00:00 Parse network configs 2 / 20.
(snip)
status: TERMINATEDNORMALLY
.... 2020-12-20 14:56:43.849000+00:00 Begin job.
\end{minted}

Last, review the output files generated by the script as it relates to
the specified snapshot. For those interested in scrubbing the data in
greater depth, all of these files have been uploaded to their respective
Cisco Live GitHub repositories in the \verb|batfish_answers/| directory.

\begin{minted}{text}
[ec2-user@devbox bf]# ls -1 outputs/*2337*
area_digrst2337.csv
area_digrst2337.html
area_digrst2337.json
area_digrst2337.pandas.txt
intf_digrst2337.csv
intf_digrst2337.html
intf_digrst2337.json
intf_digrst2337.pandas.txt
nbrs_digrst2337.csv
nbrs_digrst2337.html
nbrs_digrst2337.json
nbrs_digrst2337.pandas.txt
proc_digrst2337.csv
proc_digrst2337.html
proc_digrst2337.json
proc_digrst2337.pandas.txt
\end{minted}

As a final note, Batfish has uses beyond just network configuration analysis.
As evidenced by the empty directories above, it can trace traffic flows between
hosts, even with complex \verb|iptables| rulesets. More recently, it can
analyze Amazon Web Services (AWS) architectures within a Virtual Private
Cloud (VPC) instance. From a business perspective, integrating Batfish into
CI/CD pipelines in a pre-check or post-check role can reduce risk and rework,
both of which reduce operating expenses in the long-term.

\subsubsection{Data Validation with JSON Schema}
Often times, input data must conform to a specific structure in order to
function correctly in a given application. YANG, a data modeling language
discussed earlier in this book, is one way to define the structure of data.
YANG is technically general-purpose and can be used for non-networking
applications, but most real-life usage relates to network automation. One
of YANG's biggest drawbacks is the technical complexity and subsequent
barriers for entry; one does not simply ``use YANG'' without extensive
education and testing, both on the langauge itself and the associated tooling.

Lightweight frameworks, such as \href{http://json-schema.org/}{JSON Schema},
are attractive alternatives for some developers. Much like an XML Schema
Definition (XSD) file, a JSON schema file defines and enforces the structure
of a given JSON object. The schema file is metadata that is programmatically
consumed to ensure structural compliance \textbf{before} the data is processed.
In the context of client-server applications, this is frequently called
``client-side validation'' because the data is checked \textbf{before}
being transmitted to the server. This book will use the Python package
\href{https://github.com/Julian/jsonschema}{jsonschema for Python} to
access these capabilities.

To demonstrate JSON schema in action, consider the Cisco SD-WAN solution, which
supports a robust REST API\@. In addition to standard CRUD operations, the API
supports a flexible query language to extract real-time performance data. The
query structure is described in detail
\href{https://sdwan-docs.cisco.com/Product_Documentation/Command_Reference/Command_Reference/vManage_REST_APIs/vManage_REST_APIs_Overview/vManage_Simple_Query}{here}.
The JSON data below is a valid example of an SD-WAN query. The topmost key
of \verb|query| is required, as are the \verb|condition| and \verb|rule|
keys. The strings \verb|AND| and \verb|OR| are the only valid options
for the condition, signifying ``match-all'' versus ``match-any'' logic,
respectively. The \verb|rules| key contains a list of dictionaries (or
``objects'' in JSON schema parlance), which must contain the four keys
shown. The remaining top-level keys of \verb|size|, \verb|fields|,
and \verb|sort| are optional, containing additional constraints on
the data returned. Also, note that most values are ultimately strings,
whereas some are integers. This particular query is used to collect
a subset of vManage performance statistics (CPU, memory, and disk) over
the past one year (52 weeks). Only the newest 3 entries are returned,
which is the result of combining a size limit with a descending sort.

\begin{minted}{text}
[centos@devbox jsonschema]# cat good.json
\end{minted}

\begin{minted}{json}
{
  "query": {
    "condition": "AND",
    "rules": [
      {
        "field": "entry_time",
        "type": "date",
        "operator": "last_n_weeks",
        "value": ["52"]
      },
      {
        "field": "host_name",
        "type": "string",
        "operator": "equal",
        "value": ["vmanage"]
      }
    ]
  },
  "size": 3,
  "fields": [
    "entry_time",
    "cpu_user_new",
    "mem_util",
    "disk_used"
  ],
  "sort": [
    {
      "field": "entry_time",
      "type": "date",
      "order": "desc"
    }
  ]
}
\end{minted}

Without JSON schema, we cannot know with certainty whether this HTTP payload
is correct or not. Our only option is to try and send it to a real SD-WAN
vManage instance, which serves as the single point of management for
SD-WAN networks. This demonstration uses the Cisco DevNet SD-WAN
\href{https://devnetsandbox.cisco.com/}{reservable sandbox}
currently running version 19.2, though this may
change in the future. The script below is well-commented and should
be self-explanatory for those familiar with Python. In summary,
the script takes one CLI argument representing a JSON file, loads
the data, then sends an SD-WAN query via HTTP POST request using
the JSON data as the HTTP body. The HTTP response will either be
the requested data (success), or an error message indicating the
problem (failure). Speaking from personal experience, the SD-WAN
query language is complex. Most of the time, any errors are due to
a ``Bad Request'' because the query payload is malformed. SD-WAN
will perform ``server-side'' validation to reveal the problem.

\begin{minted}{text}
[centos@devbox jsonschema]# cat send_sdwan_query.py
\end{minted}

\begin{minted}{python}
#!/usr/bin/env python

"""
Author: Nick Russo (njrusmc@gmail.com)
Purpose: Demonstrate jsonschema to validate Cisco SD-WAN API queries.
"""

import json
import sys
import requests

def main(query_body):
    """
    Execution begins here. Requires the HTTP query body as an argument.
    """

    # Define base URL, credentials and disable SSL warnings (self-signed cert)
    base_url = "https://10.10.20.90:443"
    creds = {"j_username": "admin", "j_password": "C1sco12345"}
    requests.packages.urllib3.disable_warnings()

    # Create session and attempt to authenticate
    sess = requests.session()
    auth = sess.post(f"{base_url}/j_security_check", data=creds, verify=False)

    # Ensure auth succeeded and no HTTP body was returned
    if not auth.ok or auth.text:
        print("Authentication failed")
        sys.exit(1)

    # Collect CSRF token (required in version 19.2 and newer)
    token = sess.get(f"{base_url}/dataservice/client/token")
    token.raise_for_status()

    # Success; issue query and print resulting HTTP body
    stats = sess.post(
        f"{base_url}/dataservice/statistics/system",
        json=query_body,
        headers={"X-XSRF-TOKEN": token.text},
        verify=False,
    )
    stats_data = stats.json()
    print(json.dumps(stats_data.get("data", stats_data), indent=2))

if __name__ == "__main__":
    # Load the CLI-specified instance data from file
    with open(sys.argv[1], "r") as handle:
        instance = json.load(handle)

    main(instance)
\end{minted}

Running the script with \verb|good.json| as a CLI argument yields
valid output. There are 3 items in the list, each containing the
requested fields, plus a unique identifier string. For reference,
the Unix epoch of 1612020225512 is equivalent to 30 January 2021
at approximately 1523 UTC\@.

\begin{minted}{text}
[centos@devbox jsonschema]# python send_sdwan_query.py good.json
\end{minted}

\begin{minted}{json}
[
  {
    "entry_time": 1612020225512,
    "disk_used": 666386432,
    "cpu_user_new": 2.02,
    "mem_util": 0.61,
    "id": "AXdT83Tj2joFIJpy0ejW"
  },
  {
    "entry_time": 1612020165495,
    "disk_used": 666394624,
    "cpu_user_new": 4.04,
    "mem_util": 0.61,
    "id": "AXdT83Tj2joFIJpy0ejV"
  },
  {
    "entry_time": 1612020105485,
    "disk_used": 666386432,
    "cpu_user_new": 2.01,
    "mem_util": 0.61,
    "id": "AXdT83Tj2joFIJpy0ejU"
  }
]
\end{minted}

Now, suppose we craft an incorrect query. The example below does not include
options such as \verb|size|, \verb|fields|, and \verb|sort|, which is fine.
The problem is that the first rule object is missing a \verb|value| field
which is required.

\begin{minted}{text}
[centos@devbox jsonschema]# cat bad1.json
\end{minted}

\begin{minted}{json}
{
  "query": {
    "condition": "AND",
    "rules": [
      {
        "field": "entry_time",
        "type": "date",
        "operator": "last_n_weeks"
      },
      {
        "field": "host_name",
        "type": "string",
        "operator": "equal",
        "value": ["vmanage"]
      }
    ]
  }
}
\end{minted}

Without client-side validation, supplying this malformed query to our
script results in a failure. SD-WAN graciously tells us the problem, but
ideally, our script should handle this input validation for us. Some
applications won't tell you anything useful, which complicates troubleshooting,
and further reinforces the need for client-side validation.

\begin{minted}{text}
[centos@devbox jsonschema]# python send_sdwan_query.py bad1.json
\end{minted}

\begin{minted}{json}
{
  "error": {
    "type": "error",
    "message": "Invalid query",
    "details": "At least one value should preset.",
    "code": "ELASTIC0007"
  }
}
\end{minted}

Writing custom Python code to enforce data compliance is a common practice and
something the author has personally done frequently, but that is often a
heavy-handed approach. For an example of this approach, check out the
\href{https://github.com/nickrusso42518/narc}{narc} project.
Instead, we'll solve this client-side validation problem using the
\verb|jsonschema| package discussed earlier. First, install via \verb|pip|:

\begin{minted}{text}
[centos@devbox netbox_ansible]# pip install jsonschema
Collecting jsonschema
  (snip)
Successfully installed jsonschema-3.2.0
\end{minted}

The schema file below can be used to check our query payloads before issuing
API requests. At the top of the file, it's common to see a \verb|definitions|
block which defines reusable types. In SD-WAN, the \verb|type| key is used
both for rule matching and sorting specifications, so to avoid copy/paste,
we can define a reusuable type definition named \verb|value_type|. Moving
into the schema itself, the top-most item is an \verb|object|, which really
means ``dictionary''. Then, there is a \verb|query| property, also an object,
which has a \verb|condition| property. The strings \verb|AND| and \verb|OR|
are enumerated as the only valid options for this string-typed field. Then,
there is an array (or list) of rules. Each item is an object (dictionary)
with a variety of properties. Notice the \verb|type| property references
the custom type definition described earlier. Again, be sure to reference
the official documentation referenced earlier when building your schema files.

Rather than explain every property, we'll focus on a few other notable aspects.
First, the \verb|required| key specifies a list of strings, identifying which
properties are required. By default, all properties are assumed to be optional,
and specifying mandatory fields empowers JSON schema to validate more than just
the values themselves. The \verb|size| property is an integer in a range of
1 to 65,535 with a default value of 10,000 (unsigned 16 bit integer). While
10,000 is indeed the default value per the SD-WAN documentation, the author
arbitrarily set minimum and maximum limits for demonstration purposes only.
Additionally, any element can be documented/commented using the
\verb|description| key. This schema is sparsely commented for brevity, but
in real life, all fields should have adequate descriptions. Other annotation
keys, such as \verb|title| and \verb|examples|, can be used within the schema
alongside \verb|description| and \verb|default|.

\begin{minted}{text}
[centos@devbox jsonschema]# cat schema.json
\end{minted}

\begin{minted}{json}
{
  "definitions": {
    "value_type": {
      "type": "string",
      "description": "Specify the type of queried value",
      "enum": ["date", "double", "int", "long", "string"]
    }
  },
  "type": "object",
  "properties": {
    "query": {
      "type": "object",
      "properties": {
        "condition": {"type": "string", "enum": ["AND", "OR"]},
        "rules": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "type": {"$ref": "#/definitions/value_type"},
              "field": {"type": "string"},
              "operator": {"type": "string"},
              "value": {"type": "array", "items": {"type": "string"}}
            },
            "required": ["type", "field", "operator", "value"]
          }
        }
      },
      "required": ["condition", "rules"]
    },
    "size": {
      "type": "integer",
      "minimum": 1,
      "maximum": 65535,
      "default": 10000
    },
    "fields": {
      "type": "array",
      "items": {"type": "string", "description": "Column names to collect"}
    },
    "sort": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "type": {"$ref": "#/definitions/value_type"},
          "field": {"type": "string"},
          "order": {"type": "string", "enum": ["asc", "desc"]}
        }
      }
    }
  },
  "required": ["query"]
}
\end{minted}

The \verb|jsonschema| package can be used in two separate ways:

\begin{enumerate}
  \item Via the shell by using the \verb|jsonschema| command
  \item In a Python script by importing \verb|jsonschema|
\end{enumerate}

The shell option is useful for quickly validating a data payload, known
more generally as an ``instance'', against the schema file. When an instance
is compliant, no output is returned and the return code is 0. This indicates
success and the return code makes this useful in CI/CD pipelines. However,
checking the malformed data reveals a missing required property and results
in a non-zero return code.

\begin{minted}{text}
[centos@devbox jsonschema]# jsonschema --instance good.json schema.json
[centos@devbox jsonschema]# echo $?
0

[centos@devbox jsonschema]# jsonschema --instance bad1.json schema.json
{'field': 'entry_time', 'type': 'date', 'operator': 'last_n_weeks'}: 'value' is a required property
[centos@devbox jsonschema]# echo $?
1
\end{minted}

The Python option is useful for integrating data validation into more
complex applications or client scripts. Let's enhance our existing Python
script using \verb|jsonschema| as shown below. Note that the \verb|main()|
function remains unchanged and has been omitted for brevity. This time,
we load in the JSON schema file in addition to the instance data, then
use the \verb|validate()| function, passing in both instance and schema data.

\begin{minted}{text}
[centos@devbox jsonschema]# cat send_sdwan_query.py
\end{minted}

\begin{minted}{python}
#!/usr/bin/env python

"""
Author: Nick Russo (njrusmc@gmail.com)
Purpose: Demonstrate jsonschema to validate Cisco SD-WAN API queries.
"""

import json
import sys
import requests
import jsonschema  # new!

def main(query_body):
    # snip; no changes from previous version

if __name__ == "__main__":
    # Load the fixed schema data from file
    with open("schema.json", "r") as handle:
        schema = json.load(handle)

    # Load the CLI-specified instance data from file
    with open(sys.argv[1], "r") as handle:
        instance = json.load(handle)

    # Perform validation and issue query to API upon success
    jsonschema.validate(instance=instance, schema=schema)
    main(instance)
\end{minted}

For brevity, this book won't show another successful sample run
using the \verb|good.json| input; that still works. Instead, let's
run the updated script with \verb|bad1.json|, which we expect to fail.
This time, \verb|validate()| intercepts the bogus data before sending an
API request to SD-WAN, which leads to a faster failure (reduced
network testing time) and less network/compute load (no need to
bother the server).

\begin{minted}{text}
[centos@devbox jsonschema]# python send_sdwan_query.py bad1.json
Traceback (most recent call last): (snip)
jsonschema.exceptions.ValidationError: 'value' is a required property

Failed validating 'required' in schema['properties']['query']['properties']['rules']['items']:
    {'properties': {'field': {'type': 'string'},
                    'operator': {'type': 'string'},
                    'type': {'$ref': '#/definitions/value_type'},
                    'value': {'items': {'type': 'string'},
                              'type': 'array'}},
     'required': ['type', 'field', 'operator', 'value'],
     'type': 'object'}

On instance['query']['rules'][0]:
    {'field': 'entry_time', 'operator': 'last_n_weeks', 'type': 'date'}
\end{minted}

It's worth examining a few other malformed payloads for completeness.
Consider the query below. It has two errors:

\begin{enumerate}
  \item The \verb|condition| of \verb|XOR| is not a valid choice
  \item The \verb|size| of \verb|-1| is outside of the specified range
\end{enumerate}

\begin{minted}{text}
[centos@devbox jsonschema]# cat bad2.json
\end{minted}

\begin{minted}{json}
{
  "query": {
    "condition": "XOR",
    "rules": [
      {
        "field": "entry_time",
        "type": "date",
        "operator": "last_n_weeks",
        "value": ["52"]
      },
      {
        "field": "host_name",
        "type": "string",
        "operator": "equal",
        "value": ["vmanage"]
      }
    ]
  },
  "size": -1
}
\end{minted}

Running the \verb|bad2.json| file through both the CLI tool and Python
script, we confirm that the query is invalid. Note that the CLI tool
generally displays all schema violations while the Python package only
displays one. This is likely because the first violation discovered
is enough to raise the \verb|ValidationError|, halting the process.

\begin{minted}{text}
[centos@devbox jsonschema]# jsonschema --instance bad2.json schema.json
XOR: 'XOR' is not one of ['AND', 'OR']
-1: -1 is less than the minimum of 1


[centos@devbox jsonschema]# python send_sdwan_query.py bad2.json
Traceback (most recent call last): (snip)
jsonschema.exceptions.ValidationError: -1 is less than the minimum of 1

Failed validating 'minimum' in schema['properties']['size']:
    {'default': 10000, 'maximum': 65535, 'minimum': 1, 'type': 'integer'}

On instance['size']:
    -1
\end{minted}

Last, consider a file with three errors:

\begin{enumerate}
  \item The \verb|operator| property is missing from the first rule
  \item The \verb|value| of \verb|42518| should be a string, but is an integer
  \item The \verb|size| of \verb|"big"| should be an integer, but is an string
\end{enumerate}

\begin{minted}{text}
[centos@devbox jsonschema]# cat bad3.json
\end{minted}

\begin{minted}{json}
{
  "query": {
    "condition": "AND",
    "rules": [
      {
        "field": "entry_time",
        "type": "date",
        "value": ["52"]
      },
      {
        "field": "host_name",
        "type": "string",
        "operator": "equal",
        "value": [42518]
      }
    ]
  },
  "size": "big"
}
\end{minted}

Running the \verb|bad3.json| query through our validation tools yields the
expected results as shown below.

\begin{minted}{text}
[centos@devbox jsonschema]# jsonschema --instance bad3.json schema.json
{'field': 'entry_time', 'type': 'date', 'value': ['52']}: 'operator' is a required property
42518: 42518 is not of type 'string'
big: 'big' is not of type 'integer'


[centos@devbox jsonschema]# python send_sdwan_query.py bad3.json
Traceback (most recent call last): (snip)
jsonschema.exceptions.ValidationError: 'big' is not of type 'integer'

Failed validating 'type' in schema['properties']['size']:
    {'default': 10000, 'maximum': 65535, 'minimum': 1, 'type': 'integer'}

On instance['size']:
    'big'
\end{minted}

JSON schema has many additional capabilities beyond what has been discussed
in this document. Rather than detail every feature, try using JSON schema
in your own projects instead of developing complex, conditional-based data
validation in your source code. Also, note that while the we call it ``JSON''
schema, all of these files could be in a different format, such as YAML,
and they would still work with the Python method (but not the CLI method).
For automation frameworks that rely primarily on YAML files for variables
and inventories, such as Ansible and Nornir, a pre-validation script could
load data from YAML, then pass the structured data into \verb|validate()|
for processing. In most cases, sticking with pure JSON is often simpler.
